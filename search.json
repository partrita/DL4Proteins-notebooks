[
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html",
    "title": "Section 1: Introduction to Protein Language Models",
    "section": "",
    "text": "##Return to HomePage\nCode and tutorial adapted from Sergey Ovchinnikov’s talk and notebook from RosettaCon 2024\nOBJECTIVES: By the end of this workshop you should be able to: * Explain the benefits of finetuning pre-trained protein language models * Finetune a model using LoRA (Low Rank Adaptation) * Apply finetuning to a practical task: predicting ligand-binding sites\nTo save your work, please save a copy of this notebook into your personal google drive.\n\n\n\n\n\nIntroduction: Why finetune?\n\n\n\n\n\nDirect Finetuning: Visualizing ESM\nDirect Finetuning: Training\n\n\n\n\n\nLoRA: Attaching LoRA to ESM\nLoRA: What is LoRA?\nLoRA: Training with LoRA\n\n\n\n\n\n\nProtein language models, trained on large datasets of protein sequences, have revolutionized our ability to understand and predict protein structures and functions. While these pre-trained models capture general patterns in protein sequences, they often need to be adapted for specific tasks. This process is called finetuning.\n\n\n\nTask Specificity: Pre-trained models provide a general understanding of protein sequences, but specific tasks may require more focused knowledge.\nLimited Data: Many protein-related tasks have limited labeled data. Finetuning allows leveraging the pre-trained model’s knowledge while adapting to the specific task.\nEfficiency: Starting from a pre-trained model is often faster and more effective than training from scratch.\n\nHowever, finetuning comes with its own challenges. Finetuning all parameters of a large model can be computationally expensive, and we are still limited by our dataset size. With limited task-specific data, full finetuning may lead to overfitting and the model may lose its general knowledge while adapting to a specific task.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('What might be an example where finetuning is useful',\n                  'Any application with limited data would benefit from finetuning, as it allows us to use transfer learning from more general data.')\n\n\nWhat might be an example where finetuning is useful\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\nOne important application of finetuning is predicting which positions in a protein could interact with a ligand. This information is crucial for understanding protein function and designing new drugs.\nFor this particular task, we are using ESM embeddings to determine the probability of whether each residue binds a ligand or not. We simply tune ESM on a set of proteins with labelled binding pockets.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown Let's set up our environment and load the necessary libraries. Take a peak at the code if you are interested in learning how to import large language models using the HuggingFace transformers library!\nmodel_name = \"esm2_t6_8M_UR50D\" # @param [\"esm2_t33_650M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t6_8M_UR50D\"]\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import EsmForSequenceClassification, EsmForTokenClassification, AutoTokenizer\n\ntrainable_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\n\nSome weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\nCode\n#@markdown Let's take a look at our model and how many parameters we are training\nprint(f'NUMBER OF TRAINABLE PARAMETERS: {trainable_params(model)}')\nprint('\\nModel Information\\n----------------------------')\nmodel\n\n\nNUMBER OF TRAINABLE PARAMETERS: 7737722\n\nModel Information\n----------------------------\n\n\nEsmForTokenClassification(\n  (esm): EsmModel(\n    (embeddings): EsmEmbeddings(\n      (word_embeddings): Embedding(33, 320, padding_idx=1)\n      (dropout): Dropout(p=0.15, inplace=False)\n      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n    )\n    (encoder): EsmEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x EsmLayer(\n          (attention): EsmAttention(\n            (self): EsmSelfAttention(\n              (query): Linear(in_features=320, out_features=320, bias=True)\n              (key): Linear(in_features=320, out_features=320, bias=True)\n              (value): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n              (rotary_embeddings): RotaryEmbedding()\n            )\n            (output): EsmSelfOutput(\n              (dense): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.15, inplace=False)\n            )\n            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n          )\n          (intermediate): EsmIntermediate(\n            (dense): Linear(in_features=320, out_features=1280, bias=True)\n          )\n          (output): EsmOutput(\n            (dense): Linear(in_features=1280, out_features=320, bias=True)\n            (dropout): Dropout(p=0.15, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (contact_head): EsmContactPredictionHead(\n      (regression): Linear(in_features=120, out_features=1, bias=True)\n      (activation): Sigmoid()\n    )\n  )\n  (dropout): Dropout(p=0.15, inplace=False)\n  (classifier): Linear(in_features=320, out_features=1, bias=True)\n)\n\n\n\n\nHere, we have imported a large language model (ESM2) and added extra layers to tune it for predicting ligand contacts. Let’s test out a naive approach by just directly finetuning all of the parameters in our protein language model. As the model trains, think about this method’s limitations.\n\n\nCode\n%matplotlib inline\nfrom IPython.display import display, clear_output\n#@markdown Run training!!!\n\n# GET DATA\nbatch_size = 32\nmax_crop_len = 512\n\n!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\nimport pickle\nwith open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n  DATA = pickle.load(handle)\n\nimport numpy as np\nimport pickle\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Helper function to pad sequences\ndef pad_sequence(seq, max_len, pad_value=0):\n    pad_size = max(0, max_len - len(seq))\n    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n\nclass CustomProteinDataset(Dataset):\n    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n        self.inputs = inputs\n        self.attention_masks = attention_masks\n        self.outputs = outputs\n        self.masks = masks\n        self.max_crop_len = max_crop_len\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]\n        attention_mask = self.attention_masks[idx]\n        output = self.outputs[idx]\n        mask = self.masks[idx]\n\n        # Calculate the true length of the sequence (where attention_mask == 1)\n        true_len = int(np.sum(attention_mask))\n\n        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n        crop_len = min(self.max_crop_len, true_len)\n\n        # Randomly sample a crop starting index\n        if true_len &gt; crop_len:\n            start_idx = np.random.randint(0, true_len - crop_len + 1)\n        else:\n            start_idx = 0\n\n        # Crop the sequences\n        input_ids = input_ids[start_idx:start_idx + crop_len]\n        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n\n        # Pad the cropped sequences to max_crop_len\n        input_ids = pad_sequence(input_ids, self.max_crop_len)\n        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n        output = pad_sequence(output, self.max_crop_len)\n        mask = pad_sequence(mask, self.max_crop_len)\n\n        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n\n# Create DataLoaders\ndataloaders = []\nfor v in range(3):  # train/test/validation\n    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n                                   max_crop_len=max_crop_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n    dataloaders.append(dataloader)\n\n\n# Training Code\n\ndef compute_loss(logits, labels, mask):\n  \"\"\"Compute masked loss.\"\"\"\n  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n  masked_loss = loss * mask\n  mean_loss = masked_loss.sum() / mask.sum()\n  return mean_loss\n\ndef train_one_epoch(model, dataloader, optimizer):\n  \"\"\"Train the model for one epoch.\"\"\"\n  model.train()\n  total_loss = 0\n\n  for batch in dataloader:\n    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n    # Forward pass\n    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n    logits = outputs.logits.squeeze(-1)\n\n    # Compute loss\n    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    mean_loss.backward()\n    optimizer.step()\n\n    total_loss += mean_loss.item()\n\n  average_loss = total_loss / len(dataloader)\n  return average_loss\n\ndef validate(model, dataloader):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n      for batch in dataloader:\n        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n        # Forward pass\n        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n        logits = outputs.logits.squeeze(-1)\n\n        # Compute loss\n        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n        total_loss += mean_loss.item()\n\n    average_loss = total_loss / len(dataloader)\n    return average_loss\n\n\n\n# Training loop\ndef train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n    \"\"\"Train and validate the model.\"\"\"\n    epoch_plot = []\n    train_plot = []\n    test_plot = []\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n        test_loss = validate(model, test_dataloader)\n        #print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n        # clear matplotlib plot if already exists\n        plt.clf()\n        clear_output(wait=True)\n        # visualize with matplotlib\n        epoch_plot.append(epoch + 1)\n        train_plot.append(train_loss)\n        test_plot.append(test_loss)\n        plt.plot(epoch_plot, train_plot, 'b.-', label='Train')\n        plt.plot(epoch_plot, test_plot, 'r.-', label='Test')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss (Lower is Better)')\n        plt.title('Training and Validation Loss')\n        # Set ticks\n        plt.yticks([])\n        plt.xlim(1, num_epochs)\n        plt.xticks(range(1, num_epochs+1))\n        plt.legend()\n        plt.show()\n\n    print(\"Training complete.\")\n\n# Actually run training\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Why might directly finetuning be suboptimal?',\n                  '1. Training is slow since we need to train lots of parameters\\n2. Direct finetuning leads to overfitting when working with smaller datasets.')\n\n\nWhy might directly finetuning be suboptimal?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\nLoRA is an efficient finetuning technique that addresses many challenges associated with finetuning large models. Let’s set up our dependencies again. However, something is different when we take a look into the model and trainable parameters. Can you tell what happened?\n\n\nCode\n#@markdown Set up dependencies (Re-run for each time you run the training below)\n\n# https://github.com/huggingface/peft\n!pip -q install --no-dependencies peft\nfrom peft import LoraConfig, get_peft_model, TaskType\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\"query\", \"key\", \"value\"],\n    r=4,\n    lora_dropout=0.15,\n)\nmodel = get_peft_model(model, config)\n\n\nSome weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\nCode\n#@markdown Show trainable parameters and model\nprint(f'NUMBER OF TRAINABLE PARAMETERS: {trainable_params(model)}')\nprint('\\nModel Information\\n----------------------------')\nmodel\n\n\nNUMBER OF TRAINABLE PARAMETERS: 46401\n\nModel Information\n----------------------------\n\n\nPeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): EsmForTokenClassification(\n      (esm): EsmModel(\n        (embeddings): EsmEmbeddings(\n          (word_embeddings): Embedding(33, 320, padding_idx=1)\n          (dropout): Dropout(p=0.15, inplace=False)\n          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n        )\n        (encoder): EsmEncoder(\n          (layer): ModuleList(\n            (0-5): 6 x EsmLayer(\n              (attention): EsmAttention(\n                (self): EsmSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                  (rotary_embeddings): RotaryEmbedding()\n                )\n                (output): EsmSelfOutput(\n                  (dense): Linear(in_features=320, out_features=320, bias=True)\n                  (dropout): Dropout(p=0.15, inplace=False)\n                )\n                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (intermediate): EsmIntermediate(\n                (dense): Linear(in_features=320, out_features=1280, bias=True)\n              )\n              (output): EsmOutput(\n                (dense): Linear(in_features=1280, out_features=320, bias=True)\n                (dropout): Dropout(p=0.15, inplace=False)\n              )\n              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (contact_head): EsmContactPredictionHead(\n          (regression): Linear(in_features=120, out_features=1, bias=True)\n          (activation): Sigmoid()\n        )\n      )\n      (dropout): Dropout(p=0.15, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=320, out_features=1, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=320, out_features=1, bias=True)\n        )\n      )\n    )\n  )\n)\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is different? Select all the correct answers.\"\ncorrect_answers = [\"The model now has less trainable parameters\", \"The model has new connections at each layer\", \"The model has more parameters\"]\ndecoy_answers = [\"The model has become deeper (more layers)\", \"Nothing has changed about the model architecture\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    \n        \n        \n        Nothing has changed about the model architecture\n        \n        \n        \n        The model has new connections at each layer\n        \n        \n        \n        The model now has less trainable parameters\n        \n        \n        \n        The model has become deeper (more layers)\n        \n        \n        \n        The model has more parameters\n        \n    Submit\n    \n    \n    \n\n\n\n\nInstead of updating all model parameters, LoRA introduces a small number of trainable parameters to tune the larger model by re-weighing the output of the hidden layers. It does this by adding an extra, smaller set of hidden parameters to each layer (or user-specified layers). The entire model is then frozen, and only the extra hidden parameters are trained.\n\n\n\nBy training on the adaptor, we can not only train enormous models on a single GPU, but also prevent overfitting by adjusting the ‘bottleneck’ of the adaptor.\n\n\n\nimage.png\n\n\n\n\nCode\n%matplotlib inline\n#@markdown We can directly run training using the same code as before, now that we have added LoRA adaptors to our model!\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers):\n    answers = [correct_answer] + decoy_answers\n    random.shuffle(answers)\n\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n    &lt;/script&gt;\n    \"\"\"\n    display(HTML(html_code))\n\nquestion = \"How does LoRA help in finetuning large language models?\"\ncorrect_answer = \"It reduces the number of trainable parameters, allowing for efficient finetuning on smaller datasets and preventing overfitting.\"\ndecoy_answers = [\n    \"It increases the model's capacity by adding more layers.\",\n    \"It completely replaces the original model architecture.\",\n    \"It only trains on new data, ignoring the pre-trained weights.\"\n]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\nWe can change the rank and (optionally, dropout) to better tune our model. Specifically, editing the rank controls the degree to which we want to fine-tune our model. The adaptor restricts the range which the model parameters can change, and adjusting the rank will control this range.\nIf the rank is too big, we will effectively do the same thing as raw finetuning. If it is too small, the model will not be finetuned at all because the parameters can’t change enough\nTry playing around with different values and see how this affects the training!\n\n\nCode\n%matplotlib inline\n# Reloading model so that we don't have to re-run\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\n#@markdown Change rank and dropout\nrank = 4 # @param {\"type\":\"integer\"}\ndropout = 0.15 # @param {\"type\":\"number\"}\n\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\"query\", \"key\", \"value\"],\n    r=rank,\n    lora_dropout=0.15,\n)\nmodel = get_peft_model(model, config)\n\n#@markdown Training Parameters\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\nIn this notebook, we’ve explored how to finetune protein language models for specific tasks. We’ve learned about:  1. The benefits of finetuning. 3. Finetuning techniques, with a focus on Low Rank Adaptation (LoRA). 4. Applying these concepts to a practical task: predicting ligand-binding sites.\nBy leveraging these powerful models and efficient finetuning techniques, we can push the boundaries of protein design and accelerate discoveries in fields like drug development and enzyme engineering.\nFor further exploration, consider: - Experimenting with different model architectures and sizes, especially with structure-aware language models, such as SaProt! - Adjusting LoRA sizes to find the best balance. - Applying these models to other protein-related tasks, such as predicting protein-protein interactions or designing novel proteins with specific functions\nRemember, the field of protein language models is rapidly evolving, so stay curious and keep learning!\n\n\n\n\nRelevant papers\n\nLow-N protein engineering with data-efficient deep learning, Biswas et al., Nature Methods, 2021\nContrastive Fitness Learning: Reprogramming Protein Language Models for Low-N Learning of Protein Fitness Landscape, Zhao et al., bioRxiv, 2024\nFine-tuning protein language models boosts predictions across diverse tasks, Schmirler et al., Nature Communications, 2024\nSeqProFT: Applying LoRA Finetuning for Sequence-only Protein Property Predictions, Zhang et al., arXiv, 2024"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#table-of-contents",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#table-of-contents",
    "title": "Section 1: Introduction to Protein Language Models",
    "section": "",
    "text": "Introduction: Why finetune?\n\n\n\n\n\nDirect Finetuning: Visualizing ESM\nDirect Finetuning: Training\n\n\n\n\n\nLoRA: Attaching LoRA to ESM\nLoRA: What is LoRA?\nLoRA: Training with LoRA"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#why-finetune",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#why-finetune",
    "title": "Section 1: Introduction to Protein Language Models",
    "section": "",
    "text": "Task Specificity: Pre-trained models provide a general understanding of protein sequences, but specific tasks may require more focused knowledge.\nLimited Data: Many protein-related tasks have limited labeled data. Finetuning allows leveraging the pre-trained model’s knowledge while adapting to the specific task.\nEfficiency: Starting from a pre-trained model is often faster and more effective than training from scratch.\n\nHowever, finetuning comes with its own challenges. Finetuning all parameters of a large model can be computationally expensive, and we are still limited by our dataset size. With limited task-specific data, full finetuning may lead to overfitting and the model may lose its general knowledge while adapting to a specific task.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('What might be an example where finetuning is useful',\n                  'Any application with limited data would benefit from finetuning, as it allows us to use transfer learning from more general data.')\n\n\nWhat might be an example where finetuning is useful"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#raw-fine-tuning",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#raw-fine-tuning",
    "title": "Section 1: Introduction to Protein Language Models",
    "section": "",
    "text": "Here, we have imported a large language model (ESM2) and added extra layers to tune it for predicting ligand contacts. Let’s test out a naive approach by just directly finetuning all of the parameters in our protein language model. As the model trains, think about this method’s limitations.\n\n\nCode\n%matplotlib inline\nfrom IPython.display import display, clear_output\n#@markdown Run training!!!\n\n# GET DATA\nbatch_size = 32\nmax_crop_len = 512\n\n!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\nimport pickle\nwith open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n  DATA = pickle.load(handle)\n\nimport numpy as np\nimport pickle\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Helper function to pad sequences\ndef pad_sequence(seq, max_len, pad_value=0):\n    pad_size = max(0, max_len - len(seq))\n    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n\nclass CustomProteinDataset(Dataset):\n    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n        self.inputs = inputs\n        self.attention_masks = attention_masks\n        self.outputs = outputs\n        self.masks = masks\n        self.max_crop_len = max_crop_len\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]\n        attention_mask = self.attention_masks[idx]\n        output = self.outputs[idx]\n        mask = self.masks[idx]\n\n        # Calculate the true length of the sequence (where attention_mask == 1)\n        true_len = int(np.sum(attention_mask))\n\n        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n        crop_len = min(self.max_crop_len, true_len)\n\n        # Randomly sample a crop starting index\n        if true_len &gt; crop_len:\n            start_idx = np.random.randint(0, true_len - crop_len + 1)\n        else:\n            start_idx = 0\n\n        # Crop the sequences\n        input_ids = input_ids[start_idx:start_idx + crop_len]\n        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n\n        # Pad the cropped sequences to max_crop_len\n        input_ids = pad_sequence(input_ids, self.max_crop_len)\n        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n        output = pad_sequence(output, self.max_crop_len)\n        mask = pad_sequence(mask, self.max_crop_len)\n\n        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n\n# Create DataLoaders\ndataloaders = []\nfor v in range(3):  # train/test/validation\n    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n                                   max_crop_len=max_crop_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n    dataloaders.append(dataloader)\n\n\n# Training Code\n\ndef compute_loss(logits, labels, mask):\n  \"\"\"Compute masked loss.\"\"\"\n  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n  masked_loss = loss * mask\n  mean_loss = masked_loss.sum() / mask.sum()\n  return mean_loss\n\ndef train_one_epoch(model, dataloader, optimizer):\n  \"\"\"Train the model for one epoch.\"\"\"\n  model.train()\n  total_loss = 0\n\n  for batch in dataloader:\n    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n    # Forward pass\n    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n    logits = outputs.logits.squeeze(-1)\n\n    # Compute loss\n    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    mean_loss.backward()\n    optimizer.step()\n\n    total_loss += mean_loss.item()\n\n  average_loss = total_loss / len(dataloader)\n  return average_loss\n\ndef validate(model, dataloader):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n      for batch in dataloader:\n        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n        # Forward pass\n        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n        logits = outputs.logits.squeeze(-1)\n\n        # Compute loss\n        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n        total_loss += mean_loss.item()\n\n    average_loss = total_loss / len(dataloader)\n    return average_loss\n\n\n\n# Training loop\ndef train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n    \"\"\"Train and validate the model.\"\"\"\n    epoch_plot = []\n    train_plot = []\n    test_plot = []\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n        test_loss = validate(model, test_dataloader)\n        #print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n        # clear matplotlib plot if already exists\n        plt.clf()\n        clear_output(wait=True)\n        # visualize with matplotlib\n        epoch_plot.append(epoch + 1)\n        train_plot.append(train_loss)\n        test_plot.append(test_loss)\n        plt.plot(epoch_plot, train_plot, 'b.-', label='Train')\n        plt.plot(epoch_plot, test_plot, 'r.-', label='Test')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss (Lower is Better)')\n        plt.title('Training and Validation Loss')\n        # Set ticks\n        plt.yticks([])\n        plt.xlim(1, num_epochs)\n        plt.xticks(range(1, num_epochs+1))\n        plt.legend()\n        plt.show()\n\n    print(\"Training complete.\")\n\n# Actually run training\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Why might directly finetuning be suboptimal?',\n                  '1. Training is slow since we need to train lots of parameters\\n2. Direct finetuning leads to overfitting when working with smaller datasets.')\n\n\nWhy might directly finetuning be suboptimal?"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#conclusion",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#conclusion",
    "title": "Section 1: Introduction to Protein Language Models",
    "section": "",
    "text": "In this notebook, we’ve explored how to finetune protein language models for specific tasks. We’ve learned about:  1. The benefits of finetuning. 3. Finetuning techniques, with a focus on Low Rank Adaptation (LoRA). 4. Applying these concepts to a practical task: predicting ligand-binding sites.\nBy leveraging these powerful models and efficient finetuning techniques, we can push the boundaries of protein design and accelerate discoveries in fields like drug development and enzyme engineering.\nFor further exploration, consider: - Experimenting with different model architectures and sizes, especially with structure-aware language models, such as SaProt! - Adjusting LoRA sizes to find the best balance. - Applying these models to other protein-related tasks, such as predicting protein-protein interactions or designing novel proteins with specific functions\nRemember, the field of protein language models is rapidly evolving, so stay curious and keep learning!"
  },
  {
    "objectID": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#conclusion",
    "href": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#conclusion",
    "title": "Putting it All Together: Designing Proteins",
    "section": "Conclusion",
    "text": "Conclusion\nAt the end of this notebook, you have learned how to make unconditional proteins, generate proteins with scaffolds, and make de novo binders to sites of interest. For more advanced or extra topics, such as partial diffusion - visit the full RFDiffusion notebook or take a look at the RFDiffusion-AA notebook for diffusing with ligands and DNA.\nAfter this notebook and all the previous tutorials, you have not only learned how to use deep learning protein design tools, but also learned the theory behind how they work. This is a solid foundation for researching future research, whether it be in tool development or applying protein design to different biological tasks. Good Job!\nWith that, we wish you good luck in your research and beyond!\n– DL4Proteins Notebooks Team"
  },
  {
    "objectID": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#examples-and-instructions-for-the-pipeline",
    "href": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#examples-and-instructions-for-the-pipeline",
    "title": "Putting it All Together: Designing Proteins",
    "section": "Examples and Instructions for the Pipeline",
    "text": "Examples and Instructions for the Pipeline\n\nUse contigs to define continious chains. Use a : to define multiple contigs and a / to define mutliple segments within a contig. For example:\nunconditional - contigs='100' - diffuse monomer of length 100 - contigs='50:100' - diffuse hetero-oligomer of lengths 50 and 100\nmotif scaffolding - contigs='40/A163-181/40' pdb='5TPN' - contigs='A3-30/36/A33-68' pdb='6MRR' - diffuse a loop of length 36 between two segments of defined PDB ranges.\nbinder design - contigs='A:50' pdb='4N5T' - diffuse a binder of length 50 to chain A of defined PDB. - contigs='E6-155:70-100' pdb='5KQV' hotspot='E64,E88,E96' - diffuse a binder of length 70 to 100 (sampled randomly) to chain E and defined hotspot(s).\nhints and tips - pdb='' leave blank to get an upload prompt - contigs='50-100' use dash to specify a range of lengths to sample from\nexample solutions\nUnconditional Diffusion: contigs='100',iterations=any\nMotif Scaffolding: contigs='A1-29/5-5/A98-126 : A30-97', pdb=6w70, iterations=any\nBinder: contigs='A1-63:B1-68:100-100', hotspots=B58, A41, B56, A44"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html",
    "title": "  Section 1: Introduction – A bit of history",
    "section": "",
    "text": "##Return to HomePage\nTutorial adapted from Andrej Karpathy’s Video and Notebook\nOBJECTIVES: After completing this workshop, you will be able to:\n\nPrepare and analyze a natural language dataset\nFormulate the autoregressive training scheme in the training loop\nDefine and instantiate language model and compare architecture to prominent language models\nTrain language model on English language, then protein ‘language’ (fasta file of amino acid sequences)\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n Table of Contents \nSection 1: Introduction – A bit of history\nSection 2: Prepare dataset\n\nUse tokenization to map letters to an integer representation\nFormatting data for next token prediction training scheme\nSplit data into train and validation splits\n\nSection 3: Model\n\nConfigure neural network hyperparameters\nDefine key components of transformer block (attention, linear layer, feedforward layer)\n\nSection 4: Training\n\nDefine training loop\nEvaluate performance on both train and validation sets\n\n\n\n\n\nCode\n# imports\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n!wget --no-check-certificate 'https://drive.google.com/uc?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh' -O proteinPoem.gif\n\n\n\n--2024-12-10 02:36:30--  https://drive.google.com/uc?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh\nResolving drive.google.com (drive.google.com)... 173.194.194.101, 173.194.194.139, 173.194.194.138, ...\nConnecting to drive.google.com (drive.google.com)|173.194.194.101|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh [following]\n--2024-12-10 02:36:30--  https://drive.usercontent.google.com/download?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.183.132, 2607:f8b0:4001:c64::84\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.183.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 163525 (160K) [image/gif]\nSaving to: ‘proteinPoem.gif’\n\nproteinPoem.gif     100%[===================&gt;] 159.69K  --.-KB/s    in 0.003s  \n\n2024-12-10 02:36:32 (61.6 MB/s) - ‘proteinPoem.gif’ saved [163525/163525]\n\n\n\n\n\n\n\nChatGPT has taken the world by storm! It is a system that allows you to interact with an AI that generates a text response from a text query. When you use ChatGPT, you will see that text is produced sequentially from left to right.\n\n\n\nAnd if you ask the same question twice, you’ll see a slightly different response. This is because ChatGPT is a probabilistic system, and for any one prompt it can give more than one response.\nChatGPT is what we call a language model, because it models a sequence of words/characters/tokens, and learns how certain words follow each other in natural language. The neural network under the hood of ChatGPT is the transformer. More specifically, the transformer decoder, known as the Generatively Pretrained Transformer (GPT). The original paper to propose this architecture is “Attention is All You Need”, and within five years of its publication took over nearly all branches of AI applications.\nAlthough building ChatGPT from scratch in a singular colab notebook would be unfeasible, we can still build and train a tiny transformer-based language model to learn some of the core concepts.\n\n\n\nFor this workshop we’ll use the tiny shakespeare dataset, which is a concatenation of all of the works of Shakespeare in a single file (roughly 1 MB). We’re going to model how these characters follow each other, to hopefully produce text that looks similar to the text it has been trained on.\nNote: I suggest first completing this notebook with the Shakespeare dataset. However, the method for tokenization (explained below) is generalizable to most types of language in a txt file (i.e English, Spanish, Chinese, protein sequences, etc). The second time through this notebook, feel free to use any txt file of your choice.\nIf you would like to train your model to generate protein sequences, uncomment the second wget command below. This will download a file containing sequences in FASTA format for all entries in the PDB archive.\n\n\nCode\ntorch.manual_seed(1337)\n\n# shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# protein dataset. I suggest first running model with shakespeare dataset\n#!wget https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\n\n\n--2024-11-25 15:43:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt             0%[                    ]       0  --.-KB/s               input.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.05s   \n\n2024-11-25 15:43:34 (22.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n\n\n\n\nCode\n# lets inspect our dataset\n# if training on protein sequences... replace `input.txt` with `pdb_seqres.txt`\nwith open('input.txt', 'r', encoding='utf-8') as f:\n  text = f.read()\n\n\n\n\nCode\n# roughly 1M characters\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nCode\n# first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\nHow many unique characters are there in the text?\n\n\nCode\n# here are all the unique characters that occur in text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('unique characters:')\nprint(''.join(chars))\nprint()\nprint('Vocab size:', vocab_size)\n\n\nunique characters:\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n\nVocab size: 65\n\n\n\n\nNow we will tokenize the entire training set of Shakespeare. A tokenizer converts raw text data into smaller units called tokens. It processes the text by breaking it down into individual tokens, which can represent words, subwords, or characters, depending on the tokenizer’s configuration. The tokenizer builds a vocabulary mapping tokens to numeric indices, facilitating the conversion of text data into numerical representations for the model.\n\n\n\nTokenizers handle tasks like handling out-of-vocabulary words, incorporating special tokens for sequence boundaries, and may utilize subword tokenization techniques for morphologically rich languages or rare words. Ultimately, it prepares text data for language model training by transforming it into a format suitable for learning and processing by the model. Here we will use a pretty basic tokenizer which maps every unique/individual character to one integer.\n\n\nCode\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# sanity check that our decoder can properly decode our encoder\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\nLet’s encode the entire text dataset and store it into a torch tensor. Now the entire dataset of text is stretched out as a single lone sequence of integers.\n\n\nCode\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will be provided to GPT in this format\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\nLet’s get ready to train! We’re never going to feed the entire text all at once because it is too computationally expensive and can lead to relatively worse generalization. Instead, we only train with chunks of data at a time by sampling random chunks from the dataset. The max length of these chunks are called a block size, which is our first hyperparameter we will define.\n\n\nCode\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\nCode\nblock_size = 8\ntrain_data[:block_size+1] # first 9 characters from training set\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\nThe language model is tasked with predicting the next token given a sequence of previous tokens. It segments the training data into input-output pairs, wher ethe input x consists of a sequence of tokens, and the output y represenst the token that immediately follows the text.\nSo, in the example above, the model is tasked with predicting the token 58 given the previous order of tokens 18, 47, 56, 57, 58,  1, 15, 47,.\nBut instead of 1 prediction task, this can be augmented to 8 prediction tasks! With just this chunk the model can be trained to predict: * Input 18 and output 47 * Input 18, 47 and output 56 * Input 18, 47, 56 and output 57 * Etc…\nBy iterating over each position and forming context-target pairs, the model is trained to understand the sequential nature of the text data and make predictions accordingly, facilitating the learning of language patterns and generating coherent text sequences. This approach not only enhances training efficiency by increasing the amount of training data but also exposes the model to a variety of context lengths, allowing it to learn from short to longer sequences. During inference or generation, the model can be provided with as little as 1 character of context and predict up to the block size. However, if the context exceeds the block size, which is the maximum length the model was trained on, truncation is necessary. Truncation ensures that the input sequence length does not exceed the model’s capacity, maintaining consistency with the training process and enabling effective generation of predictions within the model’s capabilities.\n\n\nCode\n# training on each segment is actually training on multiple sets of characters\nx = train_data[:block_size] # inputs to transformer\ny = train_data[1:block_size+1] # next block size characters (offset by 1). The targets for each input\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\nThis sequential nature of the input text data is often referred to as the time dimension of the tensor of a tokenized dataset. The time dimension influences how the input text is processed by the language model. During training, the model iterates over tokens along the “time” dimension, considering each token’s context within the sequence. Likewise, during inference or prediction, the model generates output tokens sequentially, one token at a time, based on the preceding tokens in the sequence.\nWe’ve looked at the time dimension of the tensors, and now we will look at the batch dimension. In reality we will actually be feeding batches of text at a time to the transformers. This will allow us to take full advantage of the GPU, which allows us to process multiple chunks of data in parallel. BTW there’s no “communication” between chunks of data processed in parallel. Each are done independently.\n\n\nCode\nbatch_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb, yb = get_batch('train')\nprint('input intro transformer:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninput intro transformer:\ntorch.Size([16, 8])\ntensor([[ 1, 39, 52, 42,  1, 45, 43, 50],\n        [ 1, 58, 46, 39, 58,  1, 42, 53],\n        [ 1, 61, 53, 59, 50, 42,  1, 21],\n        [59, 57, 40, 39, 52, 42,  1, 40],\n        [52, 42,  8,  0,  0, 23, 21, 26],\n        [45, 53, 42, 57,  0, 23, 43, 43],\n        [52,  1, 61, 39, 57,  1, 51, 53],\n        [39, 49, 12,  1, 27,  1, 58, 56],\n        [53, 44,  1, 57, 54, 43, 43, 41],\n        [57, 53, 52, 57,  8,  0,  0, 25],\n        [ 1, 42, 43, 44, 43, 41, 58,  1],\n        [21,  1, 61, 39, 52, 42, 43, 56],\n        [43, 43, 51,  5, 42,  1, 40, 59],\n        [45, 50, 63,  1, 52, 53, 61, 12],\n        [52, 53, 58,  8,  0, 25, 63,  1],\n        [53, 58,  6,  1, 51, 63,  1, 50]])\ntargets:\ntorch.Size([16, 8])\ntensor([[39, 52, 42,  1, 45, 43, 50, 42],\n        [58, 46, 39, 58,  1, 42, 53,  1],\n        [61, 53, 59, 50, 42,  1, 21,  1],\n        [57, 40, 39, 52, 42,  1, 40, 47],\n        [42,  8,  0,  0, 23, 21, 26, 19],\n        [53, 42, 57,  0, 23, 43, 43, 54],\n        [ 1, 61, 39, 57,  1, 51, 53, 56],\n        [49, 12,  1, 27,  1, 58, 56, 39],\n        [44,  1, 57, 54, 43, 43, 41, 46],\n        [53, 52, 57,  8,  0,  0, 25, 17],\n        [42, 43, 44, 43, 41, 58,  1, 53],\n        [ 1, 61, 39, 52, 42, 43, 56,  6],\n        [43, 51,  5, 42,  1, 40, 59, 56],\n        [50, 63,  1, 52, 53, 61, 12,  0],\n        [53, 58,  8,  0, 25, 63,  1, 61],\n        [58,  6,  1, 51, 63,  1, 50, 53]])\n----\nwhen input is [1] the target: 39\nwhen input is [1, 39] the target: 52\nwhen input is [1, 39, 52] the target: 42\nwhen input is [1, 39, 52, 42] the target: 1\nwhen input is [1, 39, 52, 42, 1] the target: 45\nwhen input is [1, 39, 52, 42, 1, 45] the target: 43\nwhen input is [1, 39, 52, 42, 1, 45, 43] the target: 50\nwhen input is [1, 39, 52, 42, 1, 45, 43, 50] the target: 42\nwhen input is [1] the target: 58\nwhen input is [1, 58] the target: 46\nwhen input is [1, 58, 46] the target: 39\nwhen input is [1, 58, 46, 39] the target: 58\nwhen input is [1, 58, 46, 39, 58] the target: 1\nwhen input is [1, 58, 46, 39, 58, 1] the target: 42\nwhen input is [1, 58, 46, 39, 58, 1, 42] the target: 53\nwhen input is [1, 58, 46, 39, 58, 1, 42, 53] the target: 1\nwhen input is [1] the target: 61\nwhen input is [1, 61] the target: 53\nwhen input is [1, 61, 53] the target: 59\nwhen input is [1, 61, 53, 59] the target: 50\nwhen input is [1, 61, 53, 59, 50] the target: 42\nwhen input is [1, 61, 53, 59, 50, 42] the target: 1\nwhen input is [1, 61, 53, 59, 50, 42, 1] the target: 21\nwhen input is [1, 61, 53, 59, 50, 42, 1, 21] the target: 1\nwhen input is [59] the target: 57\nwhen input is [59, 57] the target: 40\nwhen input is [59, 57, 40] the target: 39\nwhen input is [59, 57, 40, 39] the target: 52\nwhen input is [59, 57, 40, 39, 52] the target: 42\nwhen input is [59, 57, 40, 39, 52, 42] the target: 1\nwhen input is [59, 57, 40, 39, 52, 42, 1] the target: 40\nwhen input is [59, 57, 40, 39, 52, 42, 1, 40] the target: 47\nwhen input is [52] the target: 42\nwhen input is [52, 42] the target: 8\nwhen input is [52, 42, 8] the target: 0\nwhen input is [52, 42, 8, 0] the target: 0\nwhen input is [52, 42, 8, 0, 0] the target: 23\nwhen input is [52, 42, 8, 0, 0, 23] the target: 21\nwhen input is [52, 42, 8, 0, 0, 23, 21] the target: 26\nwhen input is [52, 42, 8, 0, 0, 23, 21, 26] the target: 19\nwhen input is [45] the target: 53\nwhen input is [45, 53] the target: 42\nwhen input is [45, 53, 42] the target: 57\nwhen input is [45, 53, 42, 57] the target: 0\nwhen input is [45, 53, 42, 57, 0] the target: 23\nwhen input is [45, 53, 42, 57, 0, 23] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43, 43] the target: 54\nwhen input is [52] the target: 1\nwhen input is [52, 1] the target: 61\nwhen input is [52, 1, 61] the target: 39\nwhen input is [52, 1, 61, 39] the target: 57\nwhen input is [52, 1, 61, 39, 57] the target: 1\nwhen input is [52, 1, 61, 39, 57, 1] the target: 51\nwhen input is [52, 1, 61, 39, 57, 1, 51] the target: 53\nwhen input is [52, 1, 61, 39, 57, 1, 51, 53] the target: 56\nwhen input is [39] the target: 49\nwhen input is [39, 49] the target: 12\nwhen input is [39, 49, 12] the target: 1\nwhen input is [39, 49, 12, 1] the target: 27\nwhen input is [39, 49, 12, 1, 27] the target: 1\nwhen input is [39, 49, 12, 1, 27, 1] the target: 58\nwhen input is [39, 49, 12, 1, 27, 1, 58] the target: 56\nwhen input is [39, 49, 12, 1, 27, 1, 58, 56] the target: 39\nwhen input is [53] the target: 44\nwhen input is [53, 44] the target: 1\nwhen input is [53, 44, 1] the target: 57\nwhen input is [53, 44, 1, 57] the target: 54\nwhen input is [53, 44, 1, 57, 54] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43, 43] the target: 41\nwhen input is [53, 44, 1, 57, 54, 43, 43, 41] the target: 46\nwhen input is [57] the target: 53\nwhen input is [57, 53] the target: 52\nwhen input is [57, 53, 52] the target: 57\nwhen input is [57, 53, 52, 57] the target: 8\nwhen input is [57, 53, 52, 57, 8] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0, 0] the target: 25\nwhen input is [57, 53, 52, 57, 8, 0, 0, 25] the target: 17\nwhen input is [1] the target: 42\nwhen input is [1, 42] the target: 43\nwhen input is [1, 42, 43] the target: 44\nwhen input is [1, 42, 43, 44] the target: 43\nwhen input is [1, 42, 43, 44, 43] the target: 41\nwhen input is [1, 42, 43, 44, 43, 41] the target: 58\nwhen input is [1, 42, 43, 44, 43, 41, 58] the target: 1\nwhen input is [1, 42, 43, 44, 43, 41, 58, 1] the target: 53\nwhen input is [21] the target: 1\nwhen input is [21, 1] the target: 61\nwhen input is [21, 1, 61] the target: 39\nwhen input is [21, 1, 61, 39] the target: 52\nwhen input is [21, 1, 61, 39, 52] the target: 42\nwhen input is [21, 1, 61, 39, 52, 42] the target: 43\nwhen input is [21, 1, 61, 39, 52, 42, 43] the target: 56\nwhen input is [21, 1, 61, 39, 52, 42, 43, 56] the target: 6\nwhen input is [43] the target: 43\nwhen input is [43, 43] the target: 51\nwhen input is [43, 43, 51] the target: 5\nwhen input is [43, 43, 51, 5] the target: 42\nwhen input is [43, 43, 51, 5, 42] the target: 1\nwhen input is [43, 43, 51, 5, 42, 1] the target: 40\nwhen input is [43, 43, 51, 5, 42, 1, 40] the target: 59\nwhen input is [43, 43, 51, 5, 42, 1, 40, 59] the target: 56\nwhen input is [45] the target: 50\nwhen input is [45, 50] the target: 63\nwhen input is [45, 50, 63] the target: 1\nwhen input is [45, 50, 63, 1] the target: 52\nwhen input is [45, 50, 63, 1, 52] the target: 53\nwhen input is [45, 50, 63, 1, 52, 53] the target: 61\nwhen input is [45, 50, 63, 1, 52, 53, 61] the target: 12\nwhen input is [45, 50, 63, 1, 52, 53, 61, 12] the target: 0\nwhen input is [52] the target: 53\nwhen input is [52, 53] the target: 58\nwhen input is [52, 53, 58] the target: 8\nwhen input is [52, 53, 58, 8] the target: 0\nwhen input is [52, 53, 58, 8, 0] the target: 25\nwhen input is [52, 53, 58, 8, 0, 25] the target: 63\nwhen input is [52, 53, 58, 8, 0, 25, 63] the target: 1\nwhen input is [52, 53, 58, 8, 0, 25, 63, 1] the target: 61\nwhen input is [53] the target: 58\nwhen input is [53, 58] the target: 6\nwhen input is [53, 58, 6] the target: 1\nwhen input is [53, 58, 6, 1] the target: 51\nwhen input is [53, 58, 6, 1, 51] the target: 63\nwhen input is [53, 58, 6, 1, 51, 63] the target: 1\nwhen input is [53, 58, 6, 1, 51, 63, 1] the target: 50\nwhen input is [53, 58, 6, 1, 51, 63, 1, 50] the target: 53\n\n\nIn summary, the 16x8 input array above contains a total of 128 examples and they are completely independent\n\n\n\n\nNow that we have our data ready, let’s start feeding this into neural networks!\nThis model is a Bigram Language Model implemented using a Transformer architecture. It generates text character by character based on a given input sequence. The model consists of 4 transformer blocks, each containing a multi-head self-attention mechanism and a feedforward neural network, followed by layer normalization. The model’s output is then projected to predict the next character in the sequence. During training, it minimizes the cross-entropy loss between predicted and actual characters. Finally, the trained model is used to generate new text by sampling characters based on the predicted probability distribution.\n\n\n\nimage.png\n\n\nBoth the model below and GPT are based on the transformer architecture. I outline some similaties and differences below: * Number of layers: GPT models typically consist of a large number of transformer blocks (GPT1 has 12 blocks, GPT2-L has 48, GPT3-L has 384). Ours has 4 blocks * GPT generates text token by token (where tokens are normally word fragments).Our model generates text character by character * GPT processes variable length context sequences. Our model takes a fixed size context window (block size) and generates the next token based on this context\n\n\nCode\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\n\nWe define a function estimate_loss() which evaluates the loss of our NN on both training and validation data.The model is temporarily switched to evaluation mode to disable gradient calculation, enhancing computational efficiency. It iterates over training and validation splits, retrieving batches of data and computing losses. After computing the mean loss for each split, the model is reverted to training mode. Finally, the function returns a dictionary containing the mean losses for both splits. This aids in assessing the model’s performance without updating its parameters, crucial for evaluation in machine learning tasks.\n\n\nCode\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\nThe major success of the transformer is attention, which we define in Head and MultiHeadAttention. The Head class represents a single head of self-attention, which is a fundamental component of transformer architectures. It initializes linear layers (self.key, self.query, and self.value) for computing key, query, and value representations, respectively, along with a triangular buffer for masking. In the forward method, it calculates attention scores, applies masking, computes attention weights, and aggregates values based on these weights.\nThe MultiHeadAttention class represents multiple heads of self-attention operating in parallel. It creates a list of Head instances (self.heads) to perform multi-headed attention. In the forward method, it applies each head individually to the input tensor x, concatenates the results, projects the concatenated output, and applies dropout for regularization. This class enables the model to capture different aspects of the input data simultaneously through multiple attention mechanisms, enhancing its representational capacity and performance.\n\n\nCode\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nThe FeedForward class represents a simple feedforward neural network layer commonly found in transformer architectures. It contains two linear layers followed by ReLU activation and dropout for regularization. In the forward method, it applies these operations sequentially to the input tensor x, transforming it into a higher-dimensional space and introducing non-linearity.\nThe Block class represents a single transformer block, consisting of a multi-headed self-attention layer (self.sa) followed by a feedforward layer (self.ffwd). It also incorporates layer normalization (self.ln1 and self.ln2) after each sub-layer. In the forward method, it passes the input tensor x through the self-attention layer and feedforward layer sequentially, adds the original input to the output of each layer (residual connection), and normalizes the result using layer normalization. This block encapsulates the key components of transformer models, facilitating both information exchange between tokens via attention and non-linear transformations through feedforward layers.\n\n\nCode\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\nThe BigramLanguageModel class consists of an embedding layer for token representation, a positional embedding layer to capture token positions within the sequence, a stack of transformer blocks for processing the input, layer normalization for stabilizing training, and a linear layer for predicting the next token. During the forward pass, the model takes input indices representing tokens, applies embeddings and positional encodings, processes them through transformer blocks, and predicts the next token’s logits. It then computes the loss. For generation, the model samples from the predicted distribution iteratively, selecting the next token based on the previous context and probabilities generated by the model. This approach allows the model to generate sequences of tokens autonomously.\n\n\nCode\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\nInstantiating model\n\n\nCode\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n\n0.209729 M parameters\n\n\n\n\n\nThe code runs a loop for max_iters iterations. Every eval_interval iterations (or in the last iteration), it evaluates the loss on both the training and validation sets using the estimate_loss() function and prints the results. It then samples a batch of data (xb, yb) from the training set using the get_batch() function. The model is called with the input (xb) and target (yb) tensors to compute the logits and loss. The optimizer’s gradient is zeroed (optimizer.zero_grad()) with set_to_none=True, which allows more efficient memory usage. Backpropagation is performed (loss.backward()) to compute gradients. Finally, the optimizer takes a step (optimizer.step()) to update the model parameters based on the computed gradients.\n\n\nCode\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\nstep 0: train loss 4.4166, val loss 4.4138\nstep 100: train loss 2.6565, val loss 2.6660\nstep 200: train loss 2.5054, val loss 2.4991\nstep 300: train loss 2.4156, val loss 2.4250\nstep 400: train loss 2.3504, val loss 2.3638\nstep 500: train loss 2.3134, val loss 2.3332\nstep 600: train loss 2.2550, val loss 2.2668\nstep 700: train loss 2.2216, val loss 2.2324\nstep 800: train loss 2.1821, val loss 2.2038\nstep 900: train loss 2.1458, val loss 2.1676\nstep 1000: train loss 2.1172, val loss 2.1455\nstep 1100: train loss 2.0883, val loss 2.1382\nstep 1200: train loss 2.0587, val loss 2.1036\nstep 1300: train loss 2.0410, val loss 2.0769\nstep 1400: train loss 2.0095, val loss 2.0547\nstep 1500: train loss 1.9951, val loss 2.0612\nstep 1600: train loss 1.9671, val loss 2.0487\nstep 1700: train loss 1.9665, val loss 2.0399\nstep 1800: train loss 1.9223, val loss 2.0177\nstep 1900: train loss 1.9239, val loss 2.0033\nstep 2000: train loss 1.8967, val loss 2.0079\nstep 2100: train loss 1.8890, val loss 1.9935\nstep 2200: train loss 1.8648, val loss 1.9667\nstep 2300: train loss 1.8607, val loss 1.9619\nstep 2400: train loss 1.8465, val loss 1.9414\nstep 2500: train loss 1.8273, val loss 1.9546\nstep 2600: train loss 1.8295, val loss 1.9418\nstep 2700: train loss 1.8314, val loss 1.9478\nstep 2800: train loss 1.8072, val loss 1.9341\nstep 2900: train loss 1.8032, val loss 1.9350\nstep 3000: train loss 1.7968, val loss 1.9243\nstep 3100: train loss 1.7785, val loss 1.9223\nstep 3200: train loss 1.7684, val loss 1.9112\nstep 3300: train loss 1.7700, val loss 1.9143\nstep 3400: train loss 1.7564, val loss 1.8982\nstep 3500: train loss 1.7397, val loss 1.8972\nstep 3600: train loss 1.7348, val loss 1.8899\nstep 3700: train loss 1.7338, val loss 1.8860\nstep 3800: train loss 1.7326, val loss 1.9028\nstep 3900: train loss 1.7289, val loss 1.8715\nstep 4000: train loss 1.7228, val loss 1.8684\nstep 4100: train loss 1.7142, val loss 1.8763\nstep 4200: train loss 1.7030, val loss 1.8693\nstep 4300: train loss 1.7099, val loss 1.8551\nstep 4400: train loss 1.7138, val loss 1.8726\nstep 4500: train loss 1.6938, val loss 1.8540\nstep 4600: train loss 1.6934, val loss 1.8410\nstep 4700: train loss 1.6836, val loss 1.8389\nstep 4800: train loss 1.6751, val loss 1.8453\nstep 4900: train loss 1.6787, val loss 1.8463\nstep 4999: train loss 1.6747, val loss 1.8296\n\n\nAfter training, text generation is demonstrated by providing an initial context (context) as a tensor of zeros. Since the context tensor is initialized with zeros, it essentially represents an empty starting point for text generation. The language model will use this empty context as the beginning of the generated sequence and progressively generate tokens based on its learned patterns and the provided context. As tokens are generated, they will be appended to the context tensor, allowing the model to continue generating text based on the evolving sequence.\nThe model’s generate method is called to generate a sequence of tokens (max_new_tokens=2000) based on the given context. The generated sequence is decoded into text format and printed. As you can see the generated text isnt fully legible, but the model has done a decent job at capturing the general structure of the training data.\n\n\nCode\n# generate from the model\n# if generating proteins... if you want to see how 'protein-like' the sequences are, feed them into AlphaFold2 and see if the sequence folds!!\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n\n\n\nWhy not call, Clifffay, all there me fear the labluntch but your Cappence agmand to\nToo.\n\nMONSARY:\nGood\nWhere these bappedied Droum, to\nI will depi'gr not love fellow the my breath,\nThan furthon that My love tambled thy followns and the like daming this di;\nAnd and he but thever is very may moung twict the Levings. Consticly\nMe brothers gate?\nNot, now you some, what that my frece fullw'd thou brother,\nAs my live, ashow thyself den; and is thurncts'd wher here's up?\nI would loves minow. Pomfort life, shom Conty-driack: sol, and,\nAnd,--like as may that Kingly testens can xpurthymen, shall where like\nThel, I haffit, thy not loves!\nAnd consent the brody with in in but the carrition.\nCamb'd, take His this you\nruther thine why, I'll mansce's have with a what fell wailt afwifed\nGod stild thou lost; as now, I have this rypengt.\n\nLANTA:\nHe is not Plow; deyelf, good whither\nForth: you, would it: and thus for not\nAnd sabetruess this let old this\nAs kneep ausis, trulow.--greaden of them heavy:\nWhe\n\n\nLet’s also generate text from a more specific context\n\n\nCode\n# input context. Change this to whatever you want!\n# if you are generating protein sequences... try inserting the first feww residues of a protein chain!\ninput_context = \"In the house of\"\n\n# encode input\nencoded_context = encode(input_context)\n\n# input into tensor\ncontext = torch.tensor([encoded_context], dtype=torch.long, device=device)\n\n# generate text using context\ngenerated_text = m.generate(context, max_new_tokens=100)[0].tolist()\n\n# decode and print text\nprint(decode(generated_text))\n\n\nIn the house of the brow you not.\n\nDUCHSHOMAND:\nGo denemings given they Comentence,\nSincoud ir, I am your her\nWater\n\n\n\n\nLets modify our model’s hyperparameters and see how each affects the model’s performance!\nQuestion: Increase n_embd (embedding size) from 64 to a higher value (such as 128 or 256). How does this affect performance compared to the original model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Increasing the embedding size allows the model to capture more nuanced patterns in the data. \\n A larger embedding dimension generally improves the model's capacity to understand and generate more complex language features. \\n Final loss should decrease.\")\n\n\nQuestion: Increase n_layer (number of transformer layers) from 4 to a higher value (such as 6 or 8). How does this affect the model’s performance?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More layers allow the model to learn deeper hierarchical patterns, which is important for capturing long-range dependencies in text. \\n The final loss should decrease due to better capacity to model long-range dependencies.\")\n\n\nQuestion: Increase n_head (the number of attention heads) from 4 to a higher value (such as 8 or 16). How does this affect model performance?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More attention heads allow the model to capture multiple aspects of relationships between tokens in parallel. \\n Increasing the number of attention heads can improve the model’s ability to focus on different parts of the input simultaneously. \\n The final loss should decrease.\")\n\n\nIn general, scaling up transformer models by increasing the embedding size, number of layers, or number of attention heads tends to improve performance across many natural language processing tasks. This trend arises because larger models have greater capacity to capture more complex patterns, relationships, and dependencies in the data. With more parameters, the model can better generalize from training data and generate more accurate, coherent, and contextually rich outputs. However, the gains from scaling diminish at a certain point, and careful tuning of hyperparameters is necessary to avoid overfitting or inefficiencies, especially with limited data or computational resources.\nRelevant papers * Simulating 500 million years of evolution with a language model, Hayes et al, 2024 * ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning, 2024 * Addressing the antibody germline bias and its effect on language models for improved antibody design, 2024 * The Protein Language Visualizer: Sequence Similarity Networks for the Era of Language Models, 2024 * Fine-tuning protein language models boosts predictions across diverse tasks, 2024 * Long-context Protein Language Model, 2024 * Training Compute-Optimal Protein Language Models, 2024 * Multi-purpose controllable protein generation via prompted language models, 2024 * Designing proteins with language models, 2024 * Protein language models learn evolutionary statistics of interacting sequence motifs, 2024 * p-IgGen: A Paired Antibody Generative Language Model, 2024 * Are Protein Language Models Compute Optimal?, 2024 * Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models, 2024 * Adapting protein language models for structure-conditioned design, 2024 * On Recovering Higher-order Interactions from Protein Language Models, 2024 * Protein language models are biased by unequal sequence sampling across the tree of life, 2024 * ESM All-Atom: Multi-Scale Protein Language Model for Unified Molecular Modeling, 2024 * Knowledge-aware Reinforced Language Models for Protein Directed Evolution, 2024 * Multi-Modal Large Language Model Enables Protein Function Prediction, 2024 * Benchmarking text-integrated protein language model embeddings and embedding fusion on diverse downstream tasks, 2024 * Large scale paired antibody language models, 2024 * Unsupervised evolution of protein and antibody complexes with a structure-informed language model, 2024 * Improving antibody language models with native pairing, 2023 * Large language models generate functional protein sequences across diverse families, 2023 * ProGen2: Exploring the Boundaries of Protein Language Models, 2022 * Generative Language Modeling for Antibody Design, 2023 * Language models of protein sequences at the scale of evolution enable accurate structure prediction, 2023"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#tokenizing-our-dataset",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#tokenizing-our-dataset",
    "title": "  Section 1: Introduction – A bit of history",
    "section": "",
    "text": "Now we will tokenize the entire training set of Shakespeare. A tokenizer converts raw text data into smaller units called tokens. It processes the text by breaking it down into individual tokens, which can represent words, subwords, or characters, depending on the tokenizer’s configuration. The tokenizer builds a vocabulary mapping tokens to numeric indices, facilitating the conversion of text data into numerical representations for the model.\n\n\n\nTokenizers handle tasks like handling out-of-vocabulary words, incorporating special tokens for sequence boundaries, and may utilize subword tokenization techniques for morphologically rich languages or rare words. Ultimately, it prepares text data for language model training by transforming it into a format suitable for learning and processing by the model. Here we will use a pretty basic tokenizer which maps every unique/individual character to one integer.\n\n\nCode\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# sanity check that our decoder can properly decode our encoder\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\nLet’s encode the entire text dataset and store it into a torch tensor. Now the entire dataset of text is stretched out as a single lone sequence of integers.\n\n\nCode\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will be provided to GPT in this format\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#preparation-for-training",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#preparation-for-training",
    "title": "  Section 1: Introduction – A bit of history",
    "section": "",
    "text": "Let’s get ready to train! We’re never going to feed the entire text all at once because it is too computationally expensive and can lead to relatively worse generalization. Instead, we only train with chunks of data at a time by sampling random chunks from the dataset. The max length of these chunks are called a block size, which is our first hyperparameter we will define.\n\n\nCode\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\nCode\nblock_size = 8\ntrain_data[:block_size+1] # first 9 characters from training set\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\nThe language model is tasked with predicting the next token given a sequence of previous tokens. It segments the training data into input-output pairs, wher ethe input x consists of a sequence of tokens, and the output y represenst the token that immediately follows the text.\nSo, in the example above, the model is tasked with predicting the token 58 given the previous order of tokens 18, 47, 56, 57, 58,  1, 15, 47,.\nBut instead of 1 prediction task, this can be augmented to 8 prediction tasks! With just this chunk the model can be trained to predict: * Input 18 and output 47 * Input 18, 47 and output 56 * Input 18, 47, 56 and output 57 * Etc…\nBy iterating over each position and forming context-target pairs, the model is trained to understand the sequential nature of the text data and make predictions accordingly, facilitating the learning of language patterns and generating coherent text sequences. This approach not only enhances training efficiency by increasing the amount of training data but also exposes the model to a variety of context lengths, allowing it to learn from short to longer sequences. During inference or generation, the model can be provided with as little as 1 character of context and predict up to the block size. However, if the context exceeds the block size, which is the maximum length the model was trained on, truncation is necessary. Truncation ensures that the input sequence length does not exceed the model’s capacity, maintaining consistency with the training process and enabling effective generation of predictions within the model’s capabilities.\n\n\nCode\n# training on each segment is actually training on multiple sets of characters\nx = train_data[:block_size] # inputs to transformer\ny = train_data[1:block_size+1] # next block size characters (offset by 1). The targets for each input\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\nThis sequential nature of the input text data is often referred to as the time dimension of the tensor of a tokenized dataset. The time dimension influences how the input text is processed by the language model. During training, the model iterates over tokens along the “time” dimension, considering each token’s context within the sequence. Likewise, during inference or prediction, the model generates output tokens sequentially, one token at a time, based on the preceding tokens in the sequence.\nWe’ve looked at the time dimension of the tensors, and now we will look at the batch dimension. In reality we will actually be feeding batches of text at a time to the transformers. This will allow us to take full advantage of the GPU, which allows us to process multiple chunks of data in parallel. BTW there’s no “communication” between chunks of data processed in parallel. Each are done independently.\n\n\nCode\nbatch_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb, yb = get_batch('train')\nprint('input intro transformer:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninput intro transformer:\ntorch.Size([16, 8])\ntensor([[ 1, 39, 52, 42,  1, 45, 43, 50],\n        [ 1, 58, 46, 39, 58,  1, 42, 53],\n        [ 1, 61, 53, 59, 50, 42,  1, 21],\n        [59, 57, 40, 39, 52, 42,  1, 40],\n        [52, 42,  8,  0,  0, 23, 21, 26],\n        [45, 53, 42, 57,  0, 23, 43, 43],\n        [52,  1, 61, 39, 57,  1, 51, 53],\n        [39, 49, 12,  1, 27,  1, 58, 56],\n        [53, 44,  1, 57, 54, 43, 43, 41],\n        [57, 53, 52, 57,  8,  0,  0, 25],\n        [ 1, 42, 43, 44, 43, 41, 58,  1],\n        [21,  1, 61, 39, 52, 42, 43, 56],\n        [43, 43, 51,  5, 42,  1, 40, 59],\n        [45, 50, 63,  1, 52, 53, 61, 12],\n        [52, 53, 58,  8,  0, 25, 63,  1],\n        [53, 58,  6,  1, 51, 63,  1, 50]])\ntargets:\ntorch.Size([16, 8])\ntensor([[39, 52, 42,  1, 45, 43, 50, 42],\n        [58, 46, 39, 58,  1, 42, 53,  1],\n        [61, 53, 59, 50, 42,  1, 21,  1],\n        [57, 40, 39, 52, 42,  1, 40, 47],\n        [42,  8,  0,  0, 23, 21, 26, 19],\n        [53, 42, 57,  0, 23, 43, 43, 54],\n        [ 1, 61, 39, 57,  1, 51, 53, 56],\n        [49, 12,  1, 27,  1, 58, 56, 39],\n        [44,  1, 57, 54, 43, 43, 41, 46],\n        [53, 52, 57,  8,  0,  0, 25, 17],\n        [42, 43, 44, 43, 41, 58,  1, 53],\n        [ 1, 61, 39, 52, 42, 43, 56,  6],\n        [43, 51,  5, 42,  1, 40, 59, 56],\n        [50, 63,  1, 52, 53, 61, 12,  0],\n        [53, 58,  8,  0, 25, 63,  1, 61],\n        [58,  6,  1, 51, 63,  1, 50, 53]])\n----\nwhen input is [1] the target: 39\nwhen input is [1, 39] the target: 52\nwhen input is [1, 39, 52] the target: 42\nwhen input is [1, 39, 52, 42] the target: 1\nwhen input is [1, 39, 52, 42, 1] the target: 45\nwhen input is [1, 39, 52, 42, 1, 45] the target: 43\nwhen input is [1, 39, 52, 42, 1, 45, 43] the target: 50\nwhen input is [1, 39, 52, 42, 1, 45, 43, 50] the target: 42\nwhen input is [1] the target: 58\nwhen input is [1, 58] the target: 46\nwhen input is [1, 58, 46] the target: 39\nwhen input is [1, 58, 46, 39] the target: 58\nwhen input is [1, 58, 46, 39, 58] the target: 1\nwhen input is [1, 58, 46, 39, 58, 1] the target: 42\nwhen input is [1, 58, 46, 39, 58, 1, 42] the target: 53\nwhen input is [1, 58, 46, 39, 58, 1, 42, 53] the target: 1\nwhen input is [1] the target: 61\nwhen input is [1, 61] the target: 53\nwhen input is [1, 61, 53] the target: 59\nwhen input is [1, 61, 53, 59] the target: 50\nwhen input is [1, 61, 53, 59, 50] the target: 42\nwhen input is [1, 61, 53, 59, 50, 42] the target: 1\nwhen input is [1, 61, 53, 59, 50, 42, 1] the target: 21\nwhen input is [1, 61, 53, 59, 50, 42, 1, 21] the target: 1\nwhen input is [59] the target: 57\nwhen input is [59, 57] the target: 40\nwhen input is [59, 57, 40] the target: 39\nwhen input is [59, 57, 40, 39] the target: 52\nwhen input is [59, 57, 40, 39, 52] the target: 42\nwhen input is [59, 57, 40, 39, 52, 42] the target: 1\nwhen input is [59, 57, 40, 39, 52, 42, 1] the target: 40\nwhen input is [59, 57, 40, 39, 52, 42, 1, 40] the target: 47\nwhen input is [52] the target: 42\nwhen input is [52, 42] the target: 8\nwhen input is [52, 42, 8] the target: 0\nwhen input is [52, 42, 8, 0] the target: 0\nwhen input is [52, 42, 8, 0, 0] the target: 23\nwhen input is [52, 42, 8, 0, 0, 23] the target: 21\nwhen input is [52, 42, 8, 0, 0, 23, 21] the target: 26\nwhen input is [52, 42, 8, 0, 0, 23, 21, 26] the target: 19\nwhen input is [45] the target: 53\nwhen input is [45, 53] the target: 42\nwhen input is [45, 53, 42] the target: 57\nwhen input is [45, 53, 42, 57] the target: 0\nwhen input is [45, 53, 42, 57, 0] the target: 23\nwhen input is [45, 53, 42, 57, 0, 23] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43, 43] the target: 54\nwhen input is [52] the target: 1\nwhen input is [52, 1] the target: 61\nwhen input is [52, 1, 61] the target: 39\nwhen input is [52, 1, 61, 39] the target: 57\nwhen input is [52, 1, 61, 39, 57] the target: 1\nwhen input is [52, 1, 61, 39, 57, 1] the target: 51\nwhen input is [52, 1, 61, 39, 57, 1, 51] the target: 53\nwhen input is [52, 1, 61, 39, 57, 1, 51, 53] the target: 56\nwhen input is [39] the target: 49\nwhen input is [39, 49] the target: 12\nwhen input is [39, 49, 12] the target: 1\nwhen input is [39, 49, 12, 1] the target: 27\nwhen input is [39, 49, 12, 1, 27] the target: 1\nwhen input is [39, 49, 12, 1, 27, 1] the target: 58\nwhen input is [39, 49, 12, 1, 27, 1, 58] the target: 56\nwhen input is [39, 49, 12, 1, 27, 1, 58, 56] the target: 39\nwhen input is [53] the target: 44\nwhen input is [53, 44] the target: 1\nwhen input is [53, 44, 1] the target: 57\nwhen input is [53, 44, 1, 57] the target: 54\nwhen input is [53, 44, 1, 57, 54] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43, 43] the target: 41\nwhen input is [53, 44, 1, 57, 54, 43, 43, 41] the target: 46\nwhen input is [57] the target: 53\nwhen input is [57, 53] the target: 52\nwhen input is [57, 53, 52] the target: 57\nwhen input is [57, 53, 52, 57] the target: 8\nwhen input is [57, 53, 52, 57, 8] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0, 0] the target: 25\nwhen input is [57, 53, 52, 57, 8, 0, 0, 25] the target: 17\nwhen input is [1] the target: 42\nwhen input is [1, 42] the target: 43\nwhen input is [1, 42, 43] the target: 44\nwhen input is [1, 42, 43, 44] the target: 43\nwhen input is [1, 42, 43, 44, 43] the target: 41\nwhen input is [1, 42, 43, 44, 43, 41] the target: 58\nwhen input is [1, 42, 43, 44, 43, 41, 58] the target: 1\nwhen input is [1, 42, 43, 44, 43, 41, 58, 1] the target: 53\nwhen input is [21] the target: 1\nwhen input is [21, 1] the target: 61\nwhen input is [21, 1, 61] the target: 39\nwhen input is [21, 1, 61, 39] the target: 52\nwhen input is [21, 1, 61, 39, 52] the target: 42\nwhen input is [21, 1, 61, 39, 52, 42] the target: 43\nwhen input is [21, 1, 61, 39, 52, 42, 43] the target: 56\nwhen input is [21, 1, 61, 39, 52, 42, 43, 56] the target: 6\nwhen input is [43] the target: 43\nwhen input is [43, 43] the target: 51\nwhen input is [43, 43, 51] the target: 5\nwhen input is [43, 43, 51, 5] the target: 42\nwhen input is [43, 43, 51, 5, 42] the target: 1\nwhen input is [43, 43, 51, 5, 42, 1] the target: 40\nwhen input is [43, 43, 51, 5, 42, 1, 40] the target: 59\nwhen input is [43, 43, 51, 5, 42, 1, 40, 59] the target: 56\nwhen input is [45] the target: 50\nwhen input is [45, 50] the target: 63\nwhen input is [45, 50, 63] the target: 1\nwhen input is [45, 50, 63, 1] the target: 52\nwhen input is [45, 50, 63, 1, 52] the target: 53\nwhen input is [45, 50, 63, 1, 52, 53] the target: 61\nwhen input is [45, 50, 63, 1, 52, 53, 61] the target: 12\nwhen input is [45, 50, 63, 1, 52, 53, 61, 12] the target: 0\nwhen input is [52] the target: 53\nwhen input is [52, 53] the target: 58\nwhen input is [52, 53, 58] the target: 8\nwhen input is [52, 53, 58, 8] the target: 0\nwhen input is [52, 53, 58, 8, 0] the target: 25\nwhen input is [52, 53, 58, 8, 0, 25] the target: 63\nwhen input is [52, 53, 58, 8, 0, 25, 63] the target: 1\nwhen input is [52, 53, 58, 8, 0, 25, 63, 1] the target: 61\nwhen input is [53] the target: 58\nwhen input is [53, 58] the target: 6\nwhen input is [53, 58, 6] the target: 1\nwhen input is [53, 58, 6, 1] the target: 51\nwhen input is [53, 58, 6, 1, 51] the target: 63\nwhen input is [53, 58, 6, 1, 51, 63] the target: 1\nwhen input is [53, 58, 6, 1, 51, 63, 1] the target: 50\nwhen input is [53, 58, 6, 1, 51, 63, 1, 50] the target: 53\n\n\nIn summary, the 16x8 input array above contains a total of 128 examples and they are completely independent"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#questions",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#questions",
    "title": "  Section 1: Introduction – A bit of history",
    "section": "",
    "text": "Lets modify our model’s hyperparameters and see how each affects the model’s performance!\nQuestion: Increase n_embd (embedding size) from 64 to a higher value (such as 128 or 256). How does this affect performance compared to the original model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Increasing the embedding size allows the model to capture more nuanced patterns in the data. \\n A larger embedding dimension generally improves the model's capacity to understand and generate more complex language features. \\n Final loss should decrease.\")\n\n\nQuestion: Increase n_layer (number of transformer layers) from 4 to a higher value (such as 6 or 8). How does this affect the model’s performance?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More layers allow the model to learn deeper hierarchical patterns, which is important for capturing long-range dependencies in text. \\n The final loss should decrease due to better capacity to model long-range dependencies.\")\n\n\nQuestion: Increase n_head (the number of attention heads) from 4 to a higher value (such as 8 or 16). How does this affect model performance?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More attention heads allow the model to capture multiple aspects of relationships between tokens in parallel. \\n Increasing the number of attention heads can improve the model’s ability to focus on different parts of the input simultaneously. \\n The final loss should decrease.\")\n\n\nIn general, scaling up transformer models by increasing the embedding size, number of layers, or number of attention heads tends to improve performance across many natural language processing tasks. This trend arises because larger models have greater capacity to capture more complex patterns, relationships, and dependencies in the data. With more parameters, the model can better generalize from training data and generate more accurate, coherent, and contextually rich outputs. However, the gains from scaling diminish at a certain point, and careful tuning of hyperparameters is necessary to avoid overfitting or inefficiencies, especially with limited data or computational resources.\nRelevant papers * Simulating 500 million years of evolution with a language model, Hayes et al, 2024 * ESM Cambrian: Revealing the mysteries of proteins with unsupervised learning, 2024 * Addressing the antibody germline bias and its effect on language models for improved antibody design, 2024 * The Protein Language Visualizer: Sequence Similarity Networks for the Era of Language Models, 2024 * Fine-tuning protein language models boosts predictions across diverse tasks, 2024 * Long-context Protein Language Model, 2024 * Training Compute-Optimal Protein Language Models, 2024 * Multi-purpose controllable protein generation via prompted language models, 2024 * Designing proteins with language models, 2024 * Protein language models learn evolutionary statistics of interacting sequence motifs, 2024 * p-IgGen: A Paired Antibody Generative Language Model, 2024 * Are Protein Language Models Compute Optimal?, 2024 * Feature Reuse and Scaling: Understanding Transfer Learning with Protein Language Models, 2024 * Adapting protein language models for structure-conditioned design, 2024 * On Recovering Higher-order Interactions from Protein Language Models, 2024 * Protein language models are biased by unequal sequence sampling across the tree of life, 2024 * ESM All-Atom: Multi-Scale Protein Language Model for Unified Molecular Modeling, 2024 * Knowledge-aware Reinforced Language Models for Protein Directed Evolution, 2024 * Multi-Modal Large Language Model Enables Protein Function Prediction, 2024 * Benchmarking text-integrated protein language model embeddings and embedding fusion on diverse downstream tasks, 2024 * Large scale paired antibody language models, 2024 * Unsupervised evolution of protein and antibody complexes with a structure-informed language model, 2024 * Improving antibody language models with native pairing, 2023 * Large language models generate functional protein sequences across diverse families, 2023 * ProGen2: Exploring the Boundaries of Protein Language Models, 2022 * Generative Language Modeling for Antibody Design, 2023 * Language models of protein sequences at the scale of evolution enable accurate structure prediction, 2023"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html",
    "href": "notebooks/WS07_GNNsForProteins.html",
    "title": "Section 1: Graph Theory",
    "section": "",
    "text": "##Return to HomePage\nOBJECTIVES: By the end of this workshop you should be able to:\n\nIdentify the types of data that are ideal for GNNs and why protein structures are suitable for this type of nn.\nDefine nodes and edges in the context of graph data. How can proteins be represented as graphs?\nGive examples of node-level, edge-level, and graph-level tasks for protein prediction.\nExplain the difference between directed and undirected edges.\nClassify which operations in the GNN require invariance or equivariance and explain why.\nName and explain the steps in message passing.\nExplain the different layers within a GNN and how it differs from other models.\nApply a GNN to answer a node-level, edge-level, or graph-level question about proteins.\n\nTo save your work, please save a copy of this notebook into your personal google drive.\nTable of Contents:\n\n\n\nGraph Theory: How is a graph defined?\nGraph Theory: Graph Prediction Tasks\nGraph Theory: Representation as an Adjacency Matrix\nGraph Theory: Directed and Undirected Graphs\nGraph Theory: Further define nodes and edges with feature embeddings\n\n\n\n\n\nGNN Theory: Message Passing\nGNN Theory: Permutation Invariance vs. Permutation Equivariance\n\n\n\n\n\nImplement a GCN: Load and Prep the Input Data\nImplement a GCN: Define message passing function\nImplement a GCN: Define a GraphConv layer\nImplement a GCN: Define our model\nImplement a GCN: Train and eval\n\n\n\n\n\n\n\n\n\nCode\n# Install modules\n!pip install graphein\n!pip install pytorch3d\n!pip install torch-geometric\n!pip install torch\n!pip install biovec\n!pip install biographs\n\n\n\nRequirement already satisfied: graphein in /usr/local/lib/python3.10/dist-packages (1.7.7)\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from graphein) (2.2.2)\n\nRequirement already satisfied: biopandas&gt;=0.5.1 in /usr/local/lib/python3.10/dist-packages (from graphein) (0.5.1)\n\nRequirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from graphein) (1.84)\n\nRequirement already satisfied: bioservices&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.11.2)\n\nRequirement already satisfied: cpdb-protein==0.2.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (0.2.0)\n\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from graphein) (3.0.11)\n\nRequirement already satisfied: deepdiff in /usr/local/lib/python3.10/dist-packages (from graphein) (8.0.1)\n\nRequirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from graphein) (0.7.3)\n\nRequirement already satisfied: looseversion in /usr/local/lib/python3.10/dist-packages (from graphein) (1.1.2)\n\nRequirement already satisfied: matplotlib&gt;=3.4.3 in /usr/local/lib/python3.10/dist-packages (from graphein) (3.8.0)\n\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from graphein) (1.0.0)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from graphein) (3.4.2)\n\nRequirement already satisfied: numpy&lt;2 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.26.4)\n\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from graphein) (5.24.1)\n\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from graphein) (2.10.3)\n\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from graphein) (13.9.4)\n\nRequirement already satisfied: rich-click in /usr/local/lib/python3.10/dist-packages (from graphein) (1.8.5)\n\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from graphein) (0.13.2)\n\nRequirement already satisfied: pyyaml!=5.4.0,!=5.4.1,!=6.0.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (6.0.2)\n\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from graphein) (1.5.2)\n\nRequirement already satisfied: scipy&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.13.1)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from graphein) (4.66.6)\n\nRequirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from graphein) (3.2)\n\nRequirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from graphein) (2024.10.0)\n\nRequirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (from graphein) (0.2.36)\n\nRequirement already satisfied: mmtf-python==1.1.3 in /usr/local/lib/python3.10/dist-packages (from biopandas&gt;=0.5.1-&gt;graphein) (1.1.3)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from biopandas&gt;=0.5.1-&gt;graphein) (75.1.0)\n\nRequirement already satisfied: msgpack&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mmtf-python==1.1.3-&gt;biopandas&gt;=0.5.1-&gt;graphein) (1.1.0)\n\nRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.4.4)\n\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (4.12.3)\n\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (6.9.0)\n\nRequirement already satisfied: easydev&gt;=0.12.0 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.13.2)\n\nRequirement already satisfied: grequests in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (2.32.3)\n\nRequirement already satisfied: requests-cache in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.2.1)\n\nRequirement already satisfied: suds-community&gt;=0.7 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.2.0)\n\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (5.3.0)\n\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.17.0)\n\nRequirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.14.2)\n\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (1.3.1)\n\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (0.12.1)\n\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (4.55.1)\n\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (1.4.7)\n\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (24.2)\n\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (11.0.0)\n\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (3.2.0)\n\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;graphein) (2024.2)\n\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;graphein) (2024.2)\n\nRequirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff-&gt;graphein) (5.2.2)\n\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly-&gt;graphein) (9.0.0)\n\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (2.27.1)\n\nRequirement already satisfied: typing-extensions&gt;=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (4.12.2)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;graphein) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;graphein) (2.18.0)\n\nRequirement already satisfied: click&gt;=7 in /usr/local/lib/python3.10/dist-packages (from rich-click-&gt;graphein) (8.1.7)\n\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;graphein) (1.4.2)\n\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;graphein) (3.5.0)\n\nRequirement already satisfied: colorama&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (0.4.6)\n\nRequirement already satisfied: line-profiler&lt;5.0.0,&gt;=4.1.2 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.2.0)\n\nRequirement already satisfied: pexpect&lt;5.0.0,&gt;=4.9.0 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.9.0)\n\nRequirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.2.0 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.3.6)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;graphein) (0.1.2)\n\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.4.3-&gt;graphein) (1.16.0)\n\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2.6)\n\nRequirement already satisfied: gevent in /usr/local/lib/python3.10/dist-packages (from grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.11.1)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.4.0)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2024.8.30)\n\nRequirement already satisfied: attrs&gt;=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.2.0)\n\nRequirement already satisfied: cattrs&gt;=22.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.1.2)\n\nRequirement already satisfied: url-normalize&gt;=1.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (1.4.3)\n\nRequirement already satisfied: exceptiongroup&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs&gt;=22.2-&gt;requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (1.2.2)\n\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&lt;5.0.0,&gt;=4.9.0-&gt;easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: zope.event in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (5.0)\n\nRequirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (7.2)\n\nRequirement already satisfied: greenlet&gt;=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.1.1)\n\nERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\n\nERROR: No matching distribution found for pytorch3d\n\nRequirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n\nRequirement already satisfied: psutil&gt;=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (2.4.4)\n\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.3.1)\n\nRequirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (4.0.3)\n\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (24.2.0)\n\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.5.0)\n\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (6.1.0)\n\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (0.2.1)\n\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.18.3)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-geometric) (3.0.2)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (3.4.0)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (2024.8.30)\n\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict&lt;7.0,&gt;=4.5-&gt;aiohttp-&gt;torch-geometric) (4.12.2)\n\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-&gt;torch) (1.3.0)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (3.0.2)\n\nRequirement already satisfied: biovec in /usr/local/lib/python3.10/dist-packages (0.2.7)\n\nRequirement already satisfied: gensim==3.4.0 in /usr/local/lib/python3.10/dist-packages (from biovec) (3.4.0)\n\nRequirement already satisfied: tqdm&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from biovec) (4.66.6)\n\nRequirement already satisfied: pyfasta==0.5.2 in /usr/local/lib/python3.10/dist-packages (from biovec) (0.5.2)\n\nRequirement already satisfied: numpy&gt;=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.26.4)\n\nRequirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.13.1)\n\nRequirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.16.0)\n\nRequirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (7.0.5)\n\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open&gt;=1.2.1-&gt;gensim==3.4.0-&gt;biovec) (1.17.0)\n\nRequirement already satisfied: biographs in /usr/local/lib/python3.10/dist-packages (0.1)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from biographs) (3.4.2)\n\nRequirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from biographs) (1.84)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython-&gt;biographs) (1.26.4)\n\n\n\n\n\n\nCode\n# Get libraries\nimport os\nimport re\nimport graphein\nimport graphein.protein as gp\nfrom graphein.protein.config import ProteinGraphConfig\nfrom graphein.protein.graphs import construct_graph\nfrom graphein.protein.visualisation import plotly_protein_structure_graph\n\nimport pandas as pd\n# from Bio import PDB\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport networkx as nx\nfrom networkx import to_networkx_graph\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.utils import from_networkx\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\nimport biographs as bg\nimport Bio.PDB\nfrom biopandas.pdb import PandasPdb\n# from plotly.subplots import make_subplots\n\n\n\n\nCode\n# Clone the repository with sparse checkout and depth limit\n!git clone -n --depth=1 --filter=tree:0 \\\n  https://github.com/courtel/GNN_dl_workshop.git repo_partial\n\n\n%cd repo_partial # Get into cloned repo\n!git sparse-checkout set --no-cone images # Sparse checkout to fetch only specified folder\n!git checkout\n\n%cd ..\n!mv repo_partial/images ./images\n!rm -rf repo_partial\n\n\nCloning into 'repo_partial'...\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nReceiving objects: 100% (1/1), done.\n[Errno 2] No such file or directory: 'repo_partial # Get into cloned repo'\n/\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\n/\nmv: cannot stat 'repo_partial/images': No such file or directory\n\n\n\n\n\n\nGraphs are very prevalent in the world around us. They are a powerful way to represent data because they’re able to capture complex relationships between objects. Some examples of this are social networks, transportation networks, telecommunications networks, knowledge graphs (like search engines), even words in sentences can be represented by graphs. Graphs allow for the representation of both local (node-level) and global (graph-level) information, capturing the full complexity of what is being modeled.\nMost neural networks work on Euclidean data. For example, images are linear graphs that are ideally handled by a CNN whose functions depend on the grid-like, ordered structure of the nodes and edges. GNNs can be used for nonlinear data that is more complex and less predictable, like a social network.\n\n\nCode\n# @title Hover over the nodes, edges, and graph border to see embeddings for each.\n\n\nfrom IPython.display import HTML, display\n\n# Load D3.js\ndisplay(HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v6.min.js\"&gt;&lt;/script&gt;\n\"\"\"))\n\nhtml_code = \"\"\"\n&lt;div id=\"container\"&gt;\n  &lt;div&gt;\n    &lt;/div&gt;\n    &lt;div style=\"display: flex; align-items: flex-start;\"&gt;\n      &lt;div&gt;\n        &lt;h2&gt;Grid-like Image Graph&lt;/h2&gt;\n        &lt;div id=\"grid-image\"&gt;&lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-left: 20px;\"&gt;\n        &lt;h2&gt;Social Network Graph&lt;/h2&gt;\n        &lt;div id=\"social-network-graph\"&gt;&lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;style&gt;\n  h2 {\n    text-align: center;\n  }\n\n  .node {\n    stroke: #bbb;\n    stroke-width: 1px;\n  }\n\n  .node.hovered {\n    stroke-width: 3px;\n    stroke: #1f77b4;\n  }\n\n  .edge {\n    stroke: #bbb;\n    stroke-width: 1px;\n  }\n\n  .edge.hovered {\n    stroke-width: 3px;\n    stroke: #ff7f0e;\n  }\n\n  .overlay {\n    fill: rgba(0, 0, 0, 0);\n    cursor: pointer;\n  }\n\n  .graph {\n    stroke: #bbb;\n    stroke-width: 4px;\n    fill: none;\n  }\n\n  .graph.hovered {\n    stroke: #2ca02c;\n    stroke-width: 10px;\n  }\n\n  .tooltip {\n    position: absolute;\n    background-color: #fff;\n    border: 1px solid #bbb;\n    padding: 5px;\n    border-radius: 3px;\n    pointer-events: none;\n    font-size: 12px;\n  }\n&lt;/style&gt;\n\n&lt;div class=\"tooltip\" id=\"tooltip\" style=\"display: none;\"&gt;&lt;/div&gt;\n\n&lt;script&gt;\n  function makeGridImage() {\n    const gridSize = 5;  // Change this to adjust the grid size (e.g., 5x5 grid)\n    const nodes = [];\n    const links = [];\n\n    for (let i = 0; i &lt; gridSize; i++) {\n      for (let j = 0; j &lt; gridSize; j++) {\n        const id = i * gridSize + j;\n        nodes.push({\n          id: id,\n          x: j / (gridSize - 1),\n          y: i / (gridSize - 1),\n          element: `P(${i},${j})`,\n          features: `Pixel (${i},${j})`\n        });\n\n        // Connect the node to its right neighbor\n        if (j &lt; gridSize - 1) {\n          links.push({ source: id, target: id + 1 });\n        }\n\n        // Connect the node to its bottom neighbor\n        if (i &lt; gridSize - 1) {\n          links.push({ source: id, target: id + gridSize });\n        }\n      }\n    }\n\n    const graph = { features: 'Grid Image Representation: ...' };\n\n    return [nodes, links, graph];\n  }\n\n  function makeSocialNetworkGraph() {\n    const numNodes = 50;\n    const nodes = Array.from({ length: numNodes }, (v, i) =&gt; ({\n      id: i,\n      x: Math.random(),\n      y: Math.random(),\n      name: `User ${i}`,\n      features: `Embeddings: ${Math.random().toFixed(4)}, ${Math.random().toFixed(4)}`\n    }));\n\n    const links = [];\n    for (let i = 0; i &lt; numNodes; i++) {\n      for (let j = i + 1; j &lt; numNodes; j++) {\n        if (Math.random() &lt; 0.1) {\n          links.push({\n            source: i,\n            target: j,\n            connection: 'friend',\n            features: `Connection strength: ${Math.random().toFixed(4)}`\n          });\n        }\n      }\n    }\n\n    const graph = { features: 'Social Network Graph Embeddings: ...' };\n\n    return [nodes, links, graph];\n  }\n\n  class GraphDescription {\n    constructor(elementId, makeGraphFunction) {\n      this.parent = d3.select(`#${elementId}`);\n      this.svg = this.parent.append('svg')\n                            .attr('width', 400)\n                            .attr('height', 400);\n      const [nodes, links, graph] = makeGraphFunction();\n      this.showGraph(nodes, links, graph);\n    }\n\n    showGraph(nodes, links, graph) {\n      const pos = (x) =&gt; x * 350 + 25;\n      const tooltip = d3.select('#tooltip');\n      const self = this;\n      const graphHolder = this.svg.append('g')\n                                  .attr('transform', 'translate(10, 10)');\n\n      const graphBorder = graphHolder.append('rect')\n        .attr('width', 350)\n        .attr('height', 350)\n        .attr('x', 0)\n        .attr('y', 0)\n        .attr('rx', 20)\n        .attr('class', 'graph')\n        .on('mouseover', function(event) { self.showGraphTooltip(event, graph, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      const edgeElements = graphHolder.selectAll('line.edge')\n        .data(links)\n        .enter()\n        .append('line')\n        .classed('edge', true)\n        .attr(\"x1\", (d) =&gt; pos(nodes[d.source].x))\n        .attr(\"x2\", (d) =&gt; pos(nodes[d.target].x))\n        .attr(\"y1\", (d) =&gt; pos(nodes[d.source].y))\n        .attr(\"y2\", (d) =&gt; pos(nodes[d.target].y))\n        .on('mouseover', function(event, d) { self.showEdgeTooltip(event, d, nodes, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      graphHolder.selectAll('line.overlay')\n        .data(links)\n        .enter()\n        .append('line')\n        .classed('overlay', true)\n        .attr(\"x1\", (d) =&gt; pos(nodes[d.source].x))\n        .attr(\"x2\", (d) =&gt; pos(nodes[d.target].x))\n        .attr(\"y1\", (d) =&gt; pos(nodes[d.source].y))\n        .attr(\"y2\", (d) =&gt; pos(nodes[d.target].y))\n        .on('mouseover', function(event, d) { self.showEdgeTooltip(event, d, nodes, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      const nodeElements = graphHolder.selectAll('circle.node')\n        .data(nodes)\n        .enter()\n        .append('circle')\n        .classed('node', true)\n        .attr('r', 7)\n        .attr('cx', (d) =&gt; pos(d.x))\n        .attr('cy', (d) =&gt; pos(d.y))\n        .on('mouseover', function(event, d) { self.showNodeTooltip(event, d, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      graphHolder.selectAll('circle.overlay')\n        .data(nodes)\n        .enter()\n        .append('circle')\n        .classed('overlay', true)\n        .attr('r', 12)\n        .attr('cx', (d) =&gt; pos(d.x))\n        .attr('cy', (d) =&gt; pos(d.y))\n        .on('mouseover', function(event, d) {\n          d3.select(nodeElements.nodes()[d.id]).classed('hovered', true);\n          d3.select(this).classed('hovered', true);\n          self.showNodeTooltip(event, d, tooltip);\n        })\n        .on('mouseout', function(event, d) {\n          d3.select(nodeElements.nodes()[d.id]).classed('hovered', false);\n          d3.select(this).classed('hovered', false);\n          self.hideTooltip(tooltip);\n        });\n\n      graphBorder.on('mouseover', function(event) {\n        d3.select(this).classed('hovered', true);\n        edgeElements.classed('hovered', true);\n        nodeElements.classed('hovered', true);\n        self.showGraphTooltip(event, graph, tooltip);\n      })\n      .on('mouseout', function() {\n        d3.select(this).classed('hovered', false);\n        edgeElements.classed('hovered', false);\n        nodeElements.classed('hovered', false);\n        self.hideTooltip(tooltip);\n      });\n    }\n\n    showNodeTooltip(event, d, tooltip) {\n      tooltip.style('display', 'block')\n             .style('left', (event.pageX + 5) + 'px')\n             .style('top', (event.pageY + 5) + 'px')\n             .html(`&lt;strong&gt;ID:&lt;/strong&gt; ${d.id}&lt;br&gt;&lt;strong&gt;Features:&lt;/strong&gt; ${d.features}`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    showEdgeTooltip(event, d, nodes, tooltip) {\n      const sourceNode = nodes[d.source];\n      const targetNode = nodes[d.target];\n      tooltip.style('display', 'block')\n            .style('left', (event.pageX + 5) + 'px')\n            .style('top', (event.pageY + 5) + 'px')\n            .html(`&lt;strong&gt;Source:&lt;/strong&gt; ${sourceNode.id} (${sourceNode.features})&lt;br&gt;&lt;strong&gt;Target:&lt;/strong&gt; ${targetNode.id} (${targetNode.features})`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    showGraphTooltip(event, graph, tooltip) {\n      tooltip.style('display', 'block')\n            .style('left', (event.pageX + 5) + 'px')\n            .style('top', (event.pageY + 5) + 'px')\n            .html(`&lt;strong&gt;Graph:&lt;/strong&gt; ${graph.features}`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    hideTooltip(tooltip) {\n      tooltip.style('display', 'none');\n      d3.selectAll('.hovered').classed('hovered', false);\n    }\n  }\n\n  const gridImageGraph = new GraphDescription('grid-image', makeGridImage);\n  const socialNetworkGraph = new GraphDescription('social-network-graph', makeSocialNetworkGraph);\n&lt;/script&gt;\n\"\"\"\n\ndisplay(HTML(html_code))"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#graph-theory-how-is-a-graph-defined",
    "href": "notebooks/WS07_GNNsForProteins.html#graph-theory-how-is-a-graph-defined",
    "title": "Section 1: Graph Theory",
    "section": "Graph Theory: How is a graph defined?",
    "text": "Graph Theory: How is a graph defined?\n\n\nCode\n# @title A graph **G** is a data structure that can be defined as **G = (V, E)**, where **V** represents the vertices (or nodes) and **E** represents the edges. Edges in a graph can be directed or undirected based on whether directional dependencies exist between nodes.\nfrom IPython.display import HTML, display\n\n# Load D3.js\ndisplay(HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v6.min.js\"&gt;&lt;/script&gt;\n\"\"\"))\n\nhtml_code = \"\"\"\n&lt;div id=\"container\" style=\"display: flex; align-items: flex-start;\"&gt;\n  &lt;div id=\"graph-description\"&gt;&lt;/div&gt;\n  &lt;div id=\"text-description\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;style&gt;\n  .line-holder {\n    display: flex;\n    flex-direction: column;\n    margin: 10px 0;\n    width: 100%;\n  }\n  .line-holder div {\n    margin: 5px 0;\n  }\n  .letter {\n    font-weight: bold;\n    margin-right: 10px;\n    font-size: 2em;\n  }\n  .desc {\n    font-size: 2em;\n    margin-right: 10px;\n    margin-left: 5px;\n    white-space: pre-wrap;\n  }\n  .line-holder .top-line {\n    display: flex;\n    align-items: center;\n  }\n  .eg {\n    font-size: 1.5em;\n    margin-left: 20px;\n  }\n  .selected {\n    background-color: #ddd;\n  }\n  svg {\n    margin: 20px;\n    border: 1px solid black;\n  }\n&lt;/style&gt;\n\n&lt;script&gt;\nfunction makeGraph() {\n  const nodes = [\n    { id: 0, x: 0.8, y: 0.5 },\n    { id: 1, x: 0.6, y: 0.9 },\n    { id: 2, x: 0.2, y: 0.8 },\n    { id: 3, x: 0.2, y: 0.4 },\n    { id: 4, x: 0.6, y: 0.1 }\n  ];\n  const links = [\n    { a: nodes[0], b: nodes[0] },\n    { a: nodes[0], b: nodes[4] },\n    { a: nodes[4], b: nodes[0] },\n    { a: nodes[4], b: nodes[1] },\n    { a: nodes[1], b: nodes[2] },\n    { a: nodes[2], b: nodes[1] },\n    { a: nodes[2], b: nodes[2] },\n    { a: nodes[2], b: nodes[4] },\n    { a: nodes[4], b: nodes[2] },\n    { a: nodes[3], b: nodes[3] },\n    { a: nodes[3], b: nodes[0] },\n    { a: nodes[0], b: nodes[3] },\n    { a: nodes[4], b: nodes[4] },\n    { a: nodes[4], b: nodes[1] },\n    { a: nodes[1], b: nodes[4] },\n    { a: nodes[4], b: nodes[3] },\n    { a: nodes[3], b: nodes[4] }\n  ];\n  return [nodes, links];\n}\n\nclass GraphDescription {\n  constructor() {\n    this.parent = d3.select('#graph-description');\n    this.svg = this.parent.append('svg')\n                          .attr('width', 400)\n                          .attr('height', 400);\n    const [nodes, links] = makeGraph();\n    this.showGraph(nodes, links);\n    this.showText();\n  }\n\n  showGraph(nodes, links) {\n    const pos = (x) =&gt; x * 300 + 50;\n\n    const graphHolder = this.svg.append('g')\n                                .attr('transform', 'translate(25, 25)');\n\n    // Make global box\n    graphHolder.append('rect')\n      .attr('width', 350)\n      .attr('height', 350)\n      .attr('x', 0)\n      .attr('y', 0)\n      .attr('rx', 20)\n      .attr('fill', '#fff')\n      .attr('stroke', '#ddd')\n      .style(\"stroke-width\", 2)\n      .attr('stroke-dasharray', \"4, 4\")\n      .on('mouseover', () =&gt; this.highlightGlobal())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n\n    // Make edges\n    graphHolder.selectAll('line.vis')\n      .data(links)\n      .enter()\n      .append('line')\n      .classed('vis', true)\n      .style(\"stroke\", \"#bbb\")\n      .style(\"stroke-width\", 1)\n      .attr(\"x1\", (d) =&gt; pos(d.a.x))\n      .attr(\"x2\", (d) =&gt; pos(d.b.x))\n      .attr(\"y1\", (d) =&gt; pos(d.a.y))\n      .attr(\"y2\", (d) =&gt; pos(d.b.y));\n\n    graphHolder.selectAll('line.target')\n      .data(links)\n      .enter()\n      .append('line')\n      .classed('target', true)\n      .style(\"stroke\", \"rgba(0, 0, 0, 0)\")\n      .style(\"stroke-width\", 40)\n      .attr(\"x1\", (d) =&gt; pos(d.a.x))\n      .attr(\"x2\", (d) =&gt; pos(d.b.x))\n      .attr(\"y1\", (d) =&gt; pos(d.a.y))\n      .attr(\"y2\", (d) =&gt; pos(d.b.y))\n      .on('mouseover', () =&gt; this.highlightEdges())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n\n    // Make nodes\n    graphHolder.selectAll('circle')\n      .data(nodes)\n      .enter()\n      .append('circle')\n      .attr('r', 10)\n      .attr('cx', (d) =&gt; pos(d.x))\n      .attr('cy', (d) =&gt; pos(d.y))\n      .style('fill', '#fff')\n      .style(\"stroke-width\", '1px')\n      .style(\"stroke\", '#bbb')\n      .on('mouseover', () =&gt; this.highlightNodes())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n  }\n\n  showText() {\n    const textHolder = d3.select('#text-description');\n\n    const makeLine = (letter, desc, eg, mouseover) =&gt; {\n      const div = textHolder.append('div')\n        .classed('line-holder', true)\n        .attr('id', letter);\n      const topLine = div.append('div').classed('top-line', true);\n      topLine.append('div').text(letter).classed('letter', true);\n      topLine.append('div').text(desc).classed('desc', true);\n      div.append('div').text(eg).classed('eg', true);\n\n      div.on('mouseover', mouseover);\n      div.on('mouseout', () =&gt; this.unhighlightAll());\n    }\n\n    makeLine('V', ' Vertex (or node) attributes', 'e.g., node identity, number of neighbors', () =&gt; this.highlightNodes());\n    makeLine('E', ' Edge attributes and directions', 'e.g., edge identity, edge weight', () =&gt; this.highlightEdges());\n    makeLine('G', ' Graph attributes', 'e.g., number of nodes, longest path', () =&gt; this.highlightGlobal());\n  }\n\n  highlightEdges() {\n    d3.select('#E').classed('selected', true);\n    this.parent.selectAll('line.vis')\n      .style(\"stroke\", \"#000\")\n      .style(\"stroke-width\", 10);\n  }\n\n  highlightNodes() {\n    d3.select('#V').classed('selected', true);\n    this.parent.selectAll('circle')\n      .style(\"stroke-width\", 6)\n      .style(\"stroke\", '#000')\n      .attr(\"r\", 11);\n  }\n\n  highlightGlobal() {\n    d3.select('#G').classed('selected', true);\n    this.parent.selectAll('rect')\n      .style(\"stroke\", '#000')\n      .style(\"stroke-width\", 40);\n  }\n\n  unhighlightAll() {\n    d3.selectAll('.line-holder').classed('selected', false);\n    this.parent.selectAll('line.vis')\n      .style(\"stroke\", \"#bbb\")\n      .style(\"stroke-width\", '1px');\n\n    this.parent.selectAll('circle')\n      .style(\"stroke-width\", '1px')\n      .style(\"stroke\", '#aaa')\n      .attr(\"r\", 10);\n\n    this.parent.selectAll('rect')\n      .style(\"stroke-width\", 2)\n      .style(\"stroke\", '#ddd');\n  }\n}\n\nconst graph = new GraphDescription();\n&lt;/script&gt;\n\"\"\"\n\ndisplay(HTML(html_code))\n\n# adapted from https://distill.pub/2021/gnn-intro/\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\nAdapted from: A Gentle Introduction to Graph Neural Networks"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#graph-theory-graph-prediction-tasks",
    "href": "notebooks/WS07_GNNsForProteins.html#graph-theory-graph-prediction-tasks",
    "title": "Section 1: Graph Theory",
    "section": "Graph Theory: Graph Prediction Tasks",
    "text": "Graph Theory: Graph Prediction Tasks\nWhat kinds of problems can we solve on graphs?\n\nNode‑level tasks involve classifying the nodes in a graph.\nEdge-level tasks (or link prediction) involve the relationships between pairs of nodes in a graph.\nGraph‑level tasks involve properties that encompass the entire graph.\n\n\n\nCode\n# @markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\nimport json  # Required for converting Python dictionaries to JSON strings\n\n# Words and definitions to match\nwords = [\"Node-level task\", \"Edge-level task\", \"Graph-level task\"]\ndefinitions = [\n    \"Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\",\n    \"Predict the overall stability of the protein.\",\n    \"Predict the B-factor for each residue.\",\n]\n\ncorrect_definitions = {\n    \"Node-level task\": \"Predict the B-factor for each residue.\",\n    \"Edge-level task\": \"Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\",\n    \"Graph-level task\": \"Predict the overall stability of the protein.\",\n}\n\nexplanation = \"\"\"\n&lt;p&gt;&lt;/p&gt;\n&lt;ol&gt;\n    &lt;li&gt;&lt;strong&gt;Node-level task:&lt;/strong&gt; Predict the B-factor for each residue.&lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Edge-level task:&lt;/strong&gt; Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).&lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Graph-level task:&lt;/strong&gt; Predict the overall stability of the protein.&lt;/li&gt;\n&lt;/ol&gt;\n\"\"\"\n\n# Create the matching quiz with dropdowns\ndef create_matching_quiz(words, definitions, explanation):\n    html_code = \"&lt;h3&gt;Match the problem to solve with the correct task level:&lt;/h3&gt;\"\n\n    for i, word in enumerate(words):\n        html_code += f\"&lt;p&gt;&lt;strong&gt;{word}:&lt;/strong&gt; \" \\\n                     f\"&lt;select id='dropdown_{i}'&gt;\" \\\n                     f\"&lt;option value='' disabled selected&gt;Select task&lt;/option&gt;\"\n        for definition in definitions:\n            html_code += f\"&lt;option value='{definition}'&gt;{definition}&lt;/option&gt;\"\n        html_code += \"&lt;/select&gt;&lt;/p&gt;\"\n\n    # Add submit button, feedback, and explanation\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        {explanation}\n      &lt;/div&gt;\n    &lt;script&gt;\n    // JavaScript function to check answers\n    function checkAnswers() {{\n        // Pass the Python data into JavaScript variables\n        var correct_answers = {json.dumps(correct_definitions)};\n        var feedback = document.getElementById('feedback');\n        var correct = true;\n\n        // Loop through each dropdown and check if the selected answer is correct\n        Object.keys(correct_answers).forEach(function(word, index) {{\n            var dropdown = document.getElementById('dropdown_' + index);\n            var selected_answer = dropdown.value;\n\n            if (selected_answer === correct_answers[word]) {{\n                dropdown.style.color = \"green\";\n            }} else {{\n                dropdown.style.color = \"red\";\n                correct = false;\n            }}\n        }});\n\n        // Update feedback based on correctness\n        if (correct) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect. Try again.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n\n    // JavaScript function to show explanation\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Call the function to create the matching quiz\ncreate_matching_quiz(words, definitions, explanation)\n\n\nMatch the problem to solve with the correct task level:Node-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.Edge-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.Graph-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.\n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \n\n\n    Node-level task: Predict the B-factor for each residue.\n    Edge-level task: Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\n    Graph-level task: Predict the overall stability of the protein."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#graph-theory-representation-as-an-adjacency-matrix",
    "href": "notebooks/WS07_GNNsForProteins.html#graph-theory-representation-as-an-adjacency-matrix",
    "title": "Section 1: Graph Theory",
    "section": "Graph Theory: Representation as an Adjacency Matrix",
    "text": "Graph Theory: Representation as an Adjacency Matrix\nOne of the most common ways to computationally represent a graph for nn is by using an adjacency matrix. This is a binary matrix \\((A)\\) of size n x n where each element A[i][j] is 1 if there is an edge between nodes i and j, and 0 otherwise. In this tutorial, we will be using adjacency matrices.\n\n\n\n\n\n\n\\[\\begin{array}{l|llll}\n    & n_0 & n_1 & n_2 & n_3 \\\\ \\hline\nn_0 & 0    & 1    & 0    & 1    \\\\\nn_1 & 0    & 0    & 1    & 0    \\\\\nn_2 & 1    & 0    & 0    & 0    \\\\\nn_3 & 1    & 0    & 0    & 0\n\\end{array}\\]\n\n\n\n\n\nCode\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph\nG = nx.DiGraph()\nG.add_edges_from([\n    (0, 1),\n    (0, 2),\n    (1, 2),\n    (1, 3),\n    (2, 3),\n    (3, 0),\n    (3, 1)\n])\n\n# Draw the graph\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=700, arrows=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    # random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Import necessary libraries\nfrom IPython.display import display, HTML\n\n# Base64 encoded image string (your full string goes here)\nimage_base64 = \"data:image/png;base64,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\"\n\n# HTML to embed the image\nhtml_code = f'&lt;img src=\"{image_base64}\" style=\"width:800px; height:auto;\"/&gt;'\n\n# Display the image\ndisplay(HTML(html_code))\n\n\n# Example usage\nquestion = \"Which of the following is the correct adjacency matrix for the graph?\"\ncorrect_answer = \"A\"\ndecoy_answers = [\"B\", \"C\", \"D\"]\n\nexplanation = \"\"\"\n&lt;p&gt;The adjacency matrix &lt;em&gt;A&lt;/em&gt; for a directed graph is defined such that &lt;em&gt;A&lt;sub&gt;ij&lt;/sub&gt; = 1&lt;/em&gt; if there is a directed edge from node &lt;em&gt;i&lt;/em&gt; to node &lt;em&gt;j&lt;/em&gt;, and &lt;em&gt;A&lt;sub&gt;ij&lt;/sub&gt; = 0&lt;/em&gt; otherwise.&lt;/p&gt;\n\n&lt;ol&gt;\n    &lt;li&gt;&lt;strong&gt;Node 0:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 0 to nodes 1 and 2, so &lt;em&gt;A&lt;sub&gt;01&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;02&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 0 to node 3, so &lt;em&gt;A&lt;sub&gt;03&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 1:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 1 to nodes 2 and 3, so &lt;em&gt;A&lt;sub&gt;12&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;13&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 1 to node 0, so &lt;em&gt;A&lt;sub&gt;10&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 2:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There is an edge from node 2 to node 3, so &lt;em&gt;A&lt;sub&gt;23&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 2 to nodes 0 and 1, so &lt;em&gt;A&lt;sub&gt;20&lt;/sub&gt; = 0&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;21&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 3:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 3 to nodes 0 and 1, so &lt;em&gt;A&lt;sub&gt;30&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;31&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 3 to node 2, so &lt;em&gt;A&lt;sub&gt;32&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thus, the correct adjacency matrix is choice A:&lt;/p&gt;\n\"\"\"\n\n# Base64 encoded image string (replace this with your actual base64 string)\nimage_base64 = \"&lt;base64_string&gt;\"\n\n# Call the function to create and display the quiz with the embedded image\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n\n\n\n    \n      Which of the following is the correct adjacency matrix for the graph?\n    \n        \n          \n          A\n        \n        \n        \n          \n          B\n        \n        \n        \n          \n          C\n        \n        \n        \n          \n          D\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \nThe adjacency matrix A for a directed graph is defined such that Aij = 1 if there is a directed edge from node i to node j, and Aij = 0 otherwise.\n\n\n    Node 0:\n        \n            There are edges from node 0 to nodes 1 and 2, so A01 = 1 and A02 = 1.\n            There are no edges from node 0 to node 3, so A03 = 0.\n        \n    \n    Node 1:\n        \n            There are edges from node 1 to nodes 2 and 3, so A12 = 1 and A13 = 1.\n            There are no edges from node 1 to node 0, so A10 = 0.\n        \n    \n    Node 2:\n        \n            There is an edge from node 2 to node 3, so A23 = 1.\n            There are no edges from node 2 to nodes 0 and 1, so A20 = 0 and A21 = 0.\n        \n    \n    Node 3:\n        \n            There are edges from node 3 to nodes 0 and 1, so A30 = 1 and A31 = 1.\n            There are no edges from node 3 to node 2, so A32 = 0.\n        \n    \n\n\nThus, the correct adjacency matrix is choice A:"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#graph-theory-directed-and-undirected-graphs",
    "href": "notebooks/WS07_GNNsForProteins.html#graph-theory-directed-and-undirected-graphs",
    "title": "Section 1: Graph Theory",
    "section": "Graph Theory: Directed and Undirected Graphs",
    "text": "Graph Theory: Directed and Undirected Graphs\nEdges in a graph can be directed or undirected based on whether directional dependencies exist between nodes. In a directed graph, edges have a source node and a destination node. In an undirected graph, information flows in both directions which allows it to have symmetry. This means that in an undirected graph, A[i][j] = A[j][i], or in other words, having an undirected edge between n0 to n1 is the same as having two directed edges from n0 to n1 and n1 to n0.\nBelow is an example of each, with arrows indicating direction.\n\n\nCode\n# @title The graph on the left is directed, with 4 nodes and 5 edges. Note that there is an edge from node 0 to 1, but not from node 1 to 0. The graph on the right is undirected.\n\n\n# Create a directed graph (using DiGraph) with 4 nodes and 5 edges\nG_directed = nx.DiGraph()\nG_directed.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)])\n\n# Create an undirected graph with the same 4 nodes but undirected edges\nG_undirected = nx.Graph()\nG_undirected.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)])\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\n\n# Plot directed graph on the left\npos = nx.spring_layout(G_directed)\nnx.draw(G_directed, pos, ax=axes[0], with_labels=True, node_color='lightblue', node_size=700, arrows=True)\naxes[0].set_title(\"Directed Graph\")\n\n# Print directed adjacency matrix\nA_directed = nx.adjacency_matrix(G_directed).todense()\ndirected_matrix = (f'Adjacency matrix:\\n{A_directed}\\n')\n# Adjust the ha parameter to 'left' for left alignment\naxes[0].text(0.05, -0.2, directed_matrix, ha='left', va='center', transform=axes[0].transAxes, fontsize=10)\n\n# Plot undirected graph on the right\nnx.draw(G_undirected, pos, ax=axes[1], with_labels=True, node_color='lightgreen', node_size=700, arrows=True)\naxes[1].set_title(\"Undirected Graph\")\n\n# Print undirected adjacency matrix\nA_undirected = nx.adjacency_matrix(G_undirected).todense()\nundirected_matrix = (f'Adjacency matrix (symmetric):\\n{A_undirected}\\n')\n# Adjust the ha parameter to 'left' for left alignment\naxes[1].text(0.05, -0.2, undirected_matrix, ha='left', va='center', transform=axes[1].transAxes, fontsize=10)\n\n# Subplots so they plot side by side\nplt.subplots_adjust(bottom=0.3)\n\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTask: try changing an edge in the directed graph (named G_directed) from directed to undirected\nHint: NetworkX has a built in function to add edges to a graph. Try using add_edge.\n\n\nClick me to for more help\n\nMake the edge from 0 -&gt; 1 undirected by adding the reverse edge 1 -&gt; 0\nG_directed.add_edge(1, 0)\n\n\nCode\n# Before updating edge\nprint(\"Original edges:\", list(G_directed.edges))\n\n# YOUR CODE HERE\n# raise NotImplementedError()\n\n# Check after updating edge\nprint(\"Updated edges:\", list(G_directed.edges))\nnx.draw(G_directed, with_labels=True, node_color='lightblue', node_size=400, arrows=True)\n\n\nOriginal edges: [(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)]\nUpdated edges: [(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)]\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How many edges did the graph have before? How many are there now?\"\ncorrect_answer = \"There were 5 edges before, and now there are 6.\"\ndecoy_answers = [\n    \"There were 5 edges before, and now there are 5.\",\n    \"There were 5 edges before, and now there are 7.\",\n    \"There were 5 edges before, and now there are 4.\"\n]\n\nexplanation = \"Having an undirected edge between n0 to n1 is the same as having two directed edges from n0 to n1 and n1 to n0. When the directed edge was replaced with an undirected edge, it added a reverse edge between the same two nodes.\"\n\n\n# Create the quiz\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How many edges did the graph have before? How many are there now?\n    \n        \n          \n          There were 5 edges before, and now there are 6.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 5.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 7.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 4.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Having an undirected edge between n0 to n1 is the same as having two directed edges from n0 to n1 and n1 to n0. When the directed edge was replaced with an undirected edge, it added a reverse edge between the same two nodes."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#graph-theory-further-define-nodes-and-edges-with-feature-embeddings",
    "href": "notebooks/WS07_GNNsForProteins.html#graph-theory-further-define-nodes-and-edges-with-feature-embeddings",
    "title": "Section 1: Graph Theory",
    "section": "Graph Theory: Further define nodes and edges with feature embeddings",
    "text": "Graph Theory: Further define nodes and edges with feature embeddings\nGraphs can further be defined by adding node, feature, or graph embeddings. We can use the graphein package to add protein edge and node features and visualize them with an interactive graph.\nBelow is a set of defined parameters. We can use these to build protein graphs and visualize the node and edge features that are being added.\nGraphein’s ProteinGraphConfigs takes in a dictionary of lists of construction functions to define how the node, edge, and overall graph should be constructed.\nGraph and edge metadata functions take in a nx.Graph object and return the same nx.Graph object but with added node features. Node metadata functions take in a node, data tuple from G.nodes(data=True) and returns a pd.Series.\n\n\nCode\n# @title graphein graph\n\nfrom IPython.display import display\nfrom graphein.protein.graphs import construct_graph\nfrom graphein.protein.visualisation import plotly_protein_structure_graph\nfrom functools import partial\n\nparams_1 = {\n    \"granularity\": \"CA\",\n    \"edge_construction_functions\": [\n        gp.add_peptide_bonds,\n    ],\n    \"node_metadata_functions\": [\n        gp.amino_acid_one_hot,\n        gp.meiler_embedding,\n        partial(gp.expasy_protein_scale, add_separate=True)\n    ],\n}\n\nconfig_1 = gp.ProteinGraphConfig(**params_1)\n\nprotein_graph_1 = construct_graph(config=config_1, pdb_code='1L2Y')\n\nplot_1 = plotly_protein_structure_graph(\n    protein_graph_1,\n    colour_edges_by=\"kind\",\n    colour_nodes_by=\"degree\",\n    label_node_ids=False,\n    plot_title=\"\",\n    node_size_multiplier=1\n    )\n\ndef convert_to_networkx_and_print_info(graph):\n    # Convert to NetworkX graph\n    nx_graph = nx.Graph(graph)\n\n    # Number of nodes and edges\n    n_node = nx_graph.number_of_nodes()\n    n_edge = nx_graph.number_of_edges()\n\n    # Node and edge features (assuming attributes are stored as dicts)\n    node_features = nx.get_node_attributes(nx_graph, \"features\")  # Replace \"features\" with the correct attribute if needed\n    edge_features = nx.get_edge_attributes(nx_graph, \"features\")  # Replace \"features\" with the correct attribute if needed\n\n    # Edges\n    senders = list(nx_graph.edges())\n    receivers = [(v, u) for u, v in senders]  # For undirected graphs, this is the reverse\n\n    # Global features (if applicable)\n    global_context = nx_graph.graph.get(\"globals\", None)\n\n    # Create the information string\n    info = (\n        f\"Number of nodes: {n_node}\\n\"\n        f\"Number of edges: {n_edge}\\n\"\n        # f\"Node features: {node_features}\\n\"\n        # f\"Edge features: {edge_features}\\n\"\n        f\"Edges (senders): {senders}\\n\"\n        f\"Edges (receivers): {receivers}\\n\"\n        # f\"Graph-level features (globals): {global_context}\"\n    )\n\n    return info\n\n  # Convert to NetworkX and gather info strings\ninfo_1 = convert_to_networkx_and_print_info(protein_graph_1)\n\nprint(\"Graph 1 Information:\")\nprint(info_1)\n\n\n\n\n\n\n\n\nGraph 1 Information:\nNumber of nodes: 20\nNumber of edges: 19\nEdges (senders): [('A:ASN:1', 'A:LEU:2'), ('A:LEU:2', 'A:TYR:3'), ('A:TYR:3', 'A:ILE:4'), ('A:ILE:4', 'A:GLN:5'), ('A:GLN:5', 'A:TRP:6'), ('A:TRP:6', 'A:LEU:7'), ('A:LEU:7', 'A:LYS:8'), ('A:LYS:8', 'A:ASP:9'), ('A:ASP:9', 'A:GLY:10'), ('A:GLY:10', 'A:GLY:11'), ('A:GLY:11', 'A:PRO:12'), ('A:PRO:12', 'A:SER:13'), ('A:SER:13', 'A:SER:14'), ('A:SER:14', 'A:GLY:15'), ('A:GLY:15', 'A:ARG:16'), ('A:ARG:16', 'A:PRO:17'), ('A:PRO:17', 'A:PRO:18'), ('A:PRO:18', 'A:PRO:19'), ('A:PRO:19', 'A:SER:20')]\nEdges (receivers): [('A:LEU:2', 'A:ASN:1'), ('A:TYR:3', 'A:LEU:2'), ('A:ILE:4', 'A:TYR:3'), ('A:GLN:5', 'A:ILE:4'), ('A:TRP:6', 'A:GLN:5'), ('A:LEU:7', 'A:TRP:6'), ('A:LYS:8', 'A:LEU:7'), ('A:ASP:9', 'A:LYS:8'), ('A:GLY:10', 'A:ASP:9'), ('A:GLY:11', 'A:GLY:10'), ('A:PRO:12', 'A:GLY:11'), ('A:SER:13', 'A:PRO:12'), ('A:SER:14', 'A:SER:13'), ('A:GLY:15', 'A:SER:14'), ('A:ARG:16', 'A:GLY:15'), ('A:PRO:17', 'A:ARG:16'), ('A:PRO:18', 'A:PRO:17'), ('A:PRO:19', 'A:PRO:18'), ('A:SER:20', 'A:PRO:19')]\n\n\n\nHover over the nodes and edges in the protein graph below to see the feature embeddings.\n\n\nCode\ndisplay(plot_1)\n\n\n\n\n\n                                \n                                            \n\n\n\n\nIt is common to convert the embedded features of nodes and edges into feature matrices for input into a GNN. The feature vector for each node or edge is a numerical representation of its characteristics. Although it’s a vector of numbers, these numbers encode meaningful information, such as properties of an atom, in a protein graph. The GNN will learn the features from these feature vectors."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gnn-theory-message-passing",
    "href": "notebooks/WS07_GNNsForProteins.html#gnn-theory-message-passing",
    "title": "Section 1: Graph Theory",
    "section": "GNN Theory: Message Passing",
    "text": "GNN Theory: Message Passing\n\n Message Passing\nA simple message-passing equation for a Graph Convolutional Network (GCN) can be represented as:"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-load-and-prep-the-input-data",
    "href": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-load-and-prep-the-input-data",
    "title": "Section 1: Graph Theory",
    "section": "Implement a GCN: Load and Prep the Input Data",
    "text": "Implement a GCN: Load and Prep the Input Data\nWe will use the PSCDB dataset from Amemiya et al. (2011), which contains the pdb files from 839 paired protein structures in their bound and unbound forms across 7 classes of structural rearrangement motion: coupled domain motion, independent domain motion, coupled local motion, independent local motion, burying ligand motion, no significant motion, and other type motion. \nThis dataset has been filtered to only include the unbound pdb files from the following motion types: independent domain motion, independent local motion, and no significant motion.\n\n\n\nScreenshot 2024-09-17 at 12.33.12 PM.png\n\n\nOur goal with our GCN is to create a classification model to predict binding type from the protein structure.\nImage adapted from article: PSCDB: A database for protein structural change upon ligand binding\n\n\nCode\n# Get the .pdb files\n!git clone https://github.com/courtel/GNN_dl_workshop\n\n# Load the DataFrame from the CSV file that contains motion info\ndf_pscdb = pd.read_csv('/GNN_dl_workshop/df_pscdb.csv')\n\n# Display the DataFrame to confirm it's loaded correctly\nprint(df_pscdb.head())\n\n\nfatal: destination path 'GNN_dl_workshop' already exists and is not an empty directory.\n   Unnamed: 0  level_0  index PSCID  \\\n0           1        1      1  CD.2   \n1           4        4      4  CD.5   \n2           5        5      5  CD.6   \n3           6        6      6  CD.7   \n4           8        8      8  CD.9   \n\n                                   Protein Name Free form Bound form  Ligands  \\\n0                              ADENYLATE KINASE    4ake_A     2eck_A  ADP,AMP   \n1                           ELONGATION FACTOR 2    1n0v_D     1n0u_A      SO1   \n2                              CITRATE SYNTHASE   5csc_AB    6cts_AA    2xCIC   \n3                  ARACHIDONATE 15-LIPOXYGENASE    2p0m_A     2p0m_B      RS7   \n4  3-PHOSPHOSHIKIMATE 1-CARBOXYVINYLTRANSFERASE    2bjb_A     2o0d_A      S3P   \n\n   Classification(?)            motion_type free_pdb Free Chains Bound PDB  \\\n0             200003  coupled_domain_motion     4ake           A      2eck   \n1             110002  coupled_domain_motion     1n0v           D      1n0u   \n2             102001  coupled_domain_motion     5csc          AB      6cts   \n3             101201  coupled_domain_motion     2p0m           A      2p0m   \n4             101101  coupled_domain_motion     2bjb           A      2o0d   \n\n  Bound Chains  \n0            A  \n1            A  \n2           AA  \n3            B  \n4            A  \n\n\n\n\nCode\n# @title **Data Prep Functions**\n\n# Define a function to map motion class\nmotion_type_mapping = {row['free_pdb']: row['motion_type'] for _, row in df_pscdb.iterrows()}\n# print(\"motion_type_mapping\", motion_type_mapping)\n\n# Convert motion types to numerical labels\nmotion_type_to_label = {motion_type: idx for idx, motion_type in enumerate(df_pscdb['motion_type'].unique())}\n\ndef classification_mapping(df_pscdb, data):\n    protein_name = data.name\n    motion_type = motion_type_mapping[protein_name]\n    # print(\"motion type\", motion_type)\n    graph_label = motion_type_to_label[motion_type]\n    # print(\"graph label\", graph_label)\n    # data.graph_y = torch.tensor([graph_label])\n    data.graph_y = graph_label\n    # print(\"graph_y\", data.graph_y)\n    return data\n\n# Define a function to parse pdb and convert to networkx then to pytorch object\ndef get_distance_matrix(coords):\n    diff_tensor = np.expand_dims(coords, axis=1) - np.expand_dims(coords, axis=0)\n    distance_matrix = np.sqrt(np.sum(np.power(diff_tensor, 2), axis=-1))\n    return distance_matrix\n\ndef pdb_to_graph(pdb_path, distance_threshold=6.0, contain_b_factor=True):\n    atom_df = PandasPdb().read_pdb(pdb_path)\n    atom_df = atom_df.df['ATOM']\n    residue_df = atom_df.groupby('residue_number', as_index=False)[['x_coord', 'y_coord', 'z_coord', 'b_factor']].mean().sort_values('residue_number')\n    coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n    distance_matrix = get_distance_matrix(coords)\n    adj = distance_matrix &lt; distance_threshold\n    u, v = np.nonzero(adj)\n    u, v = torch.from_numpy(u), torch.from_numpy(v)\n    graph = nx.Graph()\n\n    # Add nodes with coordinates and b_factor included in the node feature matrix 'x'\n    for i, row in residue_df.iterrows():\n        node_features = [row['x_coord'], row['y_coord'], row['z_coord']]\n        if contain_b_factor:\n            node_features.append(row['b_factor'])  # Append b_factor to the features\n        graph.add_node(i, x=torch.tensor(node_features, dtype=torch.float))\n\n    # Add edges based on the adjacency matrix\n    for src, dst in zip(u.numpy(), v.numpy()):\n        distance = distance_matrix[src, dst]\n        graph.add_edge(src, dst, edge_attr=torch.tensor([distance], dtype=torch.float))\n\n    return graph\n\n\ndef get_pmolecule(file_path, distance_threshold=6.0, contain_b_factor=True, train=True):\n    # Create a graph using pdb_to_graph\n    graph = pdb_to_graph(file_path, distance_threshold, contain_b_factor)\n\n    # Convert to PyTorch data object\n    data = from_networkx(graph)\n\n    # Set the name attribute manually\n    data.name = os.path.basename(file_path).replace('.pdb', '')\n\n    # Check that there are feats\n    if 'x' not in data:\n        print(\"warning, no x node features\")\n\n    if 'edge_attr' not in data:\n        print(\"warning, no edge attributes\")\n\n    # Add labels for motion_type\n    data = classification_mapping(df_pscdb, data)\n\n    return data\n\ndef get_pyg_data_list(pdb_directory, pdb_id=None, distance_threshold=6.0, contain_b_factor=True, train=True):\n    pdb_files = [f for f in os.listdir(pdb_directory) if f.endswith('.pdb')]\n    pyg_data_list = []\n\n    if train:\n        for pdb_file in pdb_files:\n            file_path = os.path.join(pdb_directory, pdb_file)\n            graph = get_pmolecule(file_path, distance_threshold, contain_b_factor)\n            pyg_data_list.append(graph)\n        return pyg_data_list\n    else:\n        file_path = os.path.join(pdb_directory, f'{pdb_id}.pdb')\n        pyg_data = get_pmolecule(file_path, distance_threshold, contain_b_factor, train=False)\n        return pyg_data\n\n\nPreparing the .pdb files as input for our GCN involves extracting information to create a graph of nodes and edges that represent our original protein. The data is then converted into a pytorch data object where features are represented as tensors.\nBelow is an example using a simple three-residue protein of the data prep workflow using the functions above.\n\n\n\nmatrix_flow.jpg\n\n\n\n\nCode\n# Create directories to store train and test sets\ntrain_pdb_dir = '/GNN_dl_workshop/pdb_files/train_set'\ntest_pdb_dir = '/GNN_dl_workshop/pdb_files/test_set'\n\n# Run code to put our .pdb files through the data prep workflow\ntrain_pyg_data_list = get_pyg_data_list(train_pdb_dir)\ntest_pyg_data_list = get_pyg_data_list(test_pdb_dir)\n\n\nDouble check our pytorch data objects (our final input for the GCN) contains the data we expect\n\n\nCode\n# Double check pyg_data\nfor i, pyg_data in enumerate(train_pyg_data_list):\n    if i &gt;= 5:\n        break\n    print(pyg_data)\n\n\nData(x=[527, 4], edge_index=[2, 2749], edge_attr=[2749, 1], name='2o1p', graph_y=1)\nData(x=[282, 4], edge_index=[2, 24478], edge_attr=[24478, 1], name='1rif', graph_y=1)\nData(x=[421, 4], edge_index=[2, 36621], edge_attr=[36621, 1], name='1sry', graph_y=3)\nData(x=[233, 4], edge_index=[2, 1417], edge_attr=[1417, 1], name='2z42', graph_y=5)\nData(x=[469, 4], edge_index=[2, 2547], edge_attr=[2547, 1], name='1bbw', graph_y=1)\n\n\nHere we can see that the first protein has a feature matrix (x) that contains 217 nodes (residues) with 4 attributes (X, Y, Z coords, and b_factor). Edge_index is a matrix with 1133 connections (edges) between nodes. This is our adjacency matrix. Edge_attr is our distance matrix and has 1 attribute (distance) for all 1133 edges. Graph_y is a number that maps back to the motion type classification.\n\n\nCode\n# @title Plot graphein graph of the first protein in the dataset, 2nrt. Here we can confirm that our pytorch data objecthas the correct number of nodes. If you hover over each node in the graph, you will see X, Y, Z coords. We should have something similar in the each of the matrices for the 217 nodes in our feature matrix.\nparams_1 = {\n    \"edge_construction_functions\": [\n        gp.add_peptide_bonds\n    ]\n}\n\n# Define a graphein graph config\nconfig = ProteinGraphConfig(granularity=\"CA\", **params_1)\n# Construct the protein graph\nprotein_graph = construct_graph(config=config, pdb_code='2nrt')\n\ng = construct_graph(config=config, pdb_code='2nrt')\np = plotly_protein_structure_graph(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nCode\n# @title Functions for model metrics (show code to see detail)\ndef log_metrics(epoch, optimizer_step, train_loss, train_accuracy, log_file='train_metrics_log.txt'):\n    with open(log_file, 'a') as f:\n        f.write(f'epoch: {epoch}, optimizer_step: {optimizer_step}, train_loss: {train_loss}, train_accuracy: {train_accuracy}\\n')\n\ndef get_results(log_file, number=4):\n    with open(log_file, 'r') as file:\n        lines = file.readlines()\n    results = []\n    for line in lines:\n        if line.startswith('epoch'):\n            result_1 = [_.start() for _ in re.finditer(':', line)]\n            result_2 = [_.start() for _ in re.finditer(',', line)] + [-1]\n            bla = []\n            for i in range(number):\n                if i == number-1:\n                    bla.append(float(line[result_1[i]+2:]))\n                else:\n                    bla.append(float(line[result_1[i]+2:result_2[i]]))\n            results.append(bla)\n    results = np.array(results)\n    return results\n\ndef plot_metrics(log_file='train_metrics_log.txt'):\n    metrics = get_results(log_file)\n    epochs = metrics[:, 0]\n    train_loss = metrics[:, 2]\n    train_accuracy = metrics[:, 3]\n\n    plt.figure(figsize=(20, 8))\n\n    # Plot Loss vs Epochs\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, '--', linewidth=2.0, c='C0')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    plt.legend(['Training Loss'])\n\n    # Plot Accuracy vs Epochs\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracy, linewidth=4.0, c='C1')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.ylim(0.0, 1.0)  # Set the y-axis scale from 0.0 to 1.0\n    plt.grid(True)\n    plt.legend(['Training Accuracy'])\n\n    plt.show()"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-define-a-graphconv-layer",
    "href": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-define-a-graphconv-layer",
    "title": "Section 1: Graph Theory",
    "section": "Implement a GCN: Define a GraphConv layer",
    "text": "Implement a GCN: Define a GraphConv layer"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-architecture",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-architecture",
    "title": "Section 1: Graph Theory",
    "section": "GCN Architecture",
    "text": "GCN Architecture\n\n1. Input Layer\n\nNode Feature Matrix (X): This matrix has dimensions n x D where n is the number of nodes in the graph and D is the number of features per node. Each row represents the feature vector of a node.\nAdjacency Matrix (A): This is an n x n matrix where each element A[i][j] is 1 if there is an edge between nodes i and j, and 0 otherwise. This matrix captures the graph structure.\n\n\n\n2. Graph Convolutional Layers (GCN)\n\nAggregation and Updating Nodes: Messaging passing and aggregation for a node’s nearest neighbors (based on the adjacency matrix) take splace so that all nodes are updated based on it’s neighbors features. These features are transforned by weights specific to each layer.\nNon-linearity: A non-linear function, like ReLU, is applied to the output.\n\n\n\n3. Fully Connected Layer\n\nThe final node features are passed through a fully connected layer which maps the features used in the GCN layers to the output dimensionality needed for the specific task (e.g., classification, regression).\n\n\n\n4. Pooling Layer (depends on task)\n\nDepending on the specific task needed, a pooling layer can be applied (ie for graph-level tasks).\n\n\n\n5. Output Layer\n\nOutput: The output layer produces a vector of length n x 1, where each element corresponds to the final output for each node. This could represent a score, a classification label, or any other node-specific output depending on the specific application of the GCN.\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\ndef create_checkbox_quiz(question, correct_answers, decoy_answers, explanation):\n    html_code = f\"&lt;h3&gt;{question}&lt;/h3&gt;\"\n\n    # Create checkboxes for the correct answers and decoy answers\n    answers = correct_answers + decoy_answers\n    for i, answer in enumerate(answers):\n        html_code += f\"&lt;p&gt;&lt;input type='checkbox' id='checkbox_{i}' value='{answer}'&gt; {answer}&lt;/p&gt;\"\n\n    # Add submit button, feedback, and explanation\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkCheckboxAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n      &lt;h4&gt;Explanation:&lt;/h4&gt;\n      &lt;p&gt;{explanation}&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkCheckboxAnswers() {{\n        var correct_answers = {correct_answers};\n        var selected_answers = [];\n        var checkboxes = document.querySelectorAll(\"input[type='checkbox']\");\n\n        checkboxes.forEach((checkbox, i) =&gt; {{\n            if (checkbox.checked) {{\n                selected_answers.push(checkbox.value);\n            }}\n        }});\n\n        var feedback = document.getElementById('feedback');\n        var all_correct = selected_answers.length === correct_answers.length &&\n                          correct_answers.every(answer =&gt; selected_answers.includes(answer));\n\n        if (all_correct) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect. Try again.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Define the question, correct answers, decoy answers, and explanation\nquestion = \"Which components stored in our pytorch data objects will get updated after message passing?\"\ncorrect_answers = [\"x\", \"edge_attr\"]\ndecoy_answers = [\"edge_index\", \"name\", \"graph_y\"]\n\nexplanation = \"The 'x' is the node feature matrix and 'edge_attr' is the distance matrix which is our edge feature matrix. &lt;br&gt; Edge_index is our adjacency matrix which is used in message passing, but this matrix will stay the same. &lt;br&gt; The name (name of the protein) and graph_y (the motion label) will not change during message passing.\"\n\n# Create the quiz\ncreate_checkbox_quiz(question, correct_answers, decoy_answers, explanation)\n\n\nWhich components stored in our pytorch data objects will get updated after message passing? x edge_attr edge_index name graph_y\n    Submit\n    Show Explanation\n    \n    \n      Explanation:\n      The 'x' is the node feature matrix and 'edge_attr' is the distance matrix which is our edge feature matrix.  Edge_index is our adjacency matrix which is used in message passing, but this matrix will stay the same.  The name (name of the protein) and graph_y (the motion label) will not change during message passing.\n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How does using global_max_pool make the model invariant to node ordering?\"\ncorrect_answer = \"It aggregates node-level features into a single graph-level feature, disregarding node order.\"\ndecoy_answers = [\n    \"It selects the maximum feature from each residue, which automatically orders the residues.\",\n    \"It ensures that all residues are processed in a fixed order before pooling, making the model invariant.\",\n    \"It normalizes node feature vectors, so the order doesn’t matter.\"\n    ]\nexplanation =\"\"\n\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How does using global_max_pool make the model invariant to node ordering?\n    \n        \n          \n          It aggregates node-level features into a single graph-level feature, disregarding node order.\n        \n        \n        \n          \n          It selects the maximum feature from each residue, which automatically orders the residues.\n        \n        \n        \n          \n          It ensures that all residues are processed in a fixed order before pooling, making the model invariant.\n        \n        \n        \n          \n          It normalizes node feature vectors, so the order doesn’t matter.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \n      \n    \n    \n    \n\n\nImage Source: torch_geometric.nn conv.GCNConv"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-define-our-model",
    "href": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-define-our-model",
    "title": "Section 1: Graph Theory",
    "section": "Implement a GCN: Define our model",
    "text": "Implement a GCN: Define our model\n\n\nCode\n\nclass GCNGraphLevel(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super(GCNGraphLevel, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n\n        x = global_max_pool(x, batch)  # Need to pool node features to graph level for graph level tasks\n\n        out = self.lin(x)\n\n        return out"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-train-and-eval",
    "href": "notebooks/WS07_GNNsForProteins.html#implement-a-gcn-train-and-eval",
    "title": "Section 1: Graph Theory",
    "section": "Implement a GCN: Train and eval",
    "text": "Implement a GCN: Train and eval\n\n\nCode\n# TRAIN\n# Create DataLoader\ntrain_loader = DataLoader(train_pyg_data_list, batch_size=32, shuffle=True)\n\n# Initialize model, loss, and optimizer\nmodel = GCNGraphLevel(in_channels=4, hidden_channels=16, out_channels=len(motion_type_to_label))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Clear the log file for training metrics\nlog_file = 'train_metrics_log.txt'\nopen(log_file, 'w').close()\n\noptimizer_steps = 0\n\nfor epoch in range(100):\n    model.train()\n    total_train_loss = 0\n    correct_train = 0\n    for data in train_loader:\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.graph_y)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct_train += (pred == data.graph_y).sum().item()\n        optimizer_steps += 1\n\n    train_loss = total_train_loss / len(train_loader)\n    train_accuracy = correct_train / len(train_loader.dataset)\n\n    # Log the training metrics\n    log_metrics(epoch, optimizer_steps, train_loss, train_accuracy, log_file=log_file)\n    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n\n# Plot the training metrics after all epochs\nplot_metrics(log_file='train_metrics_log.txt')\n\n\nEpoch 0, Train Loss: 12.8542, Train Accuracy: 0.1500\nEpoch 1, Train Loss: 6.3447, Train Accuracy: 0.3167\nEpoch 2, Train Loss: 3.7893, Train Accuracy: 0.3000\nEpoch 3, Train Loss: 3.7614, Train Accuracy: 0.3333\nEpoch 4, Train Loss: 3.7813, Train Accuracy: 0.3167\nEpoch 5, Train Loss: 2.8323, Train Accuracy: 0.3333\nEpoch 6, Train Loss: 2.2536, Train Accuracy: 0.3333\nEpoch 7, Train Loss: 1.8248, Train Accuracy: 0.3500\nEpoch 8, Train Loss: 1.7774, Train Accuracy: 0.4167\nEpoch 9, Train Loss: 1.6448, Train Accuracy: 0.3667\nEpoch 10, Train Loss: 1.2767, Train Accuracy: 0.3833\nEpoch 11, Train Loss: 1.1132, Train Accuracy: 0.4667\nEpoch 12, Train Loss: 1.1480, Train Accuracy: 0.4833\nEpoch 13, Train Loss: 1.0810, Train Accuracy: 0.4833\nEpoch 14, Train Loss: 0.9532, Train Accuracy: 0.5167\nEpoch 15, Train Loss: 0.9317, Train Accuracy: 0.5167\nEpoch 16, Train Loss: 0.9816, Train Accuracy: 0.4833\nEpoch 17, Train Loss: 0.9383, Train Accuracy: 0.4833\nEpoch 18, Train Loss: 0.8855, Train Accuracy: 0.5833\nEpoch 19, Train Loss: 0.9045, Train Accuracy: 0.5833\nEpoch 20, Train Loss: 0.8739, Train Accuracy: 0.6000\nEpoch 21, Train Loss: 0.8435, Train Accuracy: 0.6500\nEpoch 22, Train Loss: 0.8285, Train Accuracy: 0.7167\nEpoch 23, Train Loss: 0.8412, Train Accuracy: 0.6000\nEpoch 24, Train Loss: 0.8264, Train Accuracy: 0.6000\nEpoch 25, Train Loss: 0.7839, Train Accuracy: 0.6167\nEpoch 26, Train Loss: 0.7857, Train Accuracy: 0.6667\nEpoch 27, Train Loss: 0.7749, Train Accuracy: 0.7333\nEpoch 28, Train Loss: 0.7649, Train Accuracy: 0.7167\nEpoch 29, Train Loss: 0.7465, Train Accuracy: 0.7167\nEpoch 30, Train Loss: 0.7388, Train Accuracy: 0.7000\nEpoch 31, Train Loss: 0.7299, Train Accuracy: 0.6667\nEpoch 32, Train Loss: 0.7232, Train Accuracy: 0.6500\nEpoch 33, Train Loss: 0.7085, Train Accuracy: 0.7167\nEpoch 34, Train Loss: 0.7005, Train Accuracy: 0.7500\nEpoch 35, Train Loss: 0.7064, Train Accuracy: 0.8000\nEpoch 36, Train Loss: 0.6970, Train Accuracy: 0.7500\nEpoch 37, Train Loss: 0.6793, Train Accuracy: 0.6667\nEpoch 38, Train Loss: 0.6814, Train Accuracy: 0.7167\nEpoch 39, Train Loss: 0.6840, Train Accuracy: 0.7500\nEpoch 40, Train Loss: 0.6615, Train Accuracy: 0.7667\nEpoch 41, Train Loss: 0.6658, Train Accuracy: 0.7833\nEpoch 42, Train Loss: 0.6540, Train Accuracy: 0.8000\nEpoch 43, Train Loss: 0.6485, Train Accuracy: 0.7833\nEpoch 44, Train Loss: 0.6251, Train Accuracy: 0.8167\nEpoch 45, Train Loss: 0.6155, Train Accuracy: 0.8167\nEpoch 46, Train Loss: 0.6059, Train Accuracy: 0.8167\nEpoch 47, Train Loss: 0.5930, Train Accuracy: 0.8667\nEpoch 48, Train Loss: 0.5906, Train Accuracy: 0.8667\nEpoch 49, Train Loss: 0.5836, Train Accuracy: 0.8500\nEpoch 50, Train Loss: 0.5747, Train Accuracy: 0.8500\nEpoch 51, Train Loss: 0.5631, Train Accuracy: 0.8667\nEpoch 52, Train Loss: 0.5603, Train Accuracy: 0.8667\nEpoch 53, Train Loss: 0.5553, Train Accuracy: 0.8667\nEpoch 54, Train Loss: 0.5519, Train Accuracy: 0.8833\nEpoch 55, Train Loss: 0.5464, Train Accuracy: 0.8833\nEpoch 56, Train Loss: 0.5261, Train Accuracy: 0.8667\nEpoch 57, Train Loss: 0.5216, Train Accuracy: 0.8667\nEpoch 58, Train Loss: 0.5193, Train Accuracy: 0.8667\nEpoch 59, Train Loss: 0.5110, Train Accuracy: 0.8833\nEpoch 60, Train Loss: 0.5131, Train Accuracy: 0.8833\nEpoch 61, Train Loss: 0.4983, Train Accuracy: 0.8667\nEpoch 62, Train Loss: 0.4830, Train Accuracy: 0.9000\nEpoch 63, Train Loss: 0.4783, Train Accuracy: 0.8833\nEpoch 64, Train Loss: 0.4704, Train Accuracy: 0.8833\nEpoch 65, Train Loss: 0.4688, Train Accuracy: 0.9000\nEpoch 66, Train Loss: 0.4667, Train Accuracy: 0.8833\nEpoch 67, Train Loss: 0.4519, Train Accuracy: 0.8833\nEpoch 68, Train Loss: 0.4655, Train Accuracy: 0.9000\nEpoch 69, Train Loss: 0.4454, Train Accuracy: 0.8833\nEpoch 70, Train Loss: 0.4393, Train Accuracy: 0.8833\nEpoch 71, Train Loss: 0.4304, Train Accuracy: 0.8833\nEpoch 72, Train Loss: 0.4180, Train Accuracy: 0.8833\nEpoch 73, Train Loss: 0.4270, Train Accuracy: 0.8833\nEpoch 74, Train Loss: 0.4265, Train Accuracy: 0.8667\nEpoch 75, Train Loss: 0.4053, Train Accuracy: 0.9000\nEpoch 76, Train Loss: 0.4087, Train Accuracy: 0.8833\nEpoch 77, Train Loss: 0.4053, Train Accuracy: 0.9000\nEpoch 78, Train Loss: 0.3857, Train Accuracy: 0.9000\nEpoch 79, Train Loss: 0.3923, Train Accuracy: 0.9167\nEpoch 80, Train Loss: 0.3872, Train Accuracy: 0.8833\nEpoch 81, Train Loss: 0.3822, Train Accuracy: 0.8833\nEpoch 82, Train Loss: 0.3630, Train Accuracy: 0.8833\nEpoch 83, Train Loss: 0.3543, Train Accuracy: 0.9000\nEpoch 84, Train Loss: 0.3656, Train Accuracy: 0.9000\nEpoch 85, Train Loss: 0.3574, Train Accuracy: 0.9167\nEpoch 86, Train Loss: 0.3587, Train Accuracy: 0.9000\nEpoch 87, Train Loss: 0.3434, Train Accuracy: 0.9167\nEpoch 88, Train Loss: 0.3382, Train Accuracy: 0.9167\nEpoch 89, Train Loss: 0.3434, Train Accuracy: 0.8833\nEpoch 90, Train Loss: 0.3268, Train Accuracy: 0.9333\nEpoch 91, Train Loss: 0.3211, Train Accuracy: 0.9167\nEpoch 92, Train Loss: 0.3376, Train Accuracy: 0.9000\nEpoch 93, Train Loss: 0.3212, Train Accuracy: 0.9167\nEpoch 94, Train Loss: 0.3384, Train Accuracy: 0.9167\nEpoch 95, Train Loss: 0.3035, Train Accuracy: 0.9167\nEpoch 96, Train Loss: 0.2945, Train Accuracy: 0.9333\nEpoch 97, Train Loss: 0.2991, Train Accuracy: 0.9333\nEpoch 98, Train Loss: 0.2932, Train Accuracy: 0.9333\nEpoch 99, Train Loss: 0.2990, Train Accuracy: 0.9000\n\n\n\n\n\n\n\n\n\n\n\nCode\n# EVAL\n# Create DataLoader\ntest_loader = DataLoader(test_pyg_data_list, batch_size=32)\n\n# Initialize model, loss, and optimizer\n# model = GCNGraphLevel(in_channels=3, hidden_channels=32, out_channels=len(motion_type_to_label))\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n# criterion = torch.nn.CrossEntropyLoss()\n\n# Clear the log file for training metrics\nlog_file = 'eval_metrics_log.txt'\nopen(log_file, 'w').close()\n\noptimizer_steps = 0\n\nfor epoch in range(10):\n    model.eval()\n    total_eval_loss = 0\n    correct_eval = 0\n    for data in test_loader:\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.graph_y)\n        loss.backward()\n        optimizer.step()\n\n        total_eval_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct_eval += (pred == data.graph_y).sum().item()\n        optimizer_steps += 1\n\n    eval_loss = total_eval_loss / len(test_loader)\n    eval_accuracy = correct_eval / len(test_loader.dataset)\n\n    # Log the training metrics\n    log_metrics(epoch, optimizer_steps, eval_loss, eval_accuracy, log_file=log_file)\n    print(f'Epoch {epoch}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}')\n\n# Plot the training metrics after all epochs\nplot_metrics(log_file='eval_metrics_log.txt')\n\n\nEpoch 0, Eval Loss: 0.2103, Eval Accuracy: 0.9333\nEpoch 1, Eval Loss: 0.1836, Eval Accuracy: 0.9333\nEpoch 2, Eval Loss: 0.1494, Eval Accuracy: 1.0000\nEpoch 3, Eval Loss: 0.1198, Eval Accuracy: 1.0000\nEpoch 4, Eval Loss: 0.0984, Eval Accuracy: 1.0000\nEpoch 5, Eval Loss: 0.0851, Eval Accuracy: 1.0000\nEpoch 6, Eval Loss: 0.0745, Eval Accuracy: 1.0000\nEpoch 7, Eval Loss: 0.0606, Eval Accuracy: 1.0000\nEpoch 8, Eval Loss: 0.0477, Eval Accuracy: 1.0000\nEpoch 9, Eval Loss: 0.0394, Eval Accuracy: 1.0000\n\n\n\n\n\n\n\n\n\n\n Expected Results:\nDuring training the GNN should have the following outcome for epoch 0 and epoch 99:   Epoch 0, Train Loss: 12.8542, Train Accuracy: 0.1500  Epoch 99, Train Loss: 0.2990, Train Accuracy: 0.9000  \nDuring eval the GNN should have the following outcome for epoch 0 and epoch 9:  Epoch 0, Eval Loss: 0.2103, Eval Accuracy: 0.9333  Epoch 9, Eval Loss: 0.0394, Eval Accuracy: 1.0000 \nBelow is a table that summarizes the number of each motion type for our proteins present in the dataset.\n\n\nCode\n# Results\nprint(df_pscdb['motion_type'].value_counts())\n\n\nmotion_type\nno_significant_motion        199\nindependent_local_motion      89\ncoupled_local_motion          81\nburying_ligand_motion         70\nindependent_domain_motion     41\ncoupled_domain_motion         37\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#questions",
    "href": "notebooks/WS07_GNNsForProteins.html#questions",
    "title": "Section 1: Graph Theory",
    "section": "Questions!",
    "text": "Questions!\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\n\ndef create_quiz_2q(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Generate the HTML code for the initial question\n    initial_question = \"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3&gt;A GCN is invariant to the order of residues (nodes).&lt;/h3&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption1\" name=\"initQuiz\" value=\"True\"&gt;\n        &lt;label for=\"initOption1\"&gt;True&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption2\" name=\"initQuiz\" value=\"False\"&gt;\n        &lt;label for=\"initOption2\"&gt;False&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;button onclick=\"checkInitialAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"initFeedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div id=\"mainQuiz\" style=\"display:none;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{0}&lt;/h3&gt;\n    \"\"\".format(question)\n\n    # Add each answer as a radio button for the main question\n    for i, answer in enumerate(answers):\n        initial_question += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section for the main quiz\n    initial_question += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkInitialAnswer() {{\n        var radios = document.getElementsByName('initQuiz');\n        var correctAnswer = \"True\";\n        var feedback = document.getElementById('initFeedback');\n        var mainQuizDiv = document.getElementById('mainQuiz');\n        var selectedOption = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                break;\n            }}\n        }}\n\n        // Provide feedback for the initial question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                mainQuizDiv.style.display = \"block\";  // Show the main quiz\n            }} else {{\n                feedback.textContent = \"Incorrect. Try again!\";\n                feedback.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback for the main question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(initial_question))\n\nquestion = \"WHY do we want the GCN to be invariant to the order of residues (nodes)?\"\ncorrect_answer = \"The order of residues in the graph does not affect the overall structure of the protein, so the GCN should not depend on the node order.\"\ndecoy_answers = [\n    \"The order of residues in the graph affects the spatial structure of the protein, so GCN should be sensitive to the node order.\",\n    \"The GCN needs to process each residue in a specific order, which determines the protein’s motion type.\",\n    \"Residue order affects the degree of each node, and the GCN should account for this.\"\n    ]\nexplanation =\"In a protein structure, what matters is the graph's structure (how residues are connected) rather than the order in which nodes (residues) are listed in the dataset. The model should be invariant to node ordering because the classification task (protein motion type) depends on the overall graph structure, not on the specific node order in the input.\"\n\n\ncreate_quiz_2q(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      A GCN is invariant to the order of residues (nodes).\n      \n        \n        True\n      \n      \n        \n        False\n      \n      Submit\n      \n    \n    \n      WHY do we want the GCN to be invariant to the order of residues (nodes)?\n    \n        \n          \n          The order of residues in the graph does not affect the overall structure of the protein, so the GCN should not depend on the node order.\n        \n        \n        \n          \n          The order of residues in the graph affects the spatial structure of the protein, so GCN should be sensitive to the node order.\n        \n        \n        \n          \n          The GCN needs to process each residue in a specific order, which determines the protein’s motion type.\n        \n        \n        \n          \n          Residue order affects the degree of each node, and the GCN should account for this.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        In a protein structure, what matters is the graph's structure (how residues are connected) rather than the order in which nodes (residues) are listed in the dataset. The model should be invariant to node ordering because the classification task (protein motion type) depends on the overall graph structure, not on the specific node order in the input.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\n\ndef create_quiz_2q(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Generate the HTML code for the initial question\n    initial_question = \"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3&gt;The GCN is equivariant to permutations of nodes.&lt;/h3&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption1\" name=\"initQuiz\" value=\"True\"&gt;\n        &lt;label for=\"initOption1\"&gt;True&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption2\" name=\"initQuiz\" value=\"False\"&gt;\n        &lt;label for=\"initOption2\"&gt;False&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;button onclick=\"checkInitialAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"initFeedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div id=\"mainQuiz\" style=\"display:none;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{0}&lt;/h3&gt;\n    \"\"\".format(question)\n\n    # Add each answer as a radio button for the main question\n    for i, answer in enumerate(answers):\n        initial_question += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section for the main quiz\n    initial_question += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkInitialAnswer() {{\n        var radios = document.getElementsByName('initQuiz');\n        var correctAnswer = \"True\";\n        var feedback = document.getElementById('initFeedback');\n        var mainQuizDiv = document.getElementById('mainQuiz');\n        var selectedOption = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                break;\n            }}\n        }}\n\n        // Provide feedback for the initial question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                mainQuizDiv.style.display = \"block\";  // Show the main quiz\n            }} else {{\n                feedback.textContent = \"Incorrect. Try again!\";\n                feedback.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback for the main question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(initial_question))\n\nquestion = \"What does that mean?\"\ncorrect_answer = \"If you permute the order of the nodes, the output features will change in the same way, maintaining the relationships between nodes.\"\ndecoy_answers = [\n    \"If you permute the order of the nodes, the output features of the GCN will change randomly, making the model more robust.\",\n    \"The model ensures that the output features are completely independent of the order of nodes.\",\n    \"Equivariance ensures that the GCN’s predictions remain unchanged when nodes are permuted.\"\n    ]\nexplanation =\"Equivariance means that if the nodes (residues) are permuted (i.e., their order is changed), the output features will change correspondingly while preserving the relationships between the nodes. This is crucial in a graph-based problem like protein classification because we care about the structure of the graph (how nodes are connected) rather than the specific order in which nodes are presented.\"\n\n\ncreate_quiz_2q(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      The GCN is equivariant to permutations of nodes.\n      \n        \n        True\n      \n      \n        \n        False\n      \n      Submit\n      \n    \n    \n      What does that mean?\n    \n        \n          \n          If you permute the order of the nodes, the output features will change in the same way, maintaining the relationships between nodes.\n        \n        \n        \n          \n          If you permute the order of the nodes, the output features of the GCN will change randomly, making the model more robust.\n        \n        \n        \n          \n          The model ensures that the output features are completely independent of the order of nodes.\n        \n        \n        \n          \n          Equivariance ensures that the GCN’s predictions remain unchanged when nodes are permuted.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Equivariance means that if the nodes (residues) are permuted (i.e., their order is changed), the output features will change correspondingly while preserving the relationships between the nodes. This is crucial in a graph-based problem like protein classification because we care about the structure of the graph (how nodes are connected) rather than the specific order in which nodes are presented.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Why is equivariance important in this protein classification problem?\"\ncorrect_answer = \"It ensures that the GCN updates the features in a way that is consistent with the graph structure, regardless of node order.\"\ndecoy_answers = [\n    \"It helps the GCN handle missing data by adjusting the order of residues based on importance.\",\n    \"It guarantees that the GCN outputs remain constant even if the spatial structure of the protein changes.\",\n    \"It ensures that the GCN outputs are identical no matter how the nodes are ordered, which is crucial for protein classification.\"\n    ]\nexplanation =\"Equivariance ensures that the relationships between nodes (residues) in the graph are preserved, no matter how the nodes are ordered in the dataset. This is important because the spatial structure of the protein (how residues are connected) determines the motion type, not the order in which the residues are listed.\"\n\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Why is equivariance important in this protein classification problem?\n    \n        \n          \n          It ensures that the GCN updates the features in a way that is consistent with the graph structure, regardless of node order.\n        \n        \n        \n          \n          It helps the GCN handle missing data by adjusting the order of residues based on importance.\n        \n        \n        \n          \n          It guarantees that the GCN outputs remain constant even if the spatial structure of the protein changes.\n        \n        \n        \n          \n          It ensures that the GCN outputs are identical no matter how the nodes are ordered, which is crucial for protein classification.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Equivariance ensures that the relationships between nodes (residues) in the graph are preserved, no matter how the nodes are ordered in the dataset. This is important because the spatial structure of the protein (how residues are connected) determines the motion type, not the order in which the residues are listed.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Where in the GCN is `graph_y` used?\"\ncorrect_answer = \"It is used as the label for the entire graph and is applied in the loss function during training.\"\ndecoy_answers = [\n    \"It is used in every GCNConv layer to update the node features.\",\n    \"It is used in the message-passing step to decide which edges to include.\",\n    \"It is used in the ReLU activation function to control the non-linearity.\"\n]\nexplanation = \"Graph_y is the graph-level label that represents the motion type classification for the entire protein. It is not involved in message passing, but is used in the loss function during training.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Where in the GCN is `graph_y` used?\n    \n        \n          \n          It is used as the label for the entire graph and is applied in the loss function during training.\n        \n        \n        \n          \n          It is used in every GCNConv layer to update the node features.\n        \n        \n        \n          \n          It is used in the message-passing step to decide which edges to include.\n        \n        \n        \n          \n          It is used in the ReLU activation function to control the non-linearity.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Graph_y is the graph-level label that represents the motion type classification for the entire protein. It is not involved in message passing, but is used in the loss function during training.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How do the XYZ coordinates of residues get updated during message passing in each GCN layer?\"\ncorrect_answer = \"Each node updates its XYZ coordinates by aggregating the feature vectors from its neighboring nodes, then applying a linear transformation and non-linearity.\"\ndecoy_answers = [\n    \"The XYZ coordinates are updated by adding random noise in each GCNConv layer to make the model more robust.\",\n    \"Each node sends its XYZ coordinates to all other nodes in the graph, and they are averaged across the graph.\",\n    \"The XYZ coordinates are directly multiplied by the edge weights without aggregation.\"\n]\nexplanation = \"Each node aggregates information from its neighbors (defined by the graph’s edges) to update its features, including XYZ coordinates. This is followed by a linear transformation and ReLU non-linearity.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How do the XYZ coordinates of residues get updated during message passing in each GCN layer?\n    \n        \n          \n          Each node updates its XYZ coordinates by aggregating the feature vectors from its neighboring nodes, then applying a linear transformation and non-linearity.\n        \n        \n        \n          \n          The XYZ coordinates are updated by adding random noise in each GCNConv layer to make the model more robust.\n        \n        \n        \n          \n          Each node sends its XYZ coordinates to all other nodes in the graph, and they are averaged across the graph.\n        \n        \n        \n          \n          The XYZ coordinates are directly multiplied by the edge weights without aggregation.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Each node aggregates information from its neighbors (defined by the graph’s edges) to update its features, including XYZ coordinates. This is followed by a linear transformation and ReLU non-linearity.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What role do the edges (`edge_index`) play in message passing? What step are they involved in and how? Do they ever get updated?\"\ncorrect_answer = \"The edges determine which nodes communicate with each other during message passing and they do not get updated during the process.\"\ndecoy_answers = [\n    \"The edges represent the distance between residues and get updated after every GCNConv layer.\",\n    \"The edges determine which nodes get disconnected after each layer, reducing the size of the graph.\",\n    \"The edges are used only in the first GCNConv layer and are discarded afterward.\"\n]\nexplanation = \"The edges in `edge_index` define how nodes (residues) communicate with each other in the graph. They remain fixed throughout the GCN layers and are not updated.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      What role do the edges (`edge_index`) play in message passing? What step are they involved in and how? Do they ever get updated?\n    \n        \n          \n          The edges determine which nodes communicate with each other during message passing and they do not get updated during the process.\n        \n        \n        \n          \n          The edges represent the distance between residues and get updated after every GCNConv layer.\n        \n        \n        \n          \n          The edges determine which nodes get disconnected after each layer, reducing the size of the graph.\n        \n        \n        \n          \n          The edges are used only in the first GCNConv layer and are discarded afterward.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        The edges in `edge_index` define how nodes (residues) communicate with each other in the graph. They remain fixed throughout the GCN layers and are not updated.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Why does the GCN have two GCNConv layers in the architecture? What might happen if you added more layers or reduced the number of layers?\"\ncorrect_answer = \"Adding more layers allows the model to aggregate information from nodes farther away, but too many layers can cause over-smoothing, where all node features become indistinguishable.\"\ndecoy_answers = [\n    \"Two layers ensure that the graph is fully connected, and adding more layers would improve the classification accuracy without any drawbacks.\",\n    \"The model uses two layers to ensure that each node’s features are multiplied by the motion type label twice, increasing accuracy.\",\n    \"Adding more layers will reduce overfitting, and having fewer layers will always lead to underfitting.\"\n]\nexplanation = \"More GCN layers allow nodes to aggregate information from more distant nodes, but too many layers can result in over-smoothing, where node features become too similar and lose their distinctiveness.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Why does the GCN have two GCNConv layers in the architecture? What might happen if you added more layers or reduced the number of layers?\n    \n        \n          \n          Adding more layers allows the model to aggregate information from nodes farther away, but too many layers can cause over-smoothing, where all node features become indistinguishable.\n        \n        \n        \n          \n          Two layers ensure that the graph is fully connected, and adding more layers would improve the classification accuracy without any drawbacks.\n        \n        \n        \n          \n          The model uses two layers to ensure that each node’s features are multiplied by the motion type label twice, increasing accuracy.\n        \n        \n        \n          \n          Adding more layers will reduce overfitting, and having fewer layers will always lead to underfitting.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        More GCN layers allow nodes to aggregate information from more distant nodes, but too many layers can result in over-smoothing, where node features become too similar and lose their distinctiveness.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How does `global_max_pool(x, batch)` help aggregate the information from multiple nodes in a protein to create a graph-level feature vector?\"\ncorrect_answer = \"It selects the maximum feature value from each node across the entire graph, creating a single vector that represents the entire graph (protein).\"\ndecoy_answers = [\n    \"It selects the minimum feature value across all nodes in a graph to create a graph-level feature.\",\n    \"It averages the feature vectors of all nodes and outputs the graph-level feature vector.\",\n    \"It sums all the node features together to create the graph-level feature vector.\"\n]\nexplanation = \"Global max pooling selects the maximum value from each node's features, creating a graph-level feature vector that summarizes the entire graph, making it ideal for tasks like protein classification.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How does `global_max_pool(x, batch)` help aggregate the information from multiple nodes in a protein to create a graph-level feature vector?\n    \n        \n          \n          It selects the maximum feature value from each node across the entire graph, creating a single vector that represents the entire graph (protein).\n        \n        \n        \n          \n          It selects the minimum feature value across all nodes in a graph to create a graph-level feature.\n        \n        \n        \n          \n          It averages the feature vectors of all nodes and outputs the graph-level feature vector.\n        \n        \n        \n          \n          It sums all the node features together to create the graph-level feature vector.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Global max pooling selects the maximum value from each node's features, creating a graph-level feature vector that summarizes the entire graph, making it ideal for tasks like protein classification.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the role of the final linear layer (`self.lin`) in this GCN model, and how does it relate to the classification task?\"\ncorrect_answer = \"The linear layer directly predicts the motion type by converting the graph-level features into class probabilities.\"\ndecoy_answers = [\n    \"The linear layer increases the dimensionality of the node features to make the graph more complex.\",\n    \"The linear layer aggregates information from all edges to determine the motion type.\",\n    \"The linear layer is used to update the feature vectors of each residue and is not involved in classification.\"\n]\nexplanation = \"The final linear layer converts the graph-level feature vector into a set of class probabilities, which in this case represent the predicted motion type of the protein.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      What is the role of the final linear layer (`self.lin`) in this GCN model, and how does it relate to the classification task?\n    \n        \n          \n          The linear layer directly predicts the motion type by converting the graph-level features into class probabilities.\n        \n        \n        \n          \n          The linear layer increases the dimensionality of the node features to make the graph more complex.\n        \n        \n        \n          \n          The linear layer aggregates information from all edges to determine the motion type.\n        \n        \n        \n          \n          The linear layer is used to update the feature vectors of each residue and is not involved in classification.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        The final linear layer converts the graph-level feature vector into a set of class probabilities, which in this case represent the predicted motion type of the protein.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How does the edge structure (e.g., 1133 edges for protein `2nrt`) affect the message passing process? How might different types of edges (e.g., chemical bonds, proximity in 3D space) influence the GCN’s ability to classify protein motion?\"\ncorrect_answer = \"The edges allow for message passing between nodes, and different types of edges (e.g., chemical bonds, spatial proximity) affect how well the GCN can capture meaningful interactions between residues.\"\ndecoy_answers = [\n    \"The edges control the activation function used in each GCNConv layer and are updated after every message-passing step.\",\n    \"The edge structure determines the order in which the nodes are processed, with more edges resulting in more iterations.\",\n    \"The edges are only used for regularization and do not affect message passing or classification performance.\"\n]\nexplanation = \"Edges define which nodes can communicate during message passing. Different types of edges, such as those representing chemical bonds or spatial proximity, capture different kinds of relationships between residues, impacting how the GCN models protein motion.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How does the edge structure (e.g., 1133 edges for protein `2nrt`) affect the message passing process? How might different types of edges (e.g., chemical bonds, proximity in 3D space) influence the GCN’s ability to classify protein motion?\n    \n        \n          \n          The edges allow for message passing between nodes, and different types of edges (e.g., chemical bonds, spatial proximity) affect how well the GCN can capture meaningful interactions between residues.\n        \n        \n        \n          \n          The edges control the activation function used in each GCNConv layer and are updated after every message-passing step.\n        \n        \n        \n          \n          The edge structure determines the order in which the nodes are processed, with more edges resulting in more iterations.\n        \n        \n        \n          \n          The edges are only used for regularization and do not affect message passing or classification performance.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Edges define which nodes can communicate during message passing. Different types of edges, such as those representing chemical bonds or spatial proximity, capture different kinds of relationships between residues, impacting how the GCN models protein motion.\n      \n    \n    \n    \n\n\n\nIf you enjoy GNNs and want to know more, here are some excellent resources:\n\nPetar Veličković is has a great lecture with accompanying slides and companion colab notebook: \nYouTube Lecture: Theoretical Foundations of Graph Neural Networks \nSlides: https://petar-v.com/talks/GNN-Wednesday.pdf \nColab notebook: Introduction to Graph Neural Nets with JAX/jraph \nAdditionally, here is a git repo he made for GAT resources (has some great visuals): GAT - Graph Attention Network (PyTorch) + graphs \nAn excellent primer: A Gentle Introduction to Graph Neural Networks\nA compilation of colab notebooks and videos: PyTorch Geometric Tutorial Project \nAnother useful colab notebok with implementations of other GNN variations: A Gentle Introduction to Geometric Graph Neural Networks"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html",
    "href": "notebooks/WS06_IntroductionToAF.html",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "##Return to HomePage\nThe implementation of AlphaFold2 is adapted from ColabFold\nOBJECTIVES: By the end of this workshop you should be able to:\n\nIdentify and Apply State of the Art Protein Folding and Multitrack Architectures\nRun the ColabFold implementation of AF2\nInterpret AF outputs, including plddt scores, coverage, and pae scores\nDistinguish different protein representations in AlphaFold\nEnumerate the basic architectural components of AF2\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n\n\n\n\n\nIntroduction: Enter deep learning!\nIntroduction: Alphafold as a magic box\n\n\n\n\n\nPractical Alphafold: Folding a sequence\nPractical Alphafold: Interpreting Results\nMultiple Sequence Alignment (MSA) Coverage\npLDDT\nPredicted Aligned Error (PAE)\n\n\n\n\n\nAlphafold Architecture: Input sequence Representation\nAlphafold Architecture: Evoformer\nAlphafold Architecture: Structure Module\n\n\n\n\n\n\nCode\n#@markdown **Please run this cell as you read the introduction**\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport os\n\ncustom_template_path = None\nuse_templates = False\n\n\nUSE_AMBER = False\nUSE_TEMPLATES = use_templates\nPYTHON_VERSION = python_version\n\nif not os.path.isfile(\"COLABFOLD_READY\"):\n  print(\"installing colabfold...\")\n  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n  if os.environ.get('TPU_NAME', False) != False:\n    os.system(\"pip uninstall -y jax jaxlib\")\n    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n  os.system(\"touch COLABFOLD_READY\")\n\nif USE_AMBER or USE_TEMPLATES:\n  if not os.path.isfile(\"CONDA_READY\"):\n    print(\"installing conda...\")\n    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\")\n    os.system(\"bash Mambaforge-Linux-x86_64.sh -bfp /usr/local\")\n    os.system(\"mamba config --set auto_update_conda false\")\n    os.system(\"touch CONDA_READY\")\n\nif USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n  print(\"installing hhsuite and amber...\")\n  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n  os.system(\"touch HH_READY\")\n  os.system(\"touch AMBER_READY\")\nelse:\n  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n    print(\"installing hhsuite...\")\n    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n    os.system(\"touch HH_READY\")\n  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n    print(\"installing amber...\")\n    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n    os.system(\"touch AMBER_READY\")\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\njobname = 'import_test'\nquery_sequence = 'A'\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\nmodel_type = \"auto\"\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\ndownload_alphafold_params(model_type, Path(\".\"))\n\n\ninstalling colabfold...\n\n\nDownloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [02:01&lt;00:00, 30.6MB/s]\n\n\n\n\n\n\n\nIf we know a protein’s sequence, what 3D structure will it fold into? This protein folding problem has been a longtime dream of biophysicists, biologists, and protein engineers. Solving holds the key to many new discoveries and innovations for research and drug design!\n\n\n\n\n\nYet, although we understood much of the individual physical concepts behind protein folding, there was no recipe of equations that could reliably predict what a protein would fold into.\nAs examples of 3D experimental protein structures increased, the dataset eventually became large enough to train neural networks to predict how proteins folded.\n\n\n\n\n\nOne of the most famous of these is AlphaFold! It kickstarted a revolution in protein folding by drastically outperforming all competitors in CASP, the yearly protein folding competition.\n\n\nHint: Previous scores were around 30-40 Global Distance Score (out of 100)\n\n\nCode\n# @markdown Take a guess!\nAF1 = 70 # @param {type:\"slider\", min:0, max:100, step:10}\nAF2 = 50 # @param {type:\"slider\", min:0, max:100, step:10}\n\nyear = [2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020]\ncasp_scores = [35, 41, 34, 30, 31, 41, 57, 88]\n\n# make bar chart\nfig = plt.figure(figsize=(10, 5))\nplt.bar(year, casp_scores)\nplt.xlabel('Year')\nplt.ylabel('CASP Score')\n\n# add prediction as scatter\nplt.scatter(2018, AF1, label='AF1', color='purple')\nplt.scatter(2020, AF2, label='AF2', color='magenta')\nplt.text(2018, AF1, f'{AF1}', ha='right')\nplt.text(2020, AF2, f'{AF2}', ha='right')\nplt.title('CASP Median Free Modeling Accuracy')\n\n# Set y axis to 0-100\nplt.ylim(0, 100)\n\n\nplt.show()\n\n\n\n\n\n\nFor it’s input, AlphaFold takes the sequence and several optional other pieces of information:\n\nMSAs – multiple sequence alignments, genetically similar amino acid sequences inferred from a database.\nPair Features – a pairwise feature representation of the possible residue interactions.\nTemplates – template proteins that may have similar structure to the unknown protein. These are combined with pair features before thrown into the model’s main modules.\n\nBriefly put, these three inputs are fed into the model, which then spits out a protein and some metrics on how plausible AF thinks the structure is. For more information, see subsection AF Architecture: How Does AF Work?\n\n\n\nScreenshot 2024-06-26 000206.png\n\n\n\n\n\n\n\nLet’s get right into it! We have a protein sequence. How do we find it’s structure with AlphaFold?\nHere, we will use the ColabFold implementation of AF2 (Mirdita et al. 2022). Usually, we would need to provide MSA alignments, but ColabFold automatically does that for us using MMseqs2. All we have to do is run the cells below, and provide it our sequence (and optionally, templates), and boom, it folds the protein!\nNote: When designing de novo proteins, it is recommended to turn MSA off because de novo proteins usually do not have high structural similarities to natural proteins.\nAdapted from. For more advanced uses, please visit the ColabFold Github page to find notebooks with more options and parameters. This notebook is mainly intended for educational purposes!\n\n\n\n\nCode\n#@markdown Input your favorite protein sequence, then RUN! (Note: Make sure you are connected to a GPU instance. Should take around 2-5 minutes)\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\nquery_sequence = 'MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH' #@param {type:\"string\"}\njobname = 'learning_af'\nnum_relax = 0\ntemplate_mode = \"none\"\n\nuse_amber = num_relax &gt; 0\n\n# remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nif template_mode == \"pdb100\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n\nprint(\"jobname\",jobname)\nprint(\"sequence\",query_sequence)\nprint(\"length\",len(query_sequence.replace(\":\",\"\")))\n\n\n\nimport os\n\nmsa_mode = \"mmseqs2_uniref_env\"\npair_mode = \"unpaired_paired\"\n\n# decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n\n\nmodel_type = \"auto\"\nnum_recycles = \"3\"\nrecycle_early_stop_tolerance = \"auto\"\nrelax_max_iterations = 200\npairing_strategy = \"greedy\"\n\n\n\nmax_msa = \"auto\"\nnum_seeds = 1\nuse_dropout = False\n\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\n\nsave_all = False\nsave_recycles = False\nsave_to_google_drive = False\ndpi = 200\nif save_to_google_drive:\n  from pydrive2.drive import GoogleDrive\n  from pydrive2.auth import GoogleAuth\n  from google.colab import auth\n  from oauth2client.client import GoogleCredentials\n  auth.authenticate_user()\n  gauth = GoogleAuth()\n  gauth.credentials = GoogleCredentials.get_application_default()\n  drive = GoogleDrive(gauth)\n  print(\"You are logged into Google Drive and are good to go!\")\n\n\ndisplay_images = True\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n\n\ndisplay_images = False\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nlog_filename = os.path.join(jobname,\"log.txt\")\nsetup_logging(Path(log_filename))\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,\n    model_type=model_type,\n    num_models=2,\n    num_recycles=2,\n    relax_max_iterations=relax_max_iterations,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2], # edited\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    pairing_strategy=pairing_strategy,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n    user_agent=\"colabfold/google-colab-main\",\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n\n\njobname learning_af_e64d1\nsequence MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH\nlength 92\n2024-10-09 17:38:47,646 Running on GPU\n2024-10-09 17:38:48,066 Found 5 citations for tools or databases\n2024-10-09 17:38:48,067 Query 1/1: learning_af_e64d1 (length 92)\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:01 remaining: ?]\n\n\n2024-10-09 17:38:49,136 Sleeping for 5s. Reason: PENDING\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:07 remaining: ?]\n\n\n2024-10-09 17:38:55,066 Sleeping for 6s. Reason: PENDING\n\n\nRUNNING:   4%|▍         | 6/150 [elapsed: 00:13 remaining: 05:34]\n\n\n2024-10-09 17:39:02,012 Sleeping for 5s. Reason: RUNNING\n\n\nRUNNING:   7%|▋         | 11/150 [elapsed: 00:19 remaining: 03:56]\n\n\n2024-10-09 17:39:07,932 Sleeping for 8s. Reason: RUNNING\n\n\nRUNNING:  13%|█▎        | 19/150 [elapsed: 00:28 remaining: 03:01]\n\n\n2024-10-09 17:39:16,910 Sleeping for 7s. Reason: RUNNING\n\n\nRUNNING:  17%|█▋        | 26/150 [elapsed: 00:36 remaining: 02:38]\n\n\n2024-10-09 17:39:24,835 Sleeping for 8s. Reason: RUNNING\n\n\nCOMPLETE: 100%|██████████| 150/150 [elapsed: 00:47 remaining: 00:00]\n\n\n2024-10-09 17:39:36,982 Setting max_seq=512, max_extra_seq=5120\n2024-10-09 17:40:15,080 alphafold2_ptm_model_1_seed_000 recycle=0 pLDDT=80 pTM=0.667\n2024-10-09 17:40:21,715 alphafold2_ptm_model_1_seed_000 recycle=1 pLDDT=82.1 pTM=0.692 tol=0.583\n2024-10-09 17:40:28,410 alphafold2_ptm_model_1_seed_000 recycle=2 pLDDT=82.7 pTM=0.702 tol=0.111\n2024-10-09 17:40:28,412 alphafold2_ptm_model_1_seed_000 took 37.7s (2 recycles)\n2024-10-09 17:40:35,204 alphafold2_ptm_model_2_seed_000 recycle=0 pLDDT=84.9 pTM=0.742\n2024-10-09 17:40:42,062 alphafold2_ptm_model_2_seed_000 recycle=1 pLDDT=85.2 pTM=0.742 tol=0.834\n2024-10-09 17:40:48,939 alphafold2_ptm_model_2_seed_000 recycle=2 pLDDT=85.8 pTM=0.749 tol=0.14\n2024-10-09 17:40:48,940 alphafold2_ptm_model_2_seed_000 took 20.5s (2 recycles)\n2024-10-09 17:40:48,961 reranking models by 'plddt' metric\n2024-10-09 17:40:48,961 rank_001_alphafold2_ptm_model_2_seed_000 pLDDT=85.8 pTM=0.749\n2024-10-09 17:40:48,962 rank_002_alphafold2_ptm_model_1_seed_000 pLDDT=82.7 pTM=0.702\n2024-10-09 17:40:49,709 Done\n\n\n0\n\n\n\n\nCode\n#@markdown Display 3D structure {run: \"auto\"}\nimport py3Dmol\nimport glob\nimport matplotlib.pyplot as plt\nfrom colabfold.colabfold import plot_plddt_legend\nfrom colabfold.colabfold import pymol_color_list, alphabet_list\nrank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\ncolor = \"lDDT\"\nshow_sidechains = True #@param {type:\"boolean\"}\nshow_mainchains = False\n\ntag = results[\"rank\"][0][rank_num - 1]\njobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\npdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\npdb_file = glob.glob(pdb_filename)\n\ndef show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n  model_name = f\"rank_{rank_num}\"\n  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n\n  if color == \"lDDT\":\n    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n  elif color == \"rainbow\":\n    view.setStyle({'cartoon': {'color':'spectrum'}})\n  elif color == \"chain\":\n    chains = len(queries[0][1]) + 1 if is_complex else 1\n    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n\n  if show_sidechains:\n    BB = ['C','O','N']\n    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n  if show_mainchains:\n    BB = ['C','O','N','CA']\n    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n\n  view.zoomTo()\n  return view\n\nshow_pdb(rank_num, show_sidechains, show_mainchains, color).show()\nif color == \"lDDT\":\n  plot_plddt_legend().show()\n\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages.\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you were to download the outputs (packaged conveniently on the actual ColabFold notebook, or less conveniently in the files for this notebook), you will get several files of interest:\n\nSeveral “ranked” 3D pdb models \nAn image detailing the “coverage”\nAn image detailing the “pLDDT”\nAn image detailing the “PAE”\n\nLet’s look into what each means!\n\n\n“Coverage” is pretty straightforward- when AF looks into its database of sequences, the coverage shows how many matches it was able to assign for parts of the input sequence.\n\n\nCode\n#@markdown Run to show the MSA coverage of your protein - Is there a lot or a little coverage? (hint: more colors = more coverage)\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Which one has more coverage?', 'Trick question!!! The one one the left has lots of coverage, as it is a highly documented natural protein (Snase), with many similar sequences in the MSA database. On the other hand, the one on the right is a complex folded without MSAs. We want to do this when folding de novo proteins- as they are not as evolutionarily related/structrually related to similar sequences found in nature, and would mess with the inference.')\n\n\nWhich one has more coverage?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\npLDDT can be interpreted as the per-residue confidence that AF has in its prediction. A higher pLDDT means the prediction is more accurate.\nFrom Deepmind’s website,\n\n&gt;90 = High accuracy\n70-90 = Modeled well\n50-70 = Low confidence\n&lt;50 = Probably disordered region \n\nThe overall pLDDT is used to rank the outputs (and the structure we visualized is colored by pLDDT – blue is higher and red is lower)\n\n\nCode\n#@markdown Run to display your pLDDT score - Is it high confidence? Where might AF2 be less confident?\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which one is AF more confident in? Why?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Alphafold has more confidence in the right one! This is because the plddt is higher (there are less \"spikes\")')\n\n\nWhich one is AF more confident in? Why?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nRepresents AF’s confidence for the interaction of protein domains and residues. Given in a contact map between all residues.\nThe color at (x, y) corresponds to the expected distance error in residue x’s position, when the prediction and true structure are aligned on residue y.\nLower is better: * Low = well defined positions/orientation (blue) * High = uncertain relative positions (red)\nFor example: If a nanobody binds well to an antigen, we would expect to see more “blue” in the interaction regions between the two chains (see the question below for an example)\n\n\nCode\n#@markdown Run to view PAE charts - take time to understand what the x and y axes mean!\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\npae = \"\"\npae_file = os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\")\nif os.path.isfile(pae_file):\n    pae = image_to_data_url(pae_file)\n\ndisplay(HTML(f\"\"\"\n&lt;div style=\"max-width:90%; padding:2em;\"&gt;\n  { '&lt;!--' if pae == '' else '' }&lt;img src=\"{pae}\" class=\"full\" /&gt;{ '--&gt;' if pae == '' else '' }\n&lt;/div&gt;\n\"\"\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box(\"2 & 3 show weak interactions between the two proteins. However, it should be noted that the PAE diagram is not exactly a perfect mirror. If there is different aligned error based on which residue we align on, the difference that will be reflected in the diagram\")\n\n\nWhich quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nBelow is a summary of all the files produced after running AlphaFold\n\n\n\nimage.png\n\n\n\n\n\n\n\nAF utilized many innovations, including multi-track architecture, multiple-sequence alignments, end-to-training, attention, and its own confidence estimator. Don’t worry if you don’t understand these terms, we will go into it!\nBelow are the three main components of AF2’s architecture that we talked about before:\n\n\n\nimage.png\n\n\n\n\n\n\nProtein sequence information fed to AlphaFold 2 are stored as a fasta file. Here’s an example below:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD\n\n\n\nThe input amino acid sequence is used as a query to several databases (normally UniRef) of protein sequences and constructs a multiple sequence alignment (MSA). An MSA identifies similar (but not identical) sequences that have been identified in living organisms. How does one go from an alignment to a structure? The theory is that residues that coevolve are generally close to each other in the protein’s folded state.\nThe similar sequences pulled from also have structural information. These templates can be converted into distance matrices to determine the distance between residues\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\ndef create_quiz(question, correct_answer, decoy_answers):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is an MSA and why is it important for AF2?\"\ncorrect_answer = \"An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\"\ndecoy_answers = [\"An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\",\n                 \"An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\",\n                 \"An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\n        \n        \n        \n          \n          An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\n        \n        \n        \n          \n          An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\n        \n        \n        \n          \n          An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nHere, we also try to identify templates, which are proteins that may have similar structure to that of the input sequence. This is used to construct an initial representation of the structure, also known as the pair representation. By identifying conserved fragments, the model uses this as a guide to construct a structure.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the concept of “pair representation” in AF2?\"\ncorrect_answer = \"Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\"\ndecoy_answers = [\"Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\",\n                 \"Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\",\n                 \"Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\n        \n        \n        \n          \n          Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\n        \n        \n        \n          \n          Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\n        \n        \n        \n          \n          Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\n\n\nHere, we take the MSA and templates and pass them through a pair of transformers. Doing so (1) identifies which pieces of information are more informative, (2) iteratively exchanges information between MSA and pair representations, and (3) refines the representations for both the MSA and pair interactions.\nBelow is a deeper look into the Evoformer (which is composed of an MSA transformer and pair transformer). Left image: The MSA transformer identifies a correlation between the two columns of the MSA, each corresponding to a residue. This information is passed to the pair representation, where subsequently the pair representation identifies another possible interaction. Right image: Information is passed back to the MSA. The MSA transformer receives an input from the pair representation and observes another pair of columns that exhibit a significant correlation.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the role of the Evoformer in the AF2 architecture?\"\ncorrect_answer = \"The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\"\ndecoy_answers = [\"The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\",\n                 \"The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\",\n                 \"The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\n        \n        \n        \n          \n          The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\n        \n        \n        \n          \n          The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\n        \n        \n        \n          \n          The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nThe pair of transformers works on the principle of Triangle inequality, where the sum of two sides must be greater than or equal to the third side. Attention is arranged in terms of triangles of residues. Using this theorem, we can determine the likely distance residues have from one another because the distance between three points can never break that theorem\n\n\n\nimage.png\n\n\n\n\n\nThe structure module takes a refined MSA representation and refined pair representation, and uses this to construct a 3D model of the structure. The end result is a long list of cartesian coordinates representing the position of each atom of the protein (including side chains). Proteins are represented as a residue gas, where every amino acid is modeled as a triangle with points at the Nitrogen, R group Carbon and the Alpha Carbon. At the beginning of the structure module, all residues are placed at the origin of coordinates. At every step, the model produces a set of matrices that displace/rotate the residues in space. This representation does not reflect any physical or geometric assumptions, and as a result the network has a tendency to generate structural violations, which you can see in this video\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"How does the structure module of AF2 generate the final protein structure?\"\ncorrect_answer = \"The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\"\ndecoy_answers = [\"The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\",\n                 \"The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\",\n                 \"The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\"]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\n        \n        \n        \n          \n          The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\n        \n        \n        \n          \n          The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\n        \n        \n        \n          \n          The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is invariant point attention (IPA) and why is it important in AF2?\"\ncorrect_answer = \"IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\"\ndecoy_answers = [\"IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\",\n                 \"IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\",\n                 \"IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\n        \n        \n        \n          \n          IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\n        \n        \n        \n          \n          IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\n        \n        \n        \n          \n          IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the Frame Aligned Point Error (FAPE) and how is it used in AF2?\"\ncorrect_answer = \"FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\"\ndecoy_answers = [\"FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\",\n                 \"FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\",\n                 \"FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\n        \n        \n        \n          \n          FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\n        \n        \n        \n          \n          FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\n        \n        \n        \n          \n          FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nAn important note is that this model works iteratively. After generating a structure, the model takes all the information (MSA representation, pair representation, and predicted structure) and passes it back to the beginning of the Evoformer blocks. Check this video out of the model refining it’s predictions across blocks over multiple iteractions.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\n# Define the question and answers\nquestion = \"How does AF2 refine its predictions iteratively?\"\ncorrect_answer = \"Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\"\ndecoy_answers = [\"AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\",\n                 \"AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\",\n                 \"AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\"]\n\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\n        \n        \n        \n          \n          AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\n        \n        \n        \n          \n          AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\n        \n        \n        \n          \n          Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\n\nAlphafold’s architecture and concepts have inspired other deep-learning based protein folding algorithms as well, such as RosettaTTAFold2 and ESMFold. Each model has its advantages and disadvantages, dependent on its accuracy (with respect to various protein types), which oftentimes trades off with runtime (more accurate models take longer to run). Based on your desired task, whether it be designing a de novo protein or investigating a natural complex, it may be beneficial to choose a particular model to use. However, that is beyond the scope of this tutorial.\nAnother active field of progress is predicting folding with ligands and post-translational modifications. At the time of writing, the state of the art includes RosettaTTAFold All Atom, AF3, HelixFold3, and Chai1. These models are been able to account for ligands in folding, such as DNA or inhibitors. In particular, HelixFold-single and Chai1 look to circumvent MSA usage by using protein language models instead! Feel free to check all of them out!"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#table-of-contents",
    "href": "notebooks/WS06_IntroductionToAF.html#table-of-contents",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Introduction: Enter deep learning!\nIntroduction: Alphafold as a magic box\n\n\n\n\n\nPractical Alphafold: Folding a sequence\nPractical Alphafold: Interpreting Results\nMultiple Sequence Alignment (MSA) Coverage\npLDDT\nPredicted Aligned Error (PAE)\n\n\n\n\n\nAlphafold Architecture: Input sequence Representation\nAlphafold Architecture: Evoformer\nAlphafold Architecture: Structure Module\n\n\n\n\n\n\nCode\n#@markdown **Please run this cell as you read the introduction**\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport os\n\ncustom_template_path = None\nuse_templates = False\n\n\nUSE_AMBER = False\nUSE_TEMPLATES = use_templates\nPYTHON_VERSION = python_version\n\nif not os.path.isfile(\"COLABFOLD_READY\"):\n  print(\"installing colabfold...\")\n  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n  if os.environ.get('TPU_NAME', False) != False:\n    os.system(\"pip uninstall -y jax jaxlib\")\n    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n  os.system(\"touch COLABFOLD_READY\")\n\nif USE_AMBER or USE_TEMPLATES:\n  if not os.path.isfile(\"CONDA_READY\"):\n    print(\"installing conda...\")\n    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\")\n    os.system(\"bash Mambaforge-Linux-x86_64.sh -bfp /usr/local\")\n    os.system(\"mamba config --set auto_update_conda false\")\n    os.system(\"touch CONDA_READY\")\n\nif USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n  print(\"installing hhsuite and amber...\")\n  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n  os.system(\"touch HH_READY\")\n  os.system(\"touch AMBER_READY\")\nelse:\n  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n    print(\"installing hhsuite...\")\n    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n    os.system(\"touch HH_READY\")\n  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n    print(\"installing amber...\")\n    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n    os.system(\"touch AMBER_READY\")\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\njobname = 'import_test'\nquery_sequence = 'A'\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\nmodel_type = \"auto\"\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\ndownload_alphafold_params(model_type, Path(\".\"))\n\n\ninstalling colabfold...\n\n\nDownloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [02:01&lt;00:00, 30.6MB/s]"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#enter-deep-learning",
    "href": "notebooks/WS06_IntroductionToAF.html#enter-deep-learning",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Yet, although we understood much of the individual physical concepts behind protein folding, there was no recipe of equations that could reliably predict what a protein would fold into.\nAs examples of 3D experimental protein structures increased, the dataset eventually became large enough to train neural networks to predict how proteins folded.\n\n\n\n\n\nOne of the most famous of these is AlphaFold! It kickstarted a revolution in protein folding by drastically outperforming all competitors in CASP, the yearly protein folding competition.\n\n\nHint: Previous scores were around 30-40 Global Distance Score (out of 100)\n\n\nCode\n# @markdown Take a guess!\nAF1 = 70 # @param {type:\"slider\", min:0, max:100, step:10}\nAF2 = 50 # @param {type:\"slider\", min:0, max:100, step:10}\n\nyear = [2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020]\ncasp_scores = [35, 41, 34, 30, 31, 41, 57, 88]\n\n# make bar chart\nfig = plt.figure(figsize=(10, 5))\nplt.bar(year, casp_scores)\nplt.xlabel('Year')\nplt.ylabel('CASP Score')\n\n# add prediction as scatter\nplt.scatter(2018, AF1, label='AF1', color='purple')\nplt.scatter(2020, AF2, label='AF2', color='magenta')\nplt.text(2018, AF1, f'{AF1}', ha='right')\nplt.text(2020, AF2, f'{AF2}', ha='right')\nplt.title('CASP Median Free Modeling Accuracy')\n\n# Set y axis to 0-100\nplt.ylim(0, 100)\n\n\nplt.show()"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#alphafold-as-a-magic-box",
    "href": "notebooks/WS06_IntroductionToAF.html#alphafold-as-a-magic-box",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "For it’s input, AlphaFold takes the sequence and several optional other pieces of information:\n\nMSAs – multiple sequence alignments, genetically similar amino acid sequences inferred from a database.\nPair Features – a pairwise feature representation of the possible residue interactions.\nTemplates – template proteins that may have similar structure to the unknown protein. These are combined with pair features before thrown into the model’s main modules.\n\nBriefly put, these three inputs are fed into the model, which then spits out a protein and some metrics on how plausible AF thinks the structure is. For more information, see subsection AF Architecture: How Does AF Work?\n\n\n\nScreenshot 2024-06-26 000206.png"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-1-folding-a-sequence",
    "href": "notebooks/WS06_IntroductionToAF.html#part-1-folding-a-sequence",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Code\n#@markdown Input your favorite protein sequence, then RUN! (Note: Make sure you are connected to a GPU instance. Should take around 2-5 minutes)\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\nquery_sequence = 'MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH' #@param {type:\"string\"}\njobname = 'learning_af'\nnum_relax = 0\ntemplate_mode = \"none\"\n\nuse_amber = num_relax &gt; 0\n\n# remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nif template_mode == \"pdb100\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n\nprint(\"jobname\",jobname)\nprint(\"sequence\",query_sequence)\nprint(\"length\",len(query_sequence.replace(\":\",\"\")))\n\n\n\nimport os\n\nmsa_mode = \"mmseqs2_uniref_env\"\npair_mode = \"unpaired_paired\"\n\n# decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n\n\nmodel_type = \"auto\"\nnum_recycles = \"3\"\nrecycle_early_stop_tolerance = \"auto\"\nrelax_max_iterations = 200\npairing_strategy = \"greedy\"\n\n\n\nmax_msa = \"auto\"\nnum_seeds = 1\nuse_dropout = False\n\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\n\nsave_all = False\nsave_recycles = False\nsave_to_google_drive = False\ndpi = 200\nif save_to_google_drive:\n  from pydrive2.drive import GoogleDrive\n  from pydrive2.auth import GoogleAuth\n  from google.colab import auth\n  from oauth2client.client import GoogleCredentials\n  auth.authenticate_user()\n  gauth = GoogleAuth()\n  gauth.credentials = GoogleCredentials.get_application_default()\n  drive = GoogleDrive(gauth)\n  print(\"You are logged into Google Drive and are good to go!\")\n\n\ndisplay_images = True\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n\n\ndisplay_images = False\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nlog_filename = os.path.join(jobname,\"log.txt\")\nsetup_logging(Path(log_filename))\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,\n    model_type=model_type,\n    num_models=2,\n    num_recycles=2,\n    relax_max_iterations=relax_max_iterations,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2], # edited\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    pairing_strategy=pairing_strategy,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n    user_agent=\"colabfold/google-colab-main\",\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n\n\njobname learning_af_e64d1\nsequence MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH\nlength 92\n2024-10-09 17:38:47,646 Running on GPU\n2024-10-09 17:38:48,066 Found 5 citations for tools or databases\n2024-10-09 17:38:48,067 Query 1/1: learning_af_e64d1 (length 92)\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:01 remaining: ?]\n\n\n2024-10-09 17:38:49,136 Sleeping for 5s. Reason: PENDING\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:07 remaining: ?]\n\n\n2024-10-09 17:38:55,066 Sleeping for 6s. Reason: PENDING\n\n\nRUNNING:   4%|▍         | 6/150 [elapsed: 00:13 remaining: 05:34]\n\n\n2024-10-09 17:39:02,012 Sleeping for 5s. Reason: RUNNING\n\n\nRUNNING:   7%|▋         | 11/150 [elapsed: 00:19 remaining: 03:56]\n\n\n2024-10-09 17:39:07,932 Sleeping for 8s. Reason: RUNNING\n\n\nRUNNING:  13%|█▎        | 19/150 [elapsed: 00:28 remaining: 03:01]\n\n\n2024-10-09 17:39:16,910 Sleeping for 7s. Reason: RUNNING\n\n\nRUNNING:  17%|█▋        | 26/150 [elapsed: 00:36 remaining: 02:38]\n\n\n2024-10-09 17:39:24,835 Sleeping for 8s. Reason: RUNNING\n\n\nCOMPLETE: 100%|██████████| 150/150 [elapsed: 00:47 remaining: 00:00]\n\n\n2024-10-09 17:39:36,982 Setting max_seq=512, max_extra_seq=5120\n2024-10-09 17:40:15,080 alphafold2_ptm_model_1_seed_000 recycle=0 pLDDT=80 pTM=0.667\n2024-10-09 17:40:21,715 alphafold2_ptm_model_1_seed_000 recycle=1 pLDDT=82.1 pTM=0.692 tol=0.583\n2024-10-09 17:40:28,410 alphafold2_ptm_model_1_seed_000 recycle=2 pLDDT=82.7 pTM=0.702 tol=0.111\n2024-10-09 17:40:28,412 alphafold2_ptm_model_1_seed_000 took 37.7s (2 recycles)\n2024-10-09 17:40:35,204 alphafold2_ptm_model_2_seed_000 recycle=0 pLDDT=84.9 pTM=0.742\n2024-10-09 17:40:42,062 alphafold2_ptm_model_2_seed_000 recycle=1 pLDDT=85.2 pTM=0.742 tol=0.834\n2024-10-09 17:40:48,939 alphafold2_ptm_model_2_seed_000 recycle=2 pLDDT=85.8 pTM=0.749 tol=0.14\n2024-10-09 17:40:48,940 alphafold2_ptm_model_2_seed_000 took 20.5s (2 recycles)\n2024-10-09 17:40:48,961 reranking models by 'plddt' metric\n2024-10-09 17:40:48,961 rank_001_alphafold2_ptm_model_2_seed_000 pLDDT=85.8 pTM=0.749\n2024-10-09 17:40:48,962 rank_002_alphafold2_ptm_model_1_seed_000 pLDDT=82.7 pTM=0.702\n2024-10-09 17:40:49,709 Done\n\n\n0\n\n\n\n\nCode\n#@markdown Display 3D structure {run: \"auto\"}\nimport py3Dmol\nimport glob\nimport matplotlib.pyplot as plt\nfrom colabfold.colabfold import plot_plddt_legend\nfrom colabfold.colabfold import pymol_color_list, alphabet_list\nrank_num = 1 #@param [\"1\", \"2\", \"3\", \"4\", \"5\"] {type:\"raw\"}\ncolor = \"lDDT\"\nshow_sidechains = True #@param {type:\"boolean\"}\nshow_mainchains = False\n\ntag = results[\"rank\"][0][rank_num - 1]\njobname_prefix = \".custom\" if msa_mode == \"custom\" else \"\"\npdb_filename = f\"{jobname}/{jobname}{jobname_prefix}_unrelaxed_{tag}.pdb\"\npdb_file = glob.glob(pdb_filename)\n\ndef show_pdb(rank_num=1, show_sidechains=False, show_mainchains=False, color=\"lDDT\"):\n  model_name = f\"rank_{rank_num}\"\n  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',)\n  view.addModel(open(pdb_file[0],'r').read(),'pdb')\n\n  if color == \"lDDT\":\n    view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})\n  elif color == \"rainbow\":\n    view.setStyle({'cartoon': {'color':'spectrum'}})\n  elif color == \"chain\":\n    chains = len(queries[0][1]) + 1 if is_complex else 1\n    for n,chain,color in zip(range(chains),alphabet_list,pymol_color_list):\n       view.setStyle({'chain':chain},{'cartoon': {'color':color}})\n\n  if show_sidechains:\n    BB = ['C','O','N']\n    view.addStyle({'and':[{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':BB,'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"GLY\"},{'atom':'CA'}]},\n                        {'sphere':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n    view.addStyle({'and':[{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n                        {'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n  if show_mainchains:\n    BB = ['C','O','N','CA']\n    view.addStyle({'atom':BB},{'stick':{'colorscheme':f\"WhiteCarbon\",'radius':0.3}})\n\n  view.zoomTo()\n  return view\n\nshow_pdb(rank_num, show_sidechains, show_mainchains, color).show()\nif color == \"lDDT\":\n  plot_plddt_legend().show()\n\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages."
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-2-how-do-we-interpret-the-results",
    "href": "notebooks/WS06_IntroductionToAF.html#part-2-how-do-we-interpret-the-results",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "If you were to download the outputs (packaged conveniently on the actual ColabFold notebook, or less conveniently in the files for this notebook), you will get several files of interest:\n\nSeveral “ranked” 3D pdb models \nAn image detailing the “coverage”\nAn image detailing the “pLDDT”\nAn image detailing the “PAE”\n\nLet’s look into what each means!\n\n\n“Coverage” is pretty straightforward- when AF looks into its database of sequences, the coverage shows how many matches it was able to assign for parts of the input sequence.\n\n\nCode\n#@markdown Run to show the MSA coverage of your protein - Is there a lot or a little coverage? (hint: more colors = more coverage)\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Which one has more coverage?', 'Trick question!!! The one one the left has lots of coverage, as it is a highly documented natural protein (Snase), with many similar sequences in the MSA database. On the other hand, the one on the right is a complex folded without MSAs. We want to do this when folding de novo proteins- as they are not as evolutionarily related/structrually related to similar sequences found in nature, and would mess with the inference.')\n\n\nWhich one has more coverage?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\npLDDT can be interpreted as the per-residue confidence that AF has in its prediction. A higher pLDDT means the prediction is more accurate.\nFrom Deepmind’s website,\n\n&gt;90 = High accuracy\n70-90 = Modeled well\n50-70 = Low confidence\n&lt;50 = Probably disordered region \n\nThe overall pLDDT is used to rank the outputs (and the structure we visualized is colored by pLDDT – blue is higher and red is lower)\n\n\nCode\n#@markdown Run to display your pLDDT score - Is it high confidence? Where might AF2 be less confident?\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which one is AF more confident in? Why?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Alphafold has more confidence in the right one! This is because the plddt is higher (there are less \"spikes\")')\n\n\nWhich one is AF more confident in? Why?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nRepresents AF’s confidence for the interaction of protein domains and residues. Given in a contact map between all residues.\nThe color at (x, y) corresponds to the expected distance error in residue x’s position, when the prediction and true structure are aligned on residue y.\nLower is better: * Low = well defined positions/orientation (blue) * High = uncertain relative positions (red)\nFor example: If a nanobody binds well to an antigen, we would expect to see more “blue” in the interaction regions between the two chains (see the question below for an example)\n\n\nCode\n#@markdown Run to view PAE charts - take time to understand what the x and y axes mean!\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\npae = \"\"\npae_file = os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\")\nif os.path.isfile(pae_file):\n    pae = image_to_data_url(pae_file)\n\ndisplay(HTML(f\"\"\"\n&lt;div style=\"max-width:90%; padding:2em;\"&gt;\n  { '&lt;!--' if pae == '' else '' }&lt;img src=\"{pae}\" class=\"full\" /&gt;{ '--&gt;' if pae == '' else '' }\n&lt;/div&gt;\n\"\"\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box(\"2 & 3 show weak interactions between the two proteins. However, it should be noted that the PAE diagram is not exactly a perfect mirror. If there is different aligned error based on which residue we align on, the difference that will be reflected in the diagram\")\n\n\nWhich quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nBelow is a summary of all the files produced after running AlphaFold\n\n\n\nimage.png"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-1-representations-of-our-input-sequence",
    "href": "notebooks/WS06_IntroductionToAF.html#part-1-representations-of-our-input-sequence",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Protein sequence information fed to AlphaFold 2 are stored as a fasta file. Here’s an example below:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD\n\n\n\nThe input amino acid sequence is used as a query to several databases (normally UniRef) of protein sequences and constructs a multiple sequence alignment (MSA). An MSA identifies similar (but not identical) sequences that have been identified in living organisms. How does one go from an alignment to a structure? The theory is that residues that coevolve are generally close to each other in the protein’s folded state.\nThe similar sequences pulled from also have structural information. These templates can be converted into distance matrices to determine the distance between residues\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\ndef create_quiz(question, correct_answer, decoy_answers):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is an MSA and why is it important for AF2?\"\ncorrect_answer = \"An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\"\ndecoy_answers = [\"An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\",\n                 \"An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\",\n                 \"An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\n        \n        \n        \n          \n          An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\n        \n        \n        \n          \n          An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\n        \n        \n        \n          \n          An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nHere, we also try to identify templates, which are proteins that may have similar structure to that of the input sequence. This is used to construct an initial representation of the structure, also known as the pair representation. By identifying conserved fragments, the model uses this as a guide to construct a structure.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the concept of “pair representation” in AF2?\"\ncorrect_answer = \"Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\"\ndecoy_answers = [\"Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\",\n                 \"Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\",\n                 \"Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\n        \n        \n        \n          \n          Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\n        \n        \n        \n          \n          Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\n        \n        \n        \n          \n          Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\n        \n        \n      Submit"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-2-evoformer",
    "href": "notebooks/WS06_IntroductionToAF.html#part-2-evoformer",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Here, we take the MSA and templates and pass them through a pair of transformers. Doing so (1) identifies which pieces of information are more informative, (2) iteratively exchanges information between MSA and pair representations, and (3) refines the representations for both the MSA and pair interactions.\nBelow is a deeper look into the Evoformer (which is composed of an MSA transformer and pair transformer). Left image: The MSA transformer identifies a correlation between the two columns of the MSA, each corresponding to a residue. This information is passed to the pair representation, where subsequently the pair representation identifies another possible interaction. Right image: Information is passed back to the MSA. The MSA transformer receives an input from the pair representation and observes another pair of columns that exhibit a significant correlation.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the role of the Evoformer in the AF2 architecture?\"\ncorrect_answer = \"The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\"\ndecoy_answers = [\"The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\",\n                 \"The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\",\n                 \"The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\n        \n        \n        \n          \n          The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\n        \n        \n        \n          \n          The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\n        \n        \n        \n          \n          The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nThe pair of transformers works on the principle of Triangle inequality, where the sum of two sides must be greater than or equal to the third side. Attention is arranged in terms of triangles of residues. Using this theorem, we can determine the likely distance residues have from one another because the distance between three points can never break that theorem\n\n\n\nimage.png"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-3-structure-module",
    "href": "notebooks/WS06_IntroductionToAF.html#part-3-structure-module",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "The structure module takes a refined MSA representation and refined pair representation, and uses this to construct a 3D model of the structure. The end result is a long list of cartesian coordinates representing the position of each atom of the protein (including side chains). Proteins are represented as a residue gas, where every amino acid is modeled as a triangle with points at the Nitrogen, R group Carbon and the Alpha Carbon. At the beginning of the structure module, all residues are placed at the origin of coordinates. At every step, the model produces a set of matrices that displace/rotate the residues in space. This representation does not reflect any physical or geometric assumptions, and as a result the network has a tendency to generate structural violations, which you can see in this video\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"How does the structure module of AF2 generate the final protein structure?\"\ncorrect_answer = \"The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\"\ndecoy_answers = [\"The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\",\n                 \"The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\",\n                 \"The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\"]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\n        \n        \n        \n          \n          The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\n        \n        \n        \n          \n          The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\n        \n        \n        \n          \n          The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is invariant point attention (IPA) and why is it important in AF2?\"\ncorrect_answer = \"IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\"\ndecoy_answers = [\"IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\",\n                 \"IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\",\n                 \"IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\n        \n        \n        \n          \n          IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\n        \n        \n        \n          \n          IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\n        \n        \n        \n          \n          IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the Frame Aligned Point Error (FAPE) and how is it used in AF2?\"\ncorrect_answer = \"FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\"\ndecoy_answers = [\"FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\",\n                 \"FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\",\n                 \"FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\n        \n        \n        \n          \n          FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\n        \n        \n        \n          \n          FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\n        \n        \n        \n          \n          FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nAn important note is that this model works iteratively. After generating a structure, the model takes all the information (MSA representation, pair representation, and predicted structure) and passes it back to the beginning of the Evoformer blocks. Check this video out of the model refining it’s predictions across blocks over multiple iteractions.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\n# Define the question and answers\nquestion = \"How does AF2 refine its predictions iteratively?\"\ncorrect_answer = \"Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\"\ndecoy_answers = [\"AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\",\n                 \"AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\",\n                 \"AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\"]\n\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\n        \n        \n        \n          \n          AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\n        \n        \n        \n          \n          AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\n        \n        \n        \n          \n          Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\n        \n        \n      Submit"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#onwards",
    "href": "notebooks/WS06_IntroductionToAF.html#onwards",
    "title": "Section 1: Introduction: A bit of history",
    "section": "",
    "text": "Alphafold’s architecture and concepts have inspired other deep-learning based protein folding algorithms as well, such as RosettaTTAFold2 and ESMFold. Each model has its advantages and disadvantages, dependent on its accuracy (with respect to various protein types), which oftentimes trades off with runtime (more accurate models take longer to run). Based on your desired task, whether it be designing a de novo protein or investigating a natural complex, it may be beneficial to choose a particular model to use. However, that is beyond the scope of this tutorial.\nAnother active field of progress is predicting folding with ligands and post-translational modifications. At the time of writing, the state of the art includes RosettaTTAFold All Atom, AF3, HelixFold3, and Chai1. These models are been able to account for ligands in folding, such as DNA or inhibitors. In particular, HelixFold-single and Chai1 look to circumvent MSA usage by using protein language models instead! Feel free to check all of them out!"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "DL4Proteins-notebooks",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Gray Lab\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "notebooks/WS08_DenoisingDiffusionProbabilisticModels.html",
    "href": "notebooks/WS08_DenoisingDiffusionProbabilisticModels.html",
    "title": " Section 0: Overview ",
    "section": "",
    "text": "Introduction to Denoising Diffusion Probabilistic Models (DDPMs) \n##Return to HomePage\nCode and tutorial adapted from workshop by Sergey Lyskov\nAdditional background reading: Denoising Diffusion Probabilistic Models by Ho et al.\nOBJECTIVES: By the end of this notebook you should be able to:\n\nExplain how diffusion models generate data\nDefine the two components of a diffusion model\nAt a high level, describe the mathematical forms and simplifications\nImplement the forward and reverse diffusion processes in code\nInterpret the loss term(s) and the parameters that the model learns\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n  Table of Contents \nSection 0: Overview\nSection 1: Forward Process * Adding Noise to Data * Scheduling the Noise Addition * Sequential vs Parallel Generation * Implementing the forward Process using Sine Waves\nSection 2: Reverse Process * Model Output: Generating the original data * Loss Term\nSection 3: DDPM Implentation * Time Embedings * Neural Network * Training * Generation * Posterior Variance\nSection 4: Summary & Questions * Understanding Diffusion Models * Mathematical Foundation * Questions\n\n\n\nA denoising diffusion model is a generative model that generates a data sample from noise. If you have ever generated images from a text input using tools like DALLE-2, Imagen or Stable Diffusion then you already know the power of diffusion models for data generation. Compared to other generative models like GANs (Generative Adversarial Networks) and VAEs (Variational AutoEncoders), DDPMs produce images of better quality at a lower computational cost.\nThere are two processes in the diffusion architecture -\n\nA Forward Process that destroys the data in the sample and converts it to noise.\nA Reverse Process that predicts the added noise and returns the original/similar images.\n\nSuppose the orange box represents the entire space of all possible data representations. Within this space, we are particularly interested in a subset of data points, which is enclosed in the white circle. The green dot serves as an example of the kind of data we are interested in, lying within this subset.\nApplying the diffusion process to the green dot, the data is pushed away from the white circle and into the broader region of the data space. The diffusion model is trained to guide the green dot back to its original position within the white circle.\n\n\n\nimage.png\n\n\nImage Source: https://learnopencv.com/denoising-diffusion-probabilistic-models/\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Question Time! Run to Show Question &lt;/b&gt;\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct! Only the reverse process requires a neural network. &lt;br&gt; Since the forward process makes the data noisy, we can use a predefined algorithm to implement the noising!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect. Only the reverse process requires a neural network. &lt;br&gt; Since the forward process makes the data noisy, we can use a predefined algorithm to implement the noising!\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"At which step is the neural network used? Select all the correct answers.\"\ncorrect_answers = [\"Reverse Process\"]\ndecoy_answers = [\"Forward Process\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    At which step is the neural network used? Select all the correct answers.\n        \n        \n        Reverse Process\n        \n        \n        \n        Forward Process\n        \n    Submit\n    \n    \n    \n\n\nLet us explore the forward and reverse processes in detail.\nRun the following cell to load the standard packages for neural networks and activate the GPU (if available).\n\n\nCode\n#Run this cell to load the standard packages for neural networks\nimport random, math\nfrom tqdm import tqdm # fun package to display a status bar during time consuming loops\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nrandom.seed(0);\ntorch.manual_seed(0)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n Section 1: Forward Process \nThe forward process, described by distribution \\(q\\), gradually adds noise to the data. It does so over a number of timesteps until the data is transformed into pure Gaussian noise. Taking the example of an image, we start with a real image \\(x_0\\) at \\(t = 0\\) and at every timestep \\(t\\) add some noise to the image at the previous time step. The noise is typically sampled from a from a Gaussian distribution with a zero mean and unit standard deviation. Thus,\n\\[x_t = x_{t-1} + \\text{noise}\\].\nThe forward process can be visualized like this:   Image Source: Step by Step visual introduction to Diffusion Models [https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models]\nTo understand the diffusion process we wil build a simple model from standard PyTorch parts. Let’s make a one dimensional dataset of sine waves that we will noise and denoise.\nEach sine wave is created by randomly sampling a phase, frequency, and amplitude. We sample a set of 32 points (input_size), with 64 sine waves in each batch (batch_size). As a simplest possible case, we’ve set the weight variable for frequency and amplitude to 0.\nFeel free to play with these variables in subsequent rounds through this notebook.\n\n\nCode\ninput_size = 32\nbatch_size = 64\n\nhistorical_total_loss = []\n\nclass SineFunctionDataSet(Dataset):\n  def __init__(self, size):\n    super().__init__()\n    self.input_size = input_size\n    data = []\n\n    for i in range(size):\n      phase = 10*random.random() * 2.0 * 3.15 * 10\n      freq = 14 + 0 * random.random()\n      amplitude = 1 + 0*random.random()\n\n      # equation for the sin wave: y = amplitude * sin( x * frequency / input_size + phase)\n      data.append( torch.Tensor([ amplitude * math.sin(t_sin*freq/input_size + phase) for t_sin in range(input_size) ] ))\n\n    self.data = data\n\n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self, i):\n    return self.data[i]\n\nsine_data = SineFunctionDataSet(1024*8)\ndata_loader = DataLoader(sine_data, batch_size = batch_size, drop_last=True)\n\n\n\n\nCode\n#visualizing the data\nfor i in sine_data[0:8]:\n  plt.plot(i)\n  plt.xlabel(\"Time\")\n  plt.ylabel(\"Amplitude\")\n  plt.title(\"Input Sine Waves\")\n\n\n\n\n\n\n\n\n\nWe can sample from this distribution to get a data sample, \\(x_0\\)\nWe define the forward process \\(q\\), that takes the data \\(x_{t-1}\\) at timestep \\(t-1\\) as input and produces the sample at the next timestep \\(t\\) by adding Gaussian noise. For the first timestep, \\(q_{t_0} \\sim x_0\\) The amount(or weight) of noise to be added follows a variance schedule \\(\\beta_t\\).\nThus, the forward process is defined as: \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\bf{I}),\\]\nwhere \\(\\mathcal{N}\\) is a normal distribution (aka a Gaussian distribution), defined by two parameters: a mean \\(\\mu_t = \\sqrt{1-\\beta_t}x_{t-1}\\) and a variance \\(\\sigma^2 = \\beta_t\\).\nThe equation can be reparameterized as \\[q(x_t | x_{t-1}) = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t} \\mathcal{E},\\] where \\(\\mathcal{E}\\in\\mathcal{N}(0,1)\\).\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Question Time! Run to Show Question &lt;/b&gt;\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect &lt;br&gt; Since we are adding noise at every timestep, the final output is Gaussian Noise!\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is the final output of the forward diffusion process?\"\ncorrect_answers = [\"Gausian Noise\"]\ndecoy_answers = [\"Denoised Data\", \"Randomized Inputs\", \"Original Features\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the final output of the forward diffusion process?\n        \n        \n        Denoised Data\n        \n        \n        \n        Randomized Inputs\n        \n        \n        \n        Original Features\n        \n        \n        \n        Gausian Noise\n        \n    Submit\n    \n    \n    \n\n\nWe start from \\(x_0\\), and produce \\(x_1\\), …, \\(x_t\\), …, \\(x_T\\), where \\(x_T\\) is pure Gaussian noise. We need a schedule \\(\\beta_t\\) to control how much noise is added. Typically, \\(\\beta_t\\) increases with every time step such that progressively more noise is added through the process:\n\\[0 &lt; \\beta_1 &lt; \\beta_2 &lt; ... &lt; \\beta_T &lt; 1\\]\n\n\nCode\n#@markdown A simple example of a noise schedule is a linear function with increasing $\\beta$s\n\n#@markdown The max value of $\\beta$ is limited to keep the variance from exploding\nbetas = torch.arange(0, .5, 0.01)\nplt.plot(betas)\nplt.xlabel(\"timestep\")\nplt.ylabel(\"beta\")\n\nplt.annotate('Max Beta', xy=(49, 0.5), xytext=(29, 0.1), fontsize=12,\n            arrowprops=dict(facecolor='green', shrink=0.05));\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Food For Thought! Run to Show Question &lt;/b&gt;\n\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;{correct_answer}&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: black;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box(\"What is the problem with an exploding gradient?\", \"Recall how a neural network is trained: \"\n                  \"at the end of the forward pass the derivatives are \"\n                  \"backpropagated and used to update the model weight. When the variance \"\n                  \"is too high these derivatives dominate the model weights, making \"\n                  \"it difficult to converge to a minimum during gradient descent.\"\n                  \"Refresh the concepts from one of the earlier notebooks if this difficult to follow\")\n\n\nWhat is the problem with an exploding gradient?\n\n\n\n\n\n\n    \n    \n\n\nHowever, researchers from OpenAI in 2021 found that a linear schedule is not the most efficient. When using a linear schedule, most of the original image is lost after around half of the total steps. The remaining data is not very useful to learn from. To overcome this drawback and utilize the training time effectively, researchers designed custom schedules with slower addition of noise. One such useful schedule is a cosine schedule. From the image below we observe that the cosine schedule on the bottom preserves more information in the earlier timesteps while adding greater noise in the later timesteps:\n\nVisualization of different schedules. Top Panel: Linear Schedule Bottom Panel: Cosine Schedule\nImage Source: Improved Denoising Diffusion Probabilistic Models, Alex Nichol, Prafulla Dhariwal 2021\n\n\nCode\n# Cosine schedule\nbetas_linear = torch.arange(0, .5, 0.01)\nbetas = 1 - ( betas_linear * torch.pi / 2 ).cos()\nplt.plot(betas);\nplt.xlabel(\"timestep\");\nplt.ylabel(\"beta\");\n\n\n\n\n\n\n\n\n\nAs described above, the forward diffusion process starts at \\(t_0\\) and moves sequentially to time \\(t_T\\) because the sample at each timestep depends on the previous one. Adding noise step by step is expensive. To make the process faster, the we can convolute the effect of multiple steps to produce one formula that defines the output at any desired timestep (details here).\n\\[q(x_t | x_0) = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\mathcal{E}\\] where \\[\\alpha_t = 1 - \\beta_t\\] \\[\\bar{\\alpha}_t := Π^t_{s=1} a_s\\].\nLet’s calculate and store the \\(\\alpha\\) values:\n\n\nCode\nalphas = torch.Tensor( [1.0 - b for b in betas] )\nalphas_cumprod = torch.cumprod(alphas, dim=0)\nsqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\nsqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\nsqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n\n# let's also store our previous alphas, which will come in use later\nalphas_prev = F.pad(alphas[:-1], (1, 0), value=1.0)\nalphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n\nplt.plot(sqrt_one_minus_alphas_cumprod, label = 'sqrt one minus alphas cumprod')\nplt.plot(alphas, label ='alpha')\nplt.xlabel(\"timestep\");\nplt.ylabel(r\"alpha, $bar{alpha}$\");\nplt.legend();\n\n\n\n\n\n\n\n\n\nNow we have our various constants. Next let’s make a function to add noise to data at some timestep \\(t\\)\n\n\nCode\ndef noised_data(x0, t, noise=None):\n    if noise is None: noise = torch.randn_like(x0)\n\n    alpha_t = alphas_cumprod[t]\n\n    x = x0 * torch.sqrt(alpha_t) + torch.sqrt(1.0 - alpha_t) * noise\n\n    return x, noise\n\n\nLet’s examine an example of adding noise to a sine wave, where the image evolves through the diffusion process, gradually becoming noisier as the variance decreases over time according to the specified schedule.\n\n\nCode\nimport time\nimport pylab as pl\nfrom IPython.display import display, clear_output\n\nexample = next(iter(data_loader))\nfor i in range(len(betas)):\n  plt.clf()\n  plt.plot( noised_data(example, i)[0][0] )\n  plt.ylabel(\"noised data (x_t)\")\n  plt.ylim(-1.5,1.5)\n  plt.text(0, 1.25, 'time $t = $'+str(i), fontsize = 22)\n  display(pl.gcf())\n  clear_output(wait=True)\n  time.sleep(0.04)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run to Show Question &lt;/b&gt;\n\ncreate_answer_box('Can we have a constant variance schedule? How will it impact the forward process?',\n                  'Yes we can! But it would make the noise uniform and suboptimally utilze the learning capacity of the model. The choice of variance schedule is important in deciding the performance of the model.')\n\n\n\n    \n\n\nCan we have a constant variance schedule? How will it impact the forward process?\n\n\n\n\n\n\n    \n    \n\n\n\n    \n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; This is a trickier question but give it a try! Run to Show Question &lt;/b&gt;\n\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    # Function to display the answer when the button is clicked\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;{correct_answer}&lt;/pre&gt;\"\n        display(HTML(mathjax_script))  # Refresh MathJax to render equations\n\n    # Create a text area for user input\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n\n    # Create a submit button\n    submit_button = widgets.Button(description=\"Submit\")\n\n    # Create an HTML widget to display the correct answer\n    result = widgets.HTML(value=\"\")\n\n    # Create a vertical box layout for the text area, button, and result\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Bind the button click event to the display_answer function\n    submit_button.on_click(display_answer)\n\n    # Display the question with LaTeX support using MathJax\n    mathjax_script = \"\"\"\n    &lt;script type=\"text/javascript\" async\n      src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;\n      &lt;/script&gt;\"\"\"\n\n    # Display the question and widgets\n    display(HTML(mathjax_script))\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom styling for the output box\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: black;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage with LaTeX equations in both question and correct answer\nquestion = r\"Here is the forward process equation: $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\mathbf{I})$$ Why do we add a weight term to the mean? $$ weight = \\sqrt{1-\\beta_t} $$\"\n\n# Correct answer with LaTeX for \\beta and other symbols\ncorrect_answer = r\"\"\"By weighing the x_t term we reduce the contribution of the original signal gradually, ensuring that the final output of the forward process is nearly Gaussian noise. To visualize this, consider a one-dimensional data distribution consisting of a waveform with multiple peaks, as shown below. The goal of the forward process is to destroy the data and convert it to a normal distribution with a single peak (mean) at 0 and unit variance.\n&lt;img src='https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_process_changing_distribution.png' width='400px'&gt;\nImage Source: https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html\n\"\"\"\ncreate_answer_box(question, correct_answer)\n\n\n\n    \n\n\nHere is the forward process equation: $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\mathbf{I})$$ Why do we add a weight term to the mean? $$ weight = \\sqrt{1-\\beta_t} $$\n\n\n\n\n\n\n    \n    \n\n\n\n\nSection 2: Reverse Process \nIn the reverse process the model learns to generate the original image from the noised version of it.\n Extra geeky detail: While we want the model to generate data that resembles the training data, we do not aim for exact replicas. Instead, the goal is to ensure that the generated data belongs to the same distribution as the original images. For example, if the model is trained on images of yellow sunflowers, it should be capable of generating sunflowers with different shades of yellow, introducing natural variations rather than reproducing a single, fixed hue. This ensures that the model captures the diversity within the data distribution, rather than simply memorizing individual data points. \nIf we knew the conditional distribution \\(p(x_{t-1}|x_t)\\), ie, if we knew the dependence of the less noisy image \\(x_{t-1}\\) on the noisier image \\(x_t\\) then we could run the process in reverse by sampling from some Gaussian noise \\(x_T\\) and then gradually denoising it so that we end up with a sample from a real distribution \\(x_0\\).\nHowever, we don’t know \\(p(x_{t-1}|x_t)\\). It’s intractable since we would need to know the distribution of all possible \\(x_{t-1}\\) that could result in a particular \\(x_t\\) to calculate this conditional probability. Hence, we’re going to use a neural network to predicts the conditional distribution \\(p(x_0|x_t)\\).\nThe model’s prediction is represented by \\(p_\\theta (x_{t-1}|x_t)\\), with \\(\\theta\\) being the parameters of the neural network, updated by gradient descent. For each step, the functional form that is typically chosen is a Gaussian:\n\\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1} ; \\mu_\\theta (x_t, t),\\sigma^2_\\theta(x_t, t ))\\]\nwhere the mean and variance are also conditioned on the noise level, \\(t\\). In this way, our network now needs to predict these parameters for the Gaussian distribution, \\(\\mu\\) and \\(\\sigma\\).\n\n\n\nimage.png\n\n\nImage Source: https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models\n\n\n THE LOSS TERM \n\n\nAt every time step, the difference between the noised image and the predicted image defines the added noise.\nThe total loss of the model is a sum of the losses at every time step:\n\\[L = L_0 + L_1 + L_2 + L_3 + L_4 ...  + L_T\\]\nWe can ignore the \\(L_T\\) loss because at that timestep there are no neural network parameters to be learnt and the best we can do is define a good variance scheduler and use appropriately sized timesteps.\nEmpirically, the authors of the denoising diffusion probabilistic models paper found that the model performed better without the \\(L_0\\) term, so it is often ignored during training.\nThen can use the Kullback–Leibler divergence to estimate \\(L_t\\) which is just measuring the difference between the original probability distribution and the predicted distribution.\n\\[ D_{KL}(q(x_t| x_0) ∥ p_θ(x_t| x_t−1)) = - ELBO + log(p_θ(x_0| x_t)) \\]\nWe want to minimize \\(D_{KL}\\) and the conditional probabilty is a constant So the only term we can use for training is the ELBO. The Evidence Lower Bound (ELBO) combines the reconstruction loss with a regularization term that encourages the model to learn a smooth, continuous distribution.\nMaximizing ELBO (thus minimizing \\(D_{KL}\\)) ensures that the model not only fits the data well but also generalizes effectively to new data.\nInstead of predicting the original probability distribution at each time step we can instead predict just the noise by reparameterizing the \\(p_θ\\) equation\n\\[x_{t-1}  = \\mathcal{N}(x_{t-1} ; \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\mathcal{E_\\theta}(x_t, t)), \\sqrt{\\beta_t}\\mathcal{E})\\]\nLearn more about variational inference at: https://gregorygundersen.com/blog/2021/04/16/variational-inference/\nWe went through a lot of new concepts but the key takeaway is that the model is trained to predict the noise at each step and the final loss term that we use for training is an Mean Squared Error (MSE) between the noise added in the forward process and the noise predicted by the model.\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run the cell for the question&lt;/b&gt;&lt;/font&gt;\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct! &lt;br&gt;  As discussed in the above section, at each step the model predicts the noise which has to be removed to obtain the original data\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect. &lt;br&gt; As discussed in the above section, at each step the model predicts the noise which has to be removed to obtain the original data\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\nquestion = \"What is the model trained to predict at each time step?\"\ncorrect_answers = [\"Noise Added to the Data\"]\ndecoy_answers = [\"Conditional Probability\", \"Randomized Probability Distribution\", \"Original Data\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the model trained to predict at each time step?\n        \n        \n        Conditional Probability\n        \n        \n        \n        Randomized Probability Distribution\n        \n        \n        \n        Noise Added to the Data\n        \n        \n        \n        Original Data\n        \n    Submit\n    \n    \n    \n\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run the cell for the question&lt;/b&gt;&lt;/font&gt;\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\nquestion = \"What is the final objective used to guide model training?\"\ncorrect_answers = [\"Minimize the difference between predicted and original distribution\"]\ndecoy_answers = [\"Predict the conditional probability\", \"Maximize the difference between predicted and original distribution\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the final objective used to guide model training?\n        \n        \n        Maximize the difference between predicted and original distribution\n        \n        \n        \n        Minimize the difference between predicted and original distribution\n        \n        \n        \n        Predict the conditional probability\n        \n    Submit\n    \n    \n    \n\n\n\n\n Section 3: DDPM Implementation (MODEL ARCHITECTURE) \n### \n\n 1. TIME EMBEDDINGS \n\nAs mentioned earlier, the model can learn from all the timesteps of a particular sample without going sequentially. Yet it is important for the model to know which timestep the data belongs to. To implement this, we define the SinusoidalTimeEmbeddings class. The Time embeddings class converts the timestep (in number) to a vector in latent space of the same dimension as the data. At each block of the MLP, these embeddings are added to the model. This is similar to the self attention embeddings in the “Attention is All You Need” paper (Vaswani et al., 2017). The embeddings allow the neural network “know” at which particular time step (noise level) it is operating, for every image in a batch.\nThe forward method takes a tensor representing the time (noise levels of several noisy images in a batch) as input and outputs Time embeddings of shape (batch_size, dim). The Time embeddings are computed using sinusoidal functions and concatenated together to form the final embeddings.\n\n\n\nimage.png\n\n\n\n\nCode\n# The SinusoidalTimeEmbeddings module takes a tensor of shape (batch_size, 1) i.e. (64,1)\n# as input (i.e. the noise levels of several noisy images in a batch), and turns this\n# into a tensor of shape (batch_size, dim), with dim being the dimensionality of the\n# Time embeddings. This is then added to each residual block, as we will see further.\nclass SinusoidalTimeEmbeddings(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        # half of embedding dim\n        half_dim = self.dim // 2\n        # scaling factor for sinusoidal Time embeddings\n        embeddings = math.log(10000) / (half_dim - 1)\n        # generate sinusoidal Time embeddings\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        # expand time tensor along second dimesion, multiply with embeddings\n        embeddings = time[:, None] * embeddings[None, :]\n        # concatenate sine and cosing embeddings along last dimension\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n\n\n### \n\n 2. NEURAL NET \n\nNow that we have our embeddings, we use a multi layer perceptron model for our simple dataset consisting of multiple deeply connected layers followed by activation functions.\nDiffuser_MLP predicts the denoised data \\(x_0\\) from the noised state \\(x_t\\) at time \\(t\\). It takes the input size (input_size), an activation function (fun) to be applied between layers, and a list of layer sizes (layers), and it returns an output the same size as the input.\nThe forward method takes an input tensor x and a time tensor, applies sinusoidal time embeddings to the time tensor, concatenates it with the input tensor, and feeds it through the MLP layers.\n\n\n\nimage.png\n\n\n\n\nCode\nclass Diffuser_MLP(nn.Module):\n  \"\"\"this model learns to predict the denoised data x_0 from the noised state x_t and the time t\"\"\"\n  def __init__(self, input_size, *, fun=None, layers=None):\n    \"\"\"fun is the activation function; layers is a list of integers, with the integer indicating the width of the embedding dimension as a factor of the input data size\"\"\"\n    super().__init__()\n\n    # dim of time embedding based on size of betas\n    self.time_embedding_dim = math.ceil( math.log2(betas.shape[0]) ) * 2\n    # initialize SinusoidalTimeEmbeddings\n    self.time_embedding_nn = SinusoidalTimeEmbeddings(self.time_embedding_dim)\n    # initialize sequential NN\n    self.net = torch.nn.Sequential()\n\n    # compute initial size of input to NN\n    prev_size = input_size + self.time_embedding_dim\n    # iterate through layers\n    for l in layers:\n      # compute layer size\n      new_size = int(input_size*l)\n      # add linear layer to NN\n      self.net.append( torch.nn.Linear(prev_size, new_size) )\n      # append activation function\n      self.net.append( fun() )\n      # update previous size for next layer\n      prev_size = new_size\n\n    # append linear layer to NN to map final size to input\n    self.net.append( torch.nn.Linear(prev_size, input_size) )\n\n  def forward(self, x, time):\n    # compute time embedding for given time\n    time_embedding = self.time_embedding_nn(time).squeeze(1)\n    # concatenate input data with time embeddings\n    x_t = torch.cat((x, time_embedding), -1)\n    # pass concatenated data through NN\n    output = self.net(x_t)\n    return output\n\n\nLet us instantiate our model, provide it with input data, compute the output of the model, and check that our data objects have the right shape.\n\n\nCode\n# instantiate\ndiff = Diffuser_MLP(input_size, fun=torch.nn.ReLU, layers=(3, 3) )\n\n# example time tensor by expanding single value along batch dimension to match 'example' shape\nexample_time = torch.Tensor([0]).expand(example.shape[0], 1)\n\n# pass exmaple data and exmaple_time through diffuser NN to get output\noutput = diff(example, example_time)\n\n# shape of input data, example_time, and output tensor\nexample.shape, example_time.shape, output.shape\n\n\n(torch.Size([64, 32]), torch.Size([64, 1]), torch.Size([64, 32]))\n\n\n### \n\n 3. TRAINING \n\nTime to train! Within each epoch, we will iterate through the data loader to process batches of data. For each batch, a random time step is chosen and noisy data is generated according to the cosine scheduler we defined earlier. The model processes this noisy input data along with the corresponding time information and computes the output. A mean squared error loss is then calculated between the model’s output and the noisy data. Subsequently, gradients are zeroed, and a backward pass is executed to update the model’s parameters using the optimizer. Throughout the training process, the total loss is accumulated for logging purposes, and the average loss per step is computed.\nAn Adam optimizer is employed to optimize the parameters of the diffusion model. An ExponentialLR scheduler is utilized to modulate the learning rate during training, providing adaptive adjustments to facilitate convergence. In the main training loop, the model is trained over multiple epochs, with each epoch comprising a series of training steps. Following each epoch’s completion, the learning rate is updated using the scheduler.\n\n\nCode\ndef train(model, optimizer, *, n_steps):\n    # Set the model in training mode\n    model.train()\n\n    # Define the loss function as Mean Squared Error\n    loss_fn = nn.MSELoss()\n\n    # Iterate through training steps\n    for epoch in range(n_steps):\n        total_loss = 0\n        total_loss_count = 0\n\n        # Iterate through data batches\n        for batch in data_loader:\n            # Randomly select a time\n            t = random.randrange(0, betas.shape[0])\n\n            # Generate noised data for the selected time step\n            input, noise = noised_data(batch, t)\n\n            # Pass the input data and time to the model to get the output\n            output = model(input, torch.Tensor([t]).expand(input.shape[0], 1))\n\n            # Compute the loss between the output and the noise\n            loss = loss_fn(output, noise)\n\n            # Compute the mean loss\n            loss = loss.mean()\n\n            # Reset gradients\n            optimizer.zero_grad(set_to_none=True)\n\n            # Perform backpropagation\n            loss.backward()\n\n            # Update model parameters\n            optimizer.step()\n\n            # Accumulate total loss\n            total_loss += loss\n            total_loss_count += 1\n\n    # Compute the total loss average over the training batch\n    total_loss = total_loss / total_loss_count\n\n    # Append the total loss to historical_total_loss list\n    with torch.no_grad():\n        historical_total_loss.append(total_loss.item())\n\n    # Return the last historical total loss\n    return historical_total_loss[-1]\n\n# Initialize Adam optimizer for the Diffuser_MLP model parameters with learning rate 1e-3\noptimizer = torch.optim.Adam(diff.parameters(), lr=1e-3)\n\n# Initialize ExponentialLR scheduler for the optimizer with gamma 0.9\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n# Iterate training for 8 epochs\nfor i in range(8):\n    # Perform training for 32 steps\n    total_loss = train(diff, optimizer, n_steps=32)\n\n    # Adjust learning rate using lr_scheduler\n    lr_scheduler.step()\n\n    # Clear previous plot and plot historical total losses\n    pl.clf()\n    plt.yscale('log')\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.plot(historical_total_loss[:])\n    display(pl.gcf())\n\n    # Calculate mean loss over the last 4 epochs\n    mean_loss = torch.Tensor(historical_total_loss[-4:]).mean().item()\n\n    # Extract the last 4 losses\n    last_losses = historical_total_loss[-4:]\n\n    # Print the last 4 losses and the mean loss\n    print(f'{last_losses=}\\n{mean_loss=}')\n\n    # Clear output for the next iteration\n    clear_output(wait=True)\n\n# Print the last 4 losses and the mean loss after completing all epochs\nprint(f'{last_losses=}\\n{mean_loss=}')\n\n\nlast_losses=[0.1390904188156128, 0.11419769376516342, 0.12434312701225281, 0.12849435210227966]\nmean_loss=0.12653139233589172\n\n\n\n\n\n\n\n\n\n### \n\n 4. GENERATION \n\nNow we can sample from a trained diffusion model to generate synthetic images. We will iteratively sample from the diffusion model in reverse order, starting from the final time step and progressing backward in time. Each sample is generated based on the model’s mean output and optionally augmented with Gaussian noise.\nThere is one final concept needed to be understood before we can proceeed - Posterior Variance\nDuring sampling phase, the goal is to generate new data from the learned model that resembles the training data. The posterior variance helps manage the uncertainty inherent in the data generation process and guides how much noise to add or remove at each step to achieve realistic and varied outputs.\nFor example a diffusion model that is used to generate images learns both the mean of the image distribution and the variance of the data. If the model only learns the mean of the image distribution but ignores the variance, it might generate images that are too similar to each other and lack diversity. By incorporating posterior variance, the model can generate a range of images with subtle differences, reflecting the true variability present in the dataset.\nThe posterior variance is given by the formula\n\nwhere the \\(\\bar{\\mu_t}, \\bar{\\beta_t}\\) depend on the cumulative products.\n\n\nCode\nposterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\nplt.plot(posterior_variance)\nplt.xlabel(\"timestep\");\nplt.title(\"Posterior Variance\");\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Decorator to temporarily disable gradient computation\n@torch.no_grad()\ndef sample(model, x, t, t_index):\n    # Extract necessary parameters for the current time step\n    betas_t = betas[t]\n    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n    sqrt_recip_alphas_t = sqrt_recip_alphas[t]\n\n    # Compute the mean of the model's output\n    model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t)\n\n    # If it's the first time step, return the model mean\n    if t_index == 0:\n        return model_mean\n    else:\n        # Extract posterior variance for the current time step\n        posterior_variance_t = posterior_variance[t]\n\n        # Generate Gaussian noise\n        noise = torch.randn_like(x)\n\n        # Return the sample by adding noise scaled by the square root of the posterior variance to the model mean\n        return model_mean + torch.sqrt(posterior_variance_t) * noise\n\n# Generate random input tensor 'x'\nx = torch.randn((input_size,)).unsqueeze(0)\n\n# Iterate through time steps in reverse order\nfor t in torch.arange(betas.shape[0]-1, -1, -1):\n    # Clear the previous plot\n    pl.clf()\n    if torch.remainder(t,2) == 0:\n      # Plot the current state of 'x' without gradients\n      plt.plot(x[0].detach())\n      plt.text(0, 1.25, 'time $t = $'+str(t.item()), fontsize = 22)\n      # plt.annotate(f'{t}', xy=(0,1))\n      # Display the plot\n      display(pl.gcf())\n\n      # Pause for a short duration\n      time.sleep(.02)\n\n      # Clear the output for the next iteration\n      clear_output(wait=True)\n\n    # Expand 't' tensor to match the shape (1, 1)\n    T = torch.Tensor([t]).to(torch.long).expand(1, 1)\n\n    # Sample a new 'x' using the model and the current time step\n    x = sample(diff, x, T, t)\n\n\n\n\n\n\n\n\n\n\n\n\n  Section 4: Summary and Review Questions\nDiffusion models offer a powerful and flexible approach to generative modeling, particularly in scenarios where high-quality, diverse samples are required. This workshop explored the concepts and implementation of Denoising Diffusion Probabilistic Models (DDPMs) in the following key areas:\n 1. Understanding Diffusion Models \n- Forward Process: The forward process involves gradually adding noise to data, converting it into pure noise over several steps. - Reverse Process: The reverse process aims to denoise the noisy data step-by-step, reconstructing the original data or generating new samples.\n 2. Mathematical Foundation  - Diffusion Equations: The mathematical equations governing both the forward and reverse diffusion processes were explained, including the noise schedule. -Loss Term: The cumulative loss and ELBO were introduced. - Posterior Variance: The role of posterior variance in ensuring diversity and stability during the sampling process was discussed.\nBy now you should have a good understanding of diffusion models’ logic, mathematics, and application.\n Try answering the following questions to test your knowledge!\n\nWhat is the purpose of the forward process q in a diffusion model? 2.What is the significance of the variance schedule \\(β_t\\) in the forward process?\nExplain the difference between a linear noise schedule and a cosine noise schedule.\nWhat is the goal of the reverse process in a diffusion model?\nWhy is the conditional distribution \\(p(x_{t−1}∣x_t)\\) important in the reverse process?\nWhy can’t we directly use the true conditional distribution \\(p(x_{t−1}∣x_t)\\) in practice?\nHow do we approximate the conditional distribution \\(p(x_{t−1}∣x_t)\\) in practice?\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run cell for all answers!&lt;/b&gt;&lt;/font&gt;\n\n\nprint(\"1. The forward process q gradually adds noise to the data over a number of timesteps until we end up with pure Gaussian noise. \\n The purpose of this process is to transform the original data x_0 into increasingly noisy versions x_t​ until it becomes pure Gaussian noise at the final timestep T\")\nprint(\"2. βt controls the amount of noise added at each timestep. It starts with a small value and gradually increases such that the data becomes more noisy over time. \\nThe final timestep T has the highest noise level, ensuring the data has been transformed into pure Gaussian noise. The schedule is designed to prevent the variance from exploding while ensuring the gradual transformation.\")\nprint(\"3. A linear noise schedule increases the noise linearly over time: the original image information is quickly lost after about half of the total steps.\\n A cosine noise schedule, on the other hand, adds noise more slowly at the beginning and increases it more rapidly towards the end. This helps in preserving the information in the earlier timesteps, making the denoising process more efficient and retaining more of the original data information.\")\nprint(\"4. To generate the original image from its noised version. This involves starting with Gaussian noise and gradually denoising it to produce a sample from the real data distribution.\")\nprint(\"5. p(xt−1∣xt) is important because it describes how to transition from the noised version at time t to a slightly less noised version at time t−1.\\n Knowing this distribution allows us to iteratively denoise an image.\")\nprint(\"6. We can't directly use p(xt−1∣xt) because it is intractable. Calculating this distribution would require knowing the distribution of all possible samples, which is computationally infeasible.\")\nprint(\"7. We approximate p(xt−1∣xt) using a neural network. The neural network learns to predict the conditional probability distribution, denoted as pθ(xt−1∣xt), \\n where θ represents the parameters of the neural network.\")\n\n\n\n\n1. The forward process q gradually adds noise to the data over a number of timesteps until we end up with pure Gaussian noise. \n The purpose of this process is to transform the original data x_0 into increasingly noisy versions x_t​ until it becomes pure Gaussian noise at the final timestep T\n2. βt controls the amount of noise added at each timestep. It starts with a small value and gradually increases such that the data becomes more noisy over time. \nThe final timestep T has the highest noise level, ensuring the data has been transformed into pure Gaussian noise. The schedule is designed to prevent the variance from exploding while ensuring the gradual transformation.\n3. A linear noise schedule increases the noise linearly over time: the original image information is quickly lost after about half of the total steps.\n A cosine noise schedule, on the other hand, adds noise more slowly at the beginning and increases it more rapidly towards the end. This helps in preserving the information in the earlier timesteps, making the denoising process more efficient and retaining more of the original data information.\n4. To generate the original image from its noised version. This involves starting with Gaussian noise and gradually denoising it to produce a sample from the real data distribution.\n5. p(xt−1∣xt) is important because it describes how to transition from the noised version at time t to a slightly less noised version at time t−1.\n Knowing this distribution allows us to iteratively denoise an image.\n6. We can't directly use p(xt−1∣xt) because it is intractable. Calculating this distribution would require knowing the distribution of all possible samples, which is computationally infeasible.\n7. We approximate p(xt−1∣xt) using a neural network. The neural network learns to predict the conditional probability distribution, denoted as pθ(xt−1∣xt), \n where θ represents the parameters of the neural network."
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html",
    "title": "  Section 1: Convolutions in NumPy",
    "section": "",
    "text": "##Return to HomePage\nOBJECTIVES: By the end of this workshop you should be able to:\n\n Identify and Define CNNs \nExplore the convolution operation performed in a convolutional layer\nObserve how CNNs learn and represent features from input images\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n Table of Contents \nSection 1: Convolutions in Numpy\nSection 2: Training a Convolutional Neural Network * Defining a model * Creating dataset * Instantiating model * Training\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\nfrom scipy import signal as sg\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, transforms\n\n\n\n\n\n\nRecall that a neural network (NN) is a hierarchical network of computational units or “neurons” for highly nonlinear function approximation. Each neuron’s input is a set of weighted upstream signals. The neuron treats this set of weighted inputs as a linear combination and optionally applies nonlinearity via an activation function.\nIn a dense layer like in the previous workshop, each neuron in the current layer is connected to every neuron in the previous layer, and every connection has it’s own weight. These layers are expensive in terms of memory and computation. Alternatively, in a convolutional layer each neuron is only connected to a few nearby local neurons in the previous layer, and the same set of weights is used to connect them. This special structure allows CNNs to efficiently and robustly detect visual patterns in images.\n\n\n\nimage.png\n\n\nThe word convolutional refers to the granularity reduction process that happens in this type of network. In the case of images, if you look inside a convolutional neural network, you will see a sliding window (called a kernel), sweeping across each pixel in your image and producing a blurrier version than the original. In the lingo, we would say that the image on the right is a convolution of our original image.\n\n\n\nimage.png\n\n\nLet’s first start with a one dimensional convolution operation. There are multiple methods to apply a kernel on a matrix: With padding (full), with padding (same), and without padding (valid). In this example, we will do a convolution without padding (valid).\nGiven an image \\(x\\) and kernel \\(k\\), the values of the convolved images \\(y\\) is given by:\n\\[y(t) = \\sum\\limits_{j=0}^{K-1} k[j] \\cdot x[t - j] \\]\n\n\\(t\\) is the index of a position in \\(y\\)\n\\(j\\) is the index of the position in kernel \\(k\\)\n\\(K\\) is the length of the kernel array\n\nThe convolution operation computes the output image \\(y\\) by sliding the kernel over the input image and performing element-wise multiplication followed by summation at each point.\n\n\nCode\nk = [6, 2]\nk_inv = k[::-1] # kernel inverted\n\nx = [1, 2, 5, 4]\n\n\n\n\nCode\nk_inv\n\n\n[2, 6]\n\n\n\n\nCode\ny1 = k_inv[0] * x[0] + k_inv[1] * x[1]\ny2 = k_inv[0] * x[1] + k_inv[1] * x[2]\ny3 = k_inv[0] * x[2] + k_inv[1] * x[3]\ny1, y2, y3\n\n\n(14, 34, 34)\n\n\nIf you look at the command reimplementation in numpy below, the convolution operation inverts kernel k under the hood before multiplying element-wise and summing. There are some interesting reasons for performing this inversion, but the TLDR is that by definition you first invert the kernel (otherwise, without the inversion, you are performing an operation known as cross-correlation), before performing element-wise multiplicatoin.\n\n\nCode\n# in numpy\ny = np.convolve(k, x, \"valid\")\ny\n\n\narray([14, 34, 34])\n\n\nNow consider the 5x5 matrix of an image’s pixels.\n\nWe can convert each pixel to binary values (1 = white pixel, 0 = black pixel). To create a convolved matrix (feature map), we need a sliding window, also known as a kernel. In the animation below, we use a 3x3 filter. The values from the filter were multiplied element-wise with the original matrix (input image), then summed up. This process is repeated for each elemenet by sliding the filter over the whole original martix, ultimately generating the full convolved feature matrix.\n\nThe 2D convolution is similar to the 1D operation performed earlier, where we take the dot product of kernel \\(K\\) with each portion of the input image \\(I\\)\n\\[y[i, j] = \\sum_{u=0}^{p-1} \\sum_{v=0}^{q-1} K[u, v] \\cdot I[i - u, j - v]\\]\n\n\\(i\\) and \\(j\\) are th row and column indices of convolved image \\(y\\)\n\\(u\\) and \\(v\\) are the row and column indices of kernel \\(K\\)\n\nBelow we recreate the convolution animation\n\n\nCode\n# 2D image\nI = [[1, 1, 1, 0, 0],\n     [0, 1, 1, 1, 0],\n     [0, 0, 1, 1, 1],\n     [0, 0, 1, 1, 0],\n     [0, 1, 1, 0, 0]]\n\n# 2D kernel\ng = [[1, 0, 1],\n     [0, 1, 0],\n     [1, 0, 1]]\n\nsg.convolve(g, I, \"valid\")\n\n\narray([[4, 3, 4],\n       [2, 4, 3],\n       [2, 3, 4]])\n\n\n\n\nCode\n# let's expand the first two operations for clarity\n# y[0][0]\ny1 = g[0][0] * I[0][0] + g[0][1] * I[0][1] + g[0][2] * I[0][2]  + \\\n     g[1][0] * I[1][0] + g[1][1] * I[1][1] + g[1][2] * I[1][2]  + \\\n     g[2][0] * I[2][0] + g[2][1] * I[2][1] + g[2][2] * I[2][2]\n\n# y[0][1]\ny2 = g[0][0] * I[0][1] + g[0][1] * I[0][2] + g[0][2] * I[0][3]  + \\\n     g[1][0] * I[1][1] + g[1][1] * I[1][2] + g[1][2] * I[1][3]  + \\\n     g[2][0] * I[2][1] + g[2][1] * I[2][2] + g[2][2] * I[2][3]\n\ny1, y2 # the first two values in the convolved feature matrix\n\n\n(4, 3)\n\n\nBefore training our convolutional neural network, let’s look at an example of convolving another image, a blurry version of a handwritten number\n\n\nCode\n# Function to perform convolution on an image with a given kernel\ndef convolve_image(image, kernel):\n    # Convolve the image with the kernel\n    convolved_image = convolve2d(image, kernel, mode='same', boundary='wrap')\n    return convolved_image\n\n# Function to display original and convolved images\ndef show_images(original_image, convolved_image):\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image, cmap='gray')\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(convolved_image, cmap='gray')\n    plt.title('Convolved Image')\n    plt.axis('off')\n\n    plt.show()\n\n# Load the MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor()])\nmnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# Select a random MNIST image\nidx = np.random.randint(len(mnist_dataset))\nimage, label = mnist_dataset[idx]\n\n# Convert image to numpy array\nimage_np = image.squeeze().numpy()\n\n# Example kernel\nkernel = np.array([[0, -1, 0],\n                   [-1, 5, -1],\n                   [0, -1, 0]])\n\n# Perform convolution\nconvolved_image = convolve_image(image_np, kernel)\n\n# Crop the convolved image to match original dimensions\nconvolved_image_cropped = convolved_image[:image_np.shape[0], :image_np.shape[1]]\n\n# Show original and convolved images\nshow_images(image_np, convolved_image_cropped)\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 101737923.78it/s]\n\n\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|██████████| 28881/28881 [00:00&lt;00:00, 99536313.74it/s]\n\n\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 54088292.91it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████| 4542/4542 [00:00&lt;00:00, 16681723.96it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    # random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n\nquestion = \"What is the purpose of the convolutional layer in a CNN?\"\ncorrect_answer = \"To extract features from the input images by applying filters that detect edges, textures, and shapes.\"\ndecoy_answers = [\n    \"To randomly initialize the weights of the model.\",\n    \"To combine different layers of the CNN into one.\"\n]\nexplanation = \"The convolutional layer applies filters to input images, helping the model detect low-level features like edges and textures, and higher-level features in deeper layers.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To extract features from the input images by applying filters that detect edges, textures, and shapes.\n        \n        \n        \n          \n          To randomly initialize the weights of the model.\n        \n        \n        \n          \n          To combine different layers of the CNN into one.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The convolutional layer applies filters to input images, helping the model detect low-level features like edges and textures, and higher-level features in deeper layers.\n      \n    \n    \n    \n\n\n\n\n\n\n\n\n\nCode\n# dense neural network\nclass DenseModel(nn.Module):\n    def __init__(self):\n        super(DenseModel, self).__init__()\n\n        # Fully connected layers\n        self.d1 = nn.Linear(28 * 28, 128)\n        self.d2 = nn.Linear(128, 128)  # Added another fully connected layer\n        self.d3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Flatten input\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.d1(x)\n        x = F.relu(x)\n        x = self.d2(x)  # Added another fully connected layer\n        x = F.relu(x)\n        x = self.d3(x)\n\n        # Softmax activation\n        out = F.softmax(x, dim=1)\n        return out\n\n# convolutional neural network\nclass ConvModel(nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n\n        # 28x28x1 =&gt; 26x26x32\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.d1 = nn.Linear(26 * 26 * 32, 128)\n        self.d2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # 32x1x28x28 =&gt; 32x32x26x26\n        x = self.conv1(x)\n        x = F.relu(x)\n\n        # flatten =&gt; 32 x (32*26*26)\n        x = x.flatten(start_dim = 1)\n\n        # 32 x (32*26*26) =&gt; 32x128\n        x = self.d1(x)\n        x = F.relu(x)\n\n        # logits =&gt; 32x10\n        logits = self.d2(x)\n        out = F.softmax(logits, dim=1)\n        return out\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What advantage does a CNN with multiple convolutional layers have?\"\ncorrect_answer = \"It allows the network to capture progressively complex features at each layer.\"\ndecoy_answers = [\n    \"It helps downsample the input, reducing the number of computations required.\",\n    \"It makes the model learn features in smaller image segments.\",\n    \"It ensures each layer has identical filters for uniform feature extraction.\"\n]\nexplanation = \"Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It allows the network to capture progressively complex features at each layer.\n        \n        \n        \n          \n          It helps downsample the input, reducing the number of computations required.\n        \n        \n        \n          \n          It makes the model learn features in smaller image segments.\n        \n        \n        \n          \n          It ensures each layer has identical filters for uniform feature extraction.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What effect does a higher learning rate have on the training process?\"\ncorrect_answer = \"It increases the step size of each update, potentially speeding up convergence but risking instability.\"\ndecoy_answers = [\n    \"It decreases the step size, improving stability and precision.\",\n    \"It has no impact on training speed or convergence.\",\n    \"It reduces model accuracy by adding noise to gradients.\"\n]\nexplanation = \"A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It increases the step size of each update, potentially speeding up convergence but risking instability.\n        \n        \n        \n          \n          It decreases the step size, improving stability and precision.\n        \n        \n        \n          \n          It has no impact on training speed or convergence.\n        \n        \n        \n          \n          It reduces model accuracy by adding noise to gradients.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the purpose of the final fully connected layer in a CNN?\"\ncorrect_answer = \"It combines features from previous layers to make final predictions.\"\ndecoy_answers = [\n    \"It normalizes feature maps for the final output.\",\n    \"It helps detect edges within the feature maps.\",\n    \"It increases the spatial resolution of the output.\"\n]\nexplanation = \"The fully connected layer aggregates features extracted by previous layers to make classification predictions.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It combines features from previous layers to make final predictions.\n        \n        \n        \n          \n          It normalizes feature maps for the final output.\n        \n        \n        \n          \n          It helps detect edges within the feature maps.\n        \n        \n        \n          \n          It increases the spatial resolution of the output.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The fully connected layer aggregates features extracted by previous layers to make classification predictions.\n      \n    \n    \n    \n\n\n\n\n\nTo prepare datasets in PyTorch we will be using the DataLoader. The PyTorch DataLoader is a utility that helps efficiently load and iterate over datasets during the training or testing process in PyTorch. Here, we define the batch size for loading data, which determines the number of samples processed together during each iteration of training or testing. Additionally, we define a transformation pipeline using transforms.Compose. In this case, we have only one transformation, transforms.ToTensor(), which converts PIL Image or numpy array to a PyTorch tensor.\nWe then create a DataLoader for the training dataset (trainset). We specify the batch size (batch_size=BATCH_SIZE), which determines the number of samples to load per batch. Setting shuffle=True shuffles the data at every epoch, which helps in randomizing the order of samples during training, thus preventing the model from learning any spurious patterns based on the order of the data. num_workers=2 indicates the number of subprocesses to use for data loading. This speeds up the data loading process by parallelizing data loading operations.\nHere we will use the MNIST dataset, which is a widely used dataset in the field of machine learning and computer vision. It consists of a collection of 28x28 pixel grayscale images of handwritten digits (0-9), along with their corresponding labels indicating the digit they represent. Each image is labeled with the digit it contains.\n\n\nCode\n%%capture\nBATCH_SIZE = 32\n\n## transformations\ntransform = transforms.Compose(\n    [transforms.ToTensor()])\n\n## download and load training dataset\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\n## download and load testing dataset\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n                                         shuffle=False, num_workers=2)\n\n\n\nWe can visualize datapoints of our dataset\n\n\nCode\n## functions to show an image\ndef imshow(img):\n    #img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n## get some random training images\ndataiter = iter(trainloader)\n\n## show images\nfor i in range(1):  # Assuming you want to show 4 images\n    images, labels = next(dataiter)\n    imshow(torchvision.utils.make_grid(images))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the dimensions\nfor images, labels in trainloader:\n    print(\"Image batch dimensions:\", images.shape)\n    print(\"Image label dimensions:\", labels.shape)\n    break\n\n\nImage batch dimensions: torch.Size([32, 1, 28, 28])\nImage label dimensions: torch.Size([32])\n\n\n\n\n\n\n\nCode\n## test the model with 1 batch\nmodel = ConvModel()\nfor images, labels in trainloader:\n    print(\"batch size:\", images.shape)\n    out = model(images)\n    print(out.shape)\n    break\n\n\nbatch size: torch.Size([32, 1, 28, 28])\ntorch.Size([32, 10])\n\n\n\n\n\n\n\nCode\nlearning_rate = 0.001\nnum_epochs = 5\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvModel()\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n\nCode\n## compute accuracy\ndef get_accuracy(logit, target, batch_size):\n    ''' Obtain accuracy for training round '''\n    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n    accuracy = 100.0 * corrects/batch_size\n    return accuracy.item()\n\n\n\n\nCode\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n\n\nEpoch: 0 | Loss: 1.5820 | Train Accuracy: 88.22\nEpoch: 1 | Loss: 1.4914 | Train Accuracy: 97.30\nEpoch: 2 | Loss: 1.4812 | Train Accuracy: 98.25\nEpoch: 3 | Loss: 1.4766 | Train Accuracy: 98.66\nEpoch: 4 | Loss: 1.4742 | Train Accuracy: 98.88\n\n\n\n\nCode\ntest_acc = 0.0\nfor i, (images, labels) in enumerate(testloader, 0):\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n\nprint('Test Accuracy: %.2f'%( test_acc/i))\n\n\nTest Accuracy: 98.37\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the backpropagation process in CNNs, what is being updated?\"\ncorrect_answer = \"The weights of the filters are updated to minimize the loss.\"\ndecoy_answers = [\n    \"The pixel values of the input images are adjusted to fit the model.\",\n    \"The number of filters in each layer is modified to improve accuracy.\",\n    \"The dimensions of the feature maps are changed.\"\n]\nexplanation = \"During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          The weights of the filters are updated to minimize the loss.\n        \n        \n        \n          \n          The pixel values of the input images are adjusted to fit the model.\n        \n        \n        \n          \n          The number of filters in each layer is modified to improve accuracy.\n        \n        \n        \n          \n          The dimensions of the feature maps are changed.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the training loop, why is it important to zero out gradients using `optimizer.zero_grad()`?\"\ncorrect_answer = \"To prevent gradients from accumulating across batches, ensuring correct updates to the model.\"\ndecoy_answers = [\n    \"To reset the weights of the model after each epoch.\",\n    \"To clear the model parameters so they can be re-initialized.\",\n    \"To reduce the learning rate with each batch.\"\n]\nexplanation = \"Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To prevent gradients from accumulating across batches, ensuring correct updates to the model.\n        \n        \n        \n          \n          To reset the weights of the model after each epoch.\n        \n        \n        \n          \n          To clear the model parameters so they can be re-initialized.\n        \n        \n        \n          \n          To reduce the learning rate with each batch.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\n      \n    \n    \n    \n\n\nLet’s select an image from our dataset and pass it through our convolutional layer in the trained model. We can extract and visualize the feature maps generated by this convolutional layer in response to the input image. This visualization will help us understand what kinds of features the CNN learns to detect, such as edges and textures.\nThe type of information captured by this kernel depends on its weights, which are learned during the training process. However, with a 3x3 kernel size, the convolution operation typically captures local spatial patterns or features in the input image. These patterns could include edges, corners, textures, or other visual attributes present in the image. Since we have 32 output channels, each channel in the output feature map will capture different aspects or variations of these local patterns, providing a richer representation of the image content.\n\n\nCode\n# Access the first image from the training set\nimage, _ = trainset[0]\n\n# Plot the image\nplt.imshow(image.squeeze(), cmap='gray')\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Choose an image from the dataset\nimage, _ = trainset[0]  # Choose any image from the dataset\n\n# Convert the image to a tensor and move it to the appropriate device\nimage = image.unsqueeze(0).to(device)\n\n# Get the feature maps from a specific convolutional layer\nconv_layer = model.conv1  # Choose the convolutional layer you want to visualize\nactivation = conv_layer(image)\n\n# Plot the feature maps\nplt.figure(figsize=(10, 10))\nfor i in range(activation.size(1)):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(activation.squeeze().detach().cpu().numpy()[i], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nImage Credit * Figure 2: Convolving an image https://gregorygundersen.com/blog/2017/02/24/cnns/ * Figure 3: 5x5 pixel image https://ibm.box.com/shared/static/0s5v7doe2p5xuzifs47bxmmuwrn3kra2.bmp * Figure 4: Kernel sweeping image animation https://ibm.box.com/shared/static/fvutcm8jwa5j2o7xv2zzqyz2yu3zwhz4.gif\nAdd the wandb (weights and biases) module to colab to efficiently track your training live!\nMake an account on the wandb website to log and access all of your projects, create project reports, run hyperparameter searches and more.\nWandb is a widely used logging module used to track architecture performance and it’s a worthwhile tool to learn how to use.\nCreate an account (you can use your student JHU account) here: https://wandb.auth0.com/login?state=hKFo2SBFTXR0eVRGQ3F2V1ZPQ3hfZ29seTVSdzhzcVkxWEVzN6FupWxvZ2luo3RpZNkgdjVEMjdCdVc4ZzY1UHBPLVRhakt3NnN5NVZ2b25YU0qjY2lk2SBWU001N1VDd1Q5d2JHU3hLdEVER1FISUtBQkhwcHpJdw&client=VSM57UCwT9wbGSxKtEDGQHIKABHppzIw&protocol=oauth2&nonce=aFV4UGtDY0gtfmphLUxtbw%3D%3D&redirect_uri=https%3A%2F%2Fapi.wandb.ai%2Foidc%2Fcallback&response_mode=form_post&response_type=id_token&scope=openid%20profile%20email&signup=true\nFollow the simple steps below to start using wandb in colab to track your models. We’ll only be tracking training data here as a minimum tool, but go through this in depth tutorial: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=TxUlHUj52t-d\n\n\nCode\nimport random\n\n\n\n\nCode\n!pip install wandb -qqq\nimport wandb\nwandb.login()\n\n#initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"pytorch-intro\",\n        config={\n            \"epochs\": 10,\n            \"batch_size\": 128,\n            \"lr\": 1e-3,\n            \"dropout\": random.uniform(0.01, 0.80),\n            }) #feel free to add other metadata like what optimizers you used etc.\n\n\n#Put the following line in both your training and validation loops so you can track how performance changes in both scenario\n#You can change variable names such as frm 'train/loss' to 'test/loss', 'accuracy', etc. and the corresponding variables will be plotted in wandb.\n#Make sure your information is put in a dictionary, or else you'll have an error!\nwandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n#After your training loop, run\nwandb.finish()\n\n\n\n\nCode\n#Putting it all together:\n\n#🐝  import the module in & login to your account\n!pip install wandb -qqq\nimport wandb\nwandb.login() #Go to bottom of cell and followw instructions. It will ask for an API from a given link that's connected to your account. Make sure you're logged into wandb before running this cell!\n\n#🐝 initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"CNN_Classifier\",\n        config={\n            \"epochs\": num_epochs,\n            \"lr\":learning_rate,\n            })\n\n#Setup training loop:\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        #🐝 insert training logging step here:\n        wandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n    #🐝 If we did a validation loop, we'd insert the logging step here:\n\n#🐝 finish wandb run\nwandb.finish()"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#defining-model",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#defining-model",
    "title": "  Section 1: Convolutions in NumPy",
    "section": "",
    "text": "Code\n# dense neural network\nclass DenseModel(nn.Module):\n    def __init__(self):\n        super(DenseModel, self).__init__()\n\n        # Fully connected layers\n        self.d1 = nn.Linear(28 * 28, 128)\n        self.d2 = nn.Linear(128, 128)  # Added another fully connected layer\n        self.d3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Flatten input\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.d1(x)\n        x = F.relu(x)\n        x = self.d2(x)  # Added another fully connected layer\n        x = F.relu(x)\n        x = self.d3(x)\n\n        # Softmax activation\n        out = F.softmax(x, dim=1)\n        return out\n\n# convolutional neural network\nclass ConvModel(nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n\n        # 28x28x1 =&gt; 26x26x32\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.d1 = nn.Linear(26 * 26 * 32, 128)\n        self.d2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # 32x1x28x28 =&gt; 32x32x26x26\n        x = self.conv1(x)\n        x = F.relu(x)\n\n        # flatten =&gt; 32 x (32*26*26)\n        x = x.flatten(start_dim = 1)\n\n        # 32 x (32*26*26) =&gt; 32x128\n        x = self.d1(x)\n        x = F.relu(x)\n\n        # logits =&gt; 32x10\n        logits = self.d2(x)\n        out = F.softmax(logits, dim=1)\n        return out\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What advantage does a CNN with multiple convolutional layers have?\"\ncorrect_answer = \"It allows the network to capture progressively complex features at each layer.\"\ndecoy_answers = [\n    \"It helps downsample the input, reducing the number of computations required.\",\n    \"It makes the model learn features in smaller image segments.\",\n    \"It ensures each layer has identical filters for uniform feature extraction.\"\n]\nexplanation = \"Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It allows the network to capture progressively complex features at each layer.\n        \n        \n        \n          \n          It helps downsample the input, reducing the number of computations required.\n        \n        \n        \n          \n          It makes the model learn features in smaller image segments.\n        \n        \n        \n          \n          It ensures each layer has identical filters for uniform feature extraction.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What effect does a higher learning rate have on the training process?\"\ncorrect_answer = \"It increases the step size of each update, potentially speeding up convergence but risking instability.\"\ndecoy_answers = [\n    \"It decreases the step size, improving stability and precision.\",\n    \"It has no impact on training speed or convergence.\",\n    \"It reduces model accuracy by adding noise to gradients.\"\n]\nexplanation = \"A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It increases the step size of each update, potentially speeding up convergence but risking instability.\n        \n        \n        \n          \n          It decreases the step size, improving stability and precision.\n        \n        \n        \n          \n          It has no impact on training speed or convergence.\n        \n        \n        \n          \n          It reduces model accuracy by adding noise to gradients.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the purpose of the final fully connected layer in a CNN?\"\ncorrect_answer = \"It combines features from previous layers to make final predictions.\"\ndecoy_answers = [\n    \"It normalizes feature maps for the final output.\",\n    \"It helps detect edges within the feature maps.\",\n    \"It increases the spatial resolution of the output.\"\n]\nexplanation = \"The fully connected layer aggregates features extracted by previous layers to make classification predictions.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It combines features from previous layers to make final predictions.\n        \n        \n        \n          \n          It normalizes feature maps for the final output.\n        \n        \n        \n          \n          It helps detect edges within the feature maps.\n        \n        \n        \n          \n          It increases the spatial resolution of the output.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The fully connected layer aggregates features extracted by previous layers to make classification predictions."
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#creating-dataset",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#creating-dataset",
    "title": "  Section 1: Convolutions in NumPy",
    "section": "",
    "text": "To prepare datasets in PyTorch we will be using the DataLoader. The PyTorch DataLoader is a utility that helps efficiently load and iterate over datasets during the training or testing process in PyTorch. Here, we define the batch size for loading data, which determines the number of samples processed together during each iteration of training or testing. Additionally, we define a transformation pipeline using transforms.Compose. In this case, we have only one transformation, transforms.ToTensor(), which converts PIL Image or numpy array to a PyTorch tensor.\nWe then create a DataLoader for the training dataset (trainset). We specify the batch size (batch_size=BATCH_SIZE), which determines the number of samples to load per batch. Setting shuffle=True shuffles the data at every epoch, which helps in randomizing the order of samples during training, thus preventing the model from learning any spurious patterns based on the order of the data. num_workers=2 indicates the number of subprocesses to use for data loading. This speeds up the data loading process by parallelizing data loading operations.\nHere we will use the MNIST dataset, which is a widely used dataset in the field of machine learning and computer vision. It consists of a collection of 28x28 pixel grayscale images of handwritten digits (0-9), along with their corresponding labels indicating the digit they represent. Each image is labeled with the digit it contains.\n\n\nCode\n%%capture\nBATCH_SIZE = 32\n\n## transformations\ntransform = transforms.Compose(\n    [transforms.ToTensor()])\n\n## download and load training dataset\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\n## download and load testing dataset\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n                                         shuffle=False, num_workers=2)\n\n\n\nWe can visualize datapoints of our dataset\n\n\nCode\n## functions to show an image\ndef imshow(img):\n    #img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n## get some random training images\ndataiter = iter(trainloader)\n\n## show images\nfor i in range(1):  # Assuming you want to show 4 images\n    images, labels = next(dataiter)\n    imshow(torchvision.utils.make_grid(images))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the dimensions\nfor images, labels in trainloader:\n    print(\"Image batch dimensions:\", images.shape)\n    print(\"Image label dimensions:\", labels.shape)\n    break\n\n\nImage batch dimensions: torch.Size([32, 1, 28, 28])\nImage label dimensions: torch.Size([32])"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#instantiating-model",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#instantiating-model",
    "title": "  Section 1: Convolutions in NumPy",
    "section": "",
    "text": "Code\n## test the model with 1 batch\nmodel = ConvModel()\nfor images, labels in trainloader:\n    print(\"batch size:\", images.shape)\n    out = model(images)\n    print(out.shape)\n    break\n\n\nbatch size: torch.Size([32, 1, 28, 28])\ntorch.Size([32, 10])"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#training",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#training",
    "title": "  Section 1: Convolutions in NumPy",
    "section": "",
    "text": "Code\nlearning_rate = 0.001\nnum_epochs = 5\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvModel()\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n\nCode\n## compute accuracy\ndef get_accuracy(logit, target, batch_size):\n    ''' Obtain accuracy for training round '''\n    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n    accuracy = 100.0 * corrects/batch_size\n    return accuracy.item()\n\n\n\n\nCode\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n\n\nEpoch: 0 | Loss: 1.5820 | Train Accuracy: 88.22\nEpoch: 1 | Loss: 1.4914 | Train Accuracy: 97.30\nEpoch: 2 | Loss: 1.4812 | Train Accuracy: 98.25\nEpoch: 3 | Loss: 1.4766 | Train Accuracy: 98.66\nEpoch: 4 | Loss: 1.4742 | Train Accuracy: 98.88\n\n\n\n\nCode\ntest_acc = 0.0\nfor i, (images, labels) in enumerate(testloader, 0):\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n\nprint('Test Accuracy: %.2f'%( test_acc/i))\n\n\nTest Accuracy: 98.37\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the backpropagation process in CNNs, what is being updated?\"\ncorrect_answer = \"The weights of the filters are updated to minimize the loss.\"\ndecoy_answers = [\n    \"The pixel values of the input images are adjusted to fit the model.\",\n    \"The number of filters in each layer is modified to improve accuracy.\",\n    \"The dimensions of the feature maps are changed.\"\n]\nexplanation = \"During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          The weights of the filters are updated to minimize the loss.\n        \n        \n        \n          \n          The pixel values of the input images are adjusted to fit the model.\n        \n        \n        \n          \n          The number of filters in each layer is modified to improve accuracy.\n        \n        \n        \n          \n          The dimensions of the feature maps are changed.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the training loop, why is it important to zero out gradients using `optimizer.zero_grad()`?\"\ncorrect_answer = \"To prevent gradients from accumulating across batches, ensuring correct updates to the model.\"\ndecoy_answers = [\n    \"To reset the weights of the model after each epoch.\",\n    \"To clear the model parameters so they can be re-initialized.\",\n    \"To reduce the learning rate with each batch.\"\n]\nexplanation = \"Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To prevent gradients from accumulating across batches, ensuring correct updates to the model.\n        \n        \n        \n          \n          To reset the weights of the model after each epoch.\n        \n        \n        \n          \n          To clear the model parameters so they can be re-initialized.\n        \n        \n        \n          \n          To reduce the learning rate with each batch.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\n      \n    \n    \n    \n\n\nLet’s select an image from our dataset and pass it through our convolutional layer in the trained model. We can extract and visualize the feature maps generated by this convolutional layer in response to the input image. This visualization will help us understand what kinds of features the CNN learns to detect, such as edges and textures.\nThe type of information captured by this kernel depends on its weights, which are learned during the training process. However, with a 3x3 kernel size, the convolution operation typically captures local spatial patterns or features in the input image. These patterns could include edges, corners, textures, or other visual attributes present in the image. Since we have 32 output channels, each channel in the output feature map will capture different aspects or variations of these local patterns, providing a richer representation of the image content.\n\n\nCode\n# Access the first image from the training set\nimage, _ = trainset[0]\n\n# Plot the image\nplt.imshow(image.squeeze(), cmap='gray')\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Choose an image from the dataset\nimage, _ = trainset[0]  # Choose any image from the dataset\n\n# Convert the image to a tensor and move it to the appropriate device\nimage = image.unsqueeze(0).to(device)\n\n# Get the feature maps from a specific convolutional layer\nconv_layer = model.conv1  # Choose the convolutional layer you want to visualize\nactivation = conv_layer(image)\n\n# Plot the feature maps\nplt.figure(figsize=(10, 10))\nfor i in range(activation.size(1)):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(activation.squeeze().detach().cpu().numpy()[i], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nImage Credit * Figure 2: Convolving an image https://gregorygundersen.com/blog/2017/02/24/cnns/ * Figure 3: 5x5 pixel image https://ibm.box.com/shared/static/0s5v7doe2p5xuzifs47bxmmuwrn3kra2.bmp * Figure 4: Kernel sweeping image animation https://ibm.box.com/shared/static/fvutcm8jwa5j2o7xv2zzqyz2yu3zwhz4.gif\nAdd the wandb (weights and biases) module to colab to efficiently track your training live!\nMake an account on the wandb website to log and access all of your projects, create project reports, run hyperparameter searches and more.\nWandb is a widely used logging module used to track architecture performance and it’s a worthwhile tool to learn how to use.\nCreate an account (you can use your student JHU account) here: https://wandb.auth0.com/login?state=hKFo2SBFTXR0eVRGQ3F2V1ZPQ3hfZ29seTVSdzhzcVkxWEVzN6FupWxvZ2luo3RpZNkgdjVEMjdCdVc4ZzY1UHBPLVRhakt3NnN5NVZ2b25YU0qjY2lk2SBWU001N1VDd1Q5d2JHU3hLdEVER1FISUtBQkhwcHpJdw&client=VSM57UCwT9wbGSxKtEDGQHIKABHppzIw&protocol=oauth2&nonce=aFV4UGtDY0gtfmphLUxtbw%3D%3D&redirect_uri=https%3A%2F%2Fapi.wandb.ai%2Foidc%2Fcallback&response_mode=form_post&response_type=id_token&scope=openid%20profile%20email&signup=true\nFollow the simple steps below to start using wandb in colab to track your models. We’ll only be tracking training data here as a minimum tool, but go through this in depth tutorial: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=TxUlHUj52t-d\n\n\nCode\nimport random\n\n\n\n\nCode\n!pip install wandb -qqq\nimport wandb\nwandb.login()\n\n#initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"pytorch-intro\",\n        config={\n            \"epochs\": 10,\n            \"batch_size\": 128,\n            \"lr\": 1e-3,\n            \"dropout\": random.uniform(0.01, 0.80),\n            }) #feel free to add other metadata like what optimizers you used etc.\n\n\n#Put the following line in both your training and validation loops so you can track how performance changes in both scenario\n#You can change variable names such as frm 'train/loss' to 'test/loss', 'accuracy', etc. and the corresponding variables will be plotted in wandb.\n#Make sure your information is put in a dictionary, or else you'll have an error!\nwandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n#After your training loop, run\nwandb.finish()\n\n\n\n\nCode\n#Putting it all together:\n\n#🐝  import the module in & login to your account\n!pip install wandb -qqq\nimport wandb\nwandb.login() #Go to bottom of cell and followw instructions. It will ask for an API from a given link that's connected to your account. Make sure you're logged into wandb before running this cell!\n\n#🐝 initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"CNN_Classifier\",\n        config={\n            \"epochs\": num_epochs,\n            \"lr\":learning_rate,\n            })\n\n#Setup training loop:\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        #🐝 insert training logging step here:\n        wandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n    #🐝 If we did a validation loop, we'd insert the logging step here:\n\n#🐝 finish wandb run\nwandb.finish()"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html",
    "title": "  Section 1: Defining model",
    "section": "",
    "text": "##Return to HomePage\nTutorial adapted and images copied from Harrison Kinsley of sentdex\nOBJECTIVES: By the end of this workshop you should be able to:\n\nDefine and instantiate a neural network model with PyTorch\nDefine and execute a training loop for a multi-class classification task\nVisualize the learned classification landscape of the neural network\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n Table of Contents \nSection 1: Defining model\n\nDefine neural network class\nCreate spiral multi-class classification dataset\nInstantiate model, loss function, and optimizer\n\nSection 2: Training\n\nCorrectly place loss, optimizer, and accuracy calculation training loop\n\nSection 3: Performance\n\nPlot a mesh representation of the learned classification bins\n\n\n\n\n\nCode\n!pip install nnfs\n\n\nCollecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.26.4)\nDownloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\n\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\n\n\n\nWhereas in the previous notebook we needed 7 classes to define components of our model, below is all we need when using PyTorch!\n\n\nCode\n# custom NN inheriting from nn.Module\nclass MyModel(nn.Module):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    # first dense (fully connected) layer with 2 input features, 64 output neurons\n    self.dense1 = nn.Linear(2, 64)\n    # define activation function (ReLU) for first hidden layer\n    self.relu1 = nn.ReLU()\n    # # second desne layer with 64 input neurons, 3 output neurons\n    self.dense2 = nn.Linear(64, 3)\n\n  # forward pass\n  # computes output of model given input data x\n  def forward(self, x):\n    # pass input x through first dense layer\n    x = self.dense1(x)\n    # apply ReLU activation function to output of first dense layer\n    x = self.relu1(x)\n    # pass output of ReLU activation function through second dense layer\n    x = self.dense2(x)\n    # return output of second dense layer\n    return x\n\n\n\n\nWe will be training a neural network to predict the color of a point (y \\(ϵ\\) \\(\\{\\)green, red, or blue \\(\\}\\) ) given an input \\(X\\) of two descripitive features (the x and y coordinates)\n\n\nCode\n# generate data for a classification task\nX, y = spiral_data(samples=100, classes=3)\n\n# convert dataset to pytorch tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\n# visualize dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\n\nWe will be using a feedforward neural network consisting of two dense (fully connected) layers with ReLU activation, where the first layer takes 2 input features and outputs 64 neurons, and the second layer takes 64 neurons and outputs 3 neurons.\n\n\n\nimage.png\n\n\n\n\nCode\n# Define the model\nmodel = MyModel()\n\n# Loss and optimizer\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-7)\n\n\n\n\n\n\n\n\nCode\n# Train in loop\nfor epoch in range(10001):\n    # Forward pass\n    outputs = model(X)\n\n    # Calculate the loss\n    loss = loss_function(outputs, y)\n\n    # Zero gradients, backward pass, and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calculate accuracy\n    _, predicted = torch.max(outputs, 1)\n    accuracy = (predicted == y).float().mean()\n\n    # Print epoch, accuracy, loss, learning rate every 100 epochs\n    if epoch % 100 == 0:\n        print(f'Epoch: {epoch}, Accuracy: {accuracy.item():.3f}, Loss: {loss.item():.3f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n\n\nEpoch: 0, Accuracy: 0.347, Loss: 1.095, Learning Rate: 0.05\nEpoch: 100, Accuracy: 0.787, Loss: 0.525, Learning Rate: 0.05\nEpoch: 200, Accuracy: 0.857, Loss: 0.360, Learning Rate: 0.05\nEpoch: 300, Accuracy: 0.887, Loss: 0.280, Learning Rate: 0.05\nEpoch: 400, Accuracy: 0.900, Loss: 0.238, Learning Rate: 0.05\nEpoch: 500, Accuracy: 0.917, Loss: 0.212, Learning Rate: 0.05\nEpoch: 600, Accuracy: 0.917, Loss: 0.198, Learning Rate: 0.05\nEpoch: 700, Accuracy: 0.927, Loss: 0.188, Learning Rate: 0.05\nEpoch: 800, Accuracy: 0.927, Loss: 0.182, Learning Rate: 0.05\nEpoch: 900, Accuracy: 0.923, Loss: 0.174, Learning Rate: 0.05\nEpoch: 1000, Accuracy: 0.890, Loss: 0.257, Learning Rate: 0.05\nEpoch: 1100, Accuracy: 0.930, Loss: 0.166, Learning Rate: 0.05\nEpoch: 1200, Accuracy: 0.930, Loss: 0.155, Learning Rate: 0.05\nEpoch: 1300, Accuracy: 0.933, Loss: 0.147, Learning Rate: 0.05\nEpoch: 1400, Accuracy: 0.937, Loss: 0.141, Learning Rate: 0.05\nEpoch: 1500, Accuracy: 0.940, Loss: 0.137, Learning Rate: 0.05\nEpoch: 1600, Accuracy: 0.937, Loss: 0.134, Learning Rate: 0.05\nEpoch: 1700, Accuracy: 0.940, Loss: 0.131, Learning Rate: 0.05\nEpoch: 1800, Accuracy: 0.947, Loss: 0.129, Learning Rate: 0.05\nEpoch: 1900, Accuracy: 0.940, Loss: 0.128, Learning Rate: 0.05\nEpoch: 2000, Accuracy: 0.947, Loss: 0.125, Learning Rate: 0.05\nEpoch: 2100, Accuracy: 0.943, Loss: 0.122, Learning Rate: 0.05\nEpoch: 2200, Accuracy: 0.943, Loss: 0.123, Learning Rate: 0.05\nEpoch: 2300, Accuracy: 0.943, Loss: 0.121, Learning Rate: 0.05\nEpoch: 2400, Accuracy: 0.947, Loss: 0.118, Learning Rate: 0.05\nEpoch: 2500, Accuracy: 0.950, Loss: 0.117, Learning Rate: 0.05\nEpoch: 2600, Accuracy: 0.947, Loss: 0.112, Learning Rate: 0.05\nEpoch: 2700, Accuracy: 0.943, Loss: 0.109, Learning Rate: 0.05\nEpoch: 2800, Accuracy: 0.957, Loss: 0.107, Learning Rate: 0.05\nEpoch: 2900, Accuracy: 0.950, Loss: 0.124, Learning Rate: 0.05\nEpoch: 3000, Accuracy: 0.950, Loss: 0.106, Learning Rate: 0.05\nEpoch: 3100, Accuracy: 0.947, Loss: 0.106, Learning Rate: 0.05\nEpoch: 3200, Accuracy: 0.950, Loss: 0.105, Learning Rate: 0.05\nEpoch: 3300, Accuracy: 0.950, Loss: 0.104, Learning Rate: 0.05\nEpoch: 3400, Accuracy: 0.950, Loss: 0.103, Learning Rate: 0.05\nEpoch: 3500, Accuracy: 0.950, Loss: 0.103, Learning Rate: 0.05\nEpoch: 3600, Accuracy: 0.950, Loss: 0.102, Learning Rate: 0.05\nEpoch: 3700, Accuracy: 0.950, Loss: 0.102, Learning Rate: 0.05\nEpoch: 3800, Accuracy: 0.950, Loss: 0.101, Learning Rate: 0.05\nEpoch: 3900, Accuracy: 0.950, Loss: 0.101, Learning Rate: 0.05\nEpoch: 4000, Accuracy: 0.950, Loss: 0.100, Learning Rate: 0.05\nEpoch: 4100, Accuracy: 0.950, Loss: 0.100, Learning Rate: 0.05\nEpoch: 4200, Accuracy: 0.950, Loss: 0.099, Learning Rate: 0.05\nEpoch: 4300, Accuracy: 0.950, Loss: 0.098, Learning Rate: 0.05\nEpoch: 4400, Accuracy: 0.950, Loss: 0.098, Learning Rate: 0.05\nEpoch: 4500, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4600, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4700, Accuracy: 0.950, Loss: 0.096, Learning Rate: 0.05\nEpoch: 4800, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4900, Accuracy: 0.960, Loss: 0.094, Learning Rate: 0.05\nEpoch: 5000, Accuracy: 0.960, Loss: 0.094, Learning Rate: 0.05\nEpoch: 5100, Accuracy: 0.957, Loss: 0.091, Learning Rate: 0.05\nEpoch: 5200, Accuracy: 0.970, Loss: 0.098, Learning Rate: 0.05\nEpoch: 5300, Accuracy: 0.953, Loss: 0.090, Learning Rate: 0.05\nEpoch: 5400, Accuracy: 0.957, Loss: 0.090, Learning Rate: 0.05\nEpoch: 5500, Accuracy: 0.960, Loss: 0.089, Learning Rate: 0.05\nEpoch: 5600, Accuracy: 0.963, Loss: 0.112, Learning Rate: 0.05\nEpoch: 5700, Accuracy: 0.963, Loss: 0.093, Learning Rate: 0.05\nEpoch: 5800, Accuracy: 0.963, Loss: 0.092, Learning Rate: 0.05\nEpoch: 5900, Accuracy: 0.963, Loss: 0.090, Learning Rate: 0.05\nEpoch: 6000, Accuracy: 0.963, Loss: 0.089, Learning Rate: 0.05\nEpoch: 6100, Accuracy: 0.963, Loss: 0.088, Learning Rate: 0.05\nEpoch: 6200, Accuracy: 0.963, Loss: 0.088, Learning Rate: 0.05\nEpoch: 6300, Accuracy: 0.963, Loss: 0.087, Learning Rate: 0.05\nEpoch: 6400, Accuracy: 0.963, Loss: 0.087, Learning Rate: 0.05\nEpoch: 6500, Accuracy: 0.963, Loss: 0.086, Learning Rate: 0.05\nEpoch: 6600, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6700, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6800, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6900, Accuracy: 0.963, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7000, Accuracy: 0.963, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7100, Accuracy: 0.967, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7200, Accuracy: 0.963, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7300, Accuracy: 0.967, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7400, Accuracy: 0.967, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7500, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7600, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7700, Accuracy: 0.967, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7800, Accuracy: 0.967, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7900, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 8000, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 8100, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8200, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8300, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8400, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8500, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8600, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8700, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8800, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8900, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 9000, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9100, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9200, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9300, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9400, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9500, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9600, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9700, Accuracy: 0.967, Loss: 0.081, Learning Rate: 0.05\nEpoch: 9800, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9900, Accuracy: 0.970, Loss: 0.077, Learning Rate: 0.05\nEpoch: 10000, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\n\n\n\n\n\n\n\nCode\n# create meshgrid of points covering the feature space\nh = 0.02\n# determine min and max values for x,y axes\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n# create meshgrid of points with spacing h\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# convert meshgrid to torch tensor\nmeshgrid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n\n# pass meshgrid points through model\nwith torch.no_grad():\n  # forward pass through first dense\n  z1 = model.dense1(meshgrid_points)\n  # apply relu activation\n  a1 = torch.relu(z1)\n  # forward pass through second dense\n  z2 = model.dense2(a1)\n  # compute softmax probabilities for each class\n  exp_scores = torch.exp(z2 - torch.max(z2, axis=1, keepdim=True).values)\n  probs = exp_scores / torch.sum(exp_scores, axis=1, keepdim=True)\n\n# predictions\n# determine predicted class for each point in meshgrid\n_, predictions = torch.max(probs, axis=1)\n# reshape predictions to match shape of meshgrid\nZ = predictions.numpy().reshape(xx.shape)\n\n# plot decision boundary based on predictions\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# plot original data on top of decision boundary\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n# plot limits set to match extent of meshgrid\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nQuestion: What happens if you change the activation function of the hidden layer from ReLU to sigmoid in a neural network? Try this change and comment on the performance change.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint('\\n ReLU outperforms sigmoid. \\n ReLU mitigates the vanishing gradient problem by allowing gradients to flow for positive inputs. \\n Sigmoid will lead to poorer convergence due to the smaller effective gradient.')\n\n\nQuestion: What effect does changing the loss function from CrossEntropyLoss to MeanSquaredError have on the training of a classification model? Try this and remark on what happens.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using MSE for classification results in worse performance. \\n This is because MSE doesn't explicitly encourage the model to push the probability of the correct class to 1 while minimizing the others.\")\n\n\nQuestion: How does using the SGD optimizer with a learning rate of 0.01 compare to using the Adam optimizer with a learning rate of 0.001?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Both optimizers perform similarly, but Adam with a learning rate of 0.001 will convergence faster.\")\n\n\nQuestion: What happens if you add another hidden layer with 128 neurons to the neural network model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Adding another hidden layer with 128 neurons to the neural network will increase the model's capacity to learn more complex patterns in the data. \\n Adding the extra layer will improve the performance slightly, but the original model already achieves high accuracy (~0.970).\")\n\n\nQuestion: How does using a weight decay of 0.001 affect the training of the neural network model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using a weight decay of 0.001 (up from the original 5e-7) will increase regularization, penalizing large weights more heavily. \\n Training accuracy may decrease slightly, but will improve generalization.\")"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#creating-dataset",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#creating-dataset",
    "title": "  Section 1: Defining model",
    "section": "",
    "text": "We will be training a neural network to predict the color of a point (y \\(ϵ\\) \\(\\{\\)green, red, or blue \\(\\}\\) ) given an input \\(X\\) of two descripitive features (the x and y coordinates)\n\n\nCode\n# generate data for a classification task\nX, y = spiral_data(samples=100, classes=3)\n\n# convert dataset to pytorch tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\n# visualize dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#instantiating-model",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#instantiating-model",
    "title": "  Section 1: Defining model",
    "section": "",
    "text": "We will be using a feedforward neural network consisting of two dense (fully connected) layers with ReLU activation, where the first layer takes 2 input features and outputs 64 neurons, and the second layer takes 64 neurons and outputs 3 neurons.\n\n\n\nimage.png\n\n\n\n\nCode\n# Define the model\nmodel = MyModel()\n\n# Loss and optimizer\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-7)"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#questions",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#questions",
    "title": "  Section 1: Defining model",
    "section": "",
    "text": "Question: What happens if you change the activation function of the hidden layer from ReLU to sigmoid in a neural network? Try this change and comment on the performance change.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint('\\n ReLU outperforms sigmoid. \\n ReLU mitigates the vanishing gradient problem by allowing gradients to flow for positive inputs. \\n Sigmoid will lead to poorer convergence due to the smaller effective gradient.')\n\n\nQuestion: What effect does changing the loss function from CrossEntropyLoss to MeanSquaredError have on the training of a classification model? Try this and remark on what happens.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using MSE for classification results in worse performance. \\n This is because MSE doesn't explicitly encourage the model to push the probability of the correct class to 1 while minimizing the others.\")\n\n\nQuestion: How does using the SGD optimizer with a learning rate of 0.01 compare to using the Adam optimizer with a learning rate of 0.001?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Both optimizers perform similarly, but Adam with a learning rate of 0.001 will convergence faster.\")\n\n\nQuestion: What happens if you add another hidden layer with 128 neurons to the neural network model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Adding another hidden layer with 128 neurons to the neural network will increase the model's capacity to learn more complex patterns in the data. \\n Adding the extra layer will improve the performance slightly, but the original model already achieves high accuracy (~0.970).\")\n\n\nQuestion: How does using a weight decay of 0.001 affect the training of the neural network model?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using a weight decay of 0.001 (up from the original 5e-7) will increase regularization, penalizing large weights more heavily. \\n Training accuracy may decrease slightly, but will improve generalization.\")"
  },
  {
    "objectID": "notebooks/WS10_RFDiffusion_AllAtom.html",
    "href": "notebooks/WS10_RFDiffusion_AllAtom.html",
    "title": "Section 1: Introduction",
    "section": "",
    "text": "De novo protein design with RFDiffusion All Atom\n##Return to HomePage RoseTTAFold All-Atom : Krishna et al., Science 384, 291 (2024)\nObjectives: By the end of this workshop you should be able to:\n\nDesign a novel protein to bind a small molecule of interest\nIdentify and apply flags for RFDiffusion All Atom model\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n Table of Contents \nSection 1: Introduction\nSection 2: Small Molecule Binder Design\n\n\nCode\n!pip install torch==2.5.0\n\n\n\n Import Libraries\n\n\nCode\n#@markdown Please run as you read the introduction\n\n## 1) clone `rf_diffusion_all_atom` repo to local environment\n!git clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\n# move into repo\n%cd rf_diffusion_all_atom\n\n\n## 2) download container used to run RFAA\n'''\nauthor Sergey Lyskov - \"Minimal example of running Singularity Containers using\nNeurodesk tools inside Colab notebook.\nBased on https://colab.research.google.com/drive/1g5cnZxj1llRaHmOs4xSglqsXnFkQYuol\n-- big thanks to Neurodesk team for sharing this!\"\n'''\n\n#set environment variables\nimport os\n#set `LR_PRELOAD` to an empy string to prevent any preloaded libraries from interfering\nos.environ[\"LD_PRELOAD\"] = \"\";\n#set `APPTAINER_BINDPATH` to `/content` tp ensure colab's working dir is accessible in container\nos.environ[\"APPTAINER_BINDPATH\"] = \"/content\"\n#`LMOD_CMD` points to the system used to manage environment settings\nos.environ[\"LMOD_CMD\"] = \"/usr/share/lmod/lmod/libexec/lmod\"\n# download script from NeuroDesk\n!curl -J -O https://raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n# make script executable\n!chmod +x googlecolab_setup.sh\n# setup NeuroDesk env within colab\n!./googlecolab_setup.sh\n# set path for variable used by LMOD\nos.environ[\"MODULEPATH\"] = ':'.join(map(str, list(map(lambda x: os.path.join(os.path.abspath('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/'), x),os.listdir('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/')))))\n\n# print Alpine Linux image from DockerHub inside container\n!apptainer exec docker://alpine cat /etc/alpine-release\n# check version of Alpine Linux inside image\n!singularity exec docker://alpine cat /etc/alpine-release\n\n# download singularity conatainer file for RF-AA\n!wget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\n\n# run os-release in RF-SS container\n!singularity exec rf_se3_diffusion.sif cat /etc/os-release\n\n#!singularity run shub://vsoch/hello-world\n!singularity run docker://godlovedc/lolcow\n\n\n## 3) download model weights\n!wget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\n\n\n## 4) initialize git submodules\n!git submodule init\n!git submodule update\n\n\n\n\n\n\n\n\n\nThis workshop will explore the principles behind RFAA and RFdiffusionAA, examining their methodologies and applications to provide a comprehensive understanding of how computational tools are reshaping biomolecular science.\nSo far in the last few workshops we have focused on protein-only systems. In reality, proteins not only form complexes with other proteins, they interact with DNA, RNA, small molecules, and other non-protein ligands, during various cellular processes.\nThe goal of RoseTTAFold All-Atom (RFAA) was to develop a model that enables generalized prediction and design of such diverse biomolecular assemblies. Unlike previous approaches that focus solely on proteins, RFAA integrates atomic-level graph representations for small molecules and chemical modifications with sequence-based descriptions of proteins and nucleic acids. This innovation allows for accurate modeling of protein–small molecule complexes, covalently modified proteins, and assemblies involving multiple biomolecular components.\n\n\n\nimage.png\n\n\nRFAA enhances the RoseTTAFold2 (RF2) framework by incorporating a three-track architecture and specialized input representations tailored to each type of molecule\nInput Representations: * Proteins and Nucleic Acids: Represented as linear sequences of residues or bases, similar to RF2. * Small Molecules: Represented as atomic graphs where nodes correspond to atoms and edges denote bonds. Bond types (single, double, triple, aromatic) and stereochemical information (chirality) are explicitly encoded. * Covalent Modifications: Represented as part of the residue graph with bonds connecting small molecule atoms to protein residues.\nThe multi-track network contains 1D, 2D, and 3D tracks that encode separate representations of the inputs described above.\n\n1D Track: Encodes sequence-based information, including amino acid, nucleic acid, and chemical element types.\n2D Track: Encodes pairwise relationships such as chemical bonds and distances between atoms.\n3D Track: Encodes spatial configurations, including angles and chirality. This track is updated iteratively to refine the atomic coordinates.\n\n\n\n\nimage.png\n\n\nAfter the initial inputs to each of the three tracks, described above, the system is represented as a disconnected gas of nucleic acid bases, amino acid residues, and freely moving atoms. Through the various blocks in the architecture this disconnected gas transforms into plausible assembly structures.\nWhile RFAA provides a robust framework for predicting complex biomolecular assemblies, RFdiffusion All-Atom (RFdiffusionAA) extends these capabilities to the design of proteins that bind small molecules. By fine-tuning RFAA for diffusion-based tasks, RFdiffusionAA generates de novo protein structures with binding pockets custom-tailored to specific ligands.\nRFdiffusionAA starts with random residue distributions and uses a denoising diffusion probabilistic model (DDPM) to iteratively refine noisy structures into physically plausible protein backbones.\nSome key innovations of RFdiffusionAA include: * Unconditional Design: Can generate proteins from scratch rather than modifying existing scaffolds, enabling greater flexibility in design. * Structural Diversity: Produces a range of designs for the same target molecule, enhancing the likelihood of finding functional candidates. * High Compatibility: RFdiffusionAA-generated designs align well with predictions from AlphaFold2, indicating strong agreement between computational and experimental models.\n\n\n Section 2: Small molecule binder design\nIn the following PDB (7v11), the anticoagulant drug OQO (cyan) interacts with the blood clotting enzyme Factor XIa (green). OQO inhibits Factor XIa and reduces thrombosis (blood clotting formation) with minimal bleeding risks.\n\n\n\n7v11_rfdiff.png\n\n\nLet’s remove Factor XAi, and design a new protein to bind to OQO. To generate a binder to the ligand OQO from PDB 7v11, run the following:\nExample (ligand binder):\nExplanation of arguments * inference.deterministic=True seeds the random number generators used so that results are reproducible. i.e. running with inference.design_startnum=X will produce the same reusults. Note that torch does not guarantee reproducibility across CPU/GPU architectures: https://pytorch.org/docs/stable/notes/randomness.html * inference.num_designs=1 specifies that 1 design will be generated * contigmap.contigs=[\\'150-150\\'] specifies that the length of the generated protein should be 150 * diffuser.T=100 specifies the number of denoising steps taken.\nExpected outputs: * output/ligand_only/sample_0.pdb The design PDB * output/ligand_only/sample_0_Xt-1_traj.pdb The partially denoised intermediate structures * output/ligand_only/sample_0_X0-1_traj.pdb The predictions of the ground truth made by the network at each step\n\n\nCode\nimport time\n\n\n\n\nCode\nstart_time = time.time()\n\n\n\n\nCode\n!singularity run --nv rf_se3_diffusion.sif -u run_inference.py inference.deterministic=True diffuser.T=100 inference.output_prefix=output/ligand_only/sample inference.input_pdb=input/7v11.pdb contigmap.contigs=[\\'150-150\\'] inference.ligand=OQO inference.num_designs=1 inference.design_startnum=0\n\n\n\nINFO:    underlay of /usr/bin/nvidia-smi required more than 50 (496) bind mounts\n\n/bin/sh: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n\nDGL backend not selected or invalid.  Assuming PyTorch for now.\n\nSetting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n\n[14:48:53] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /opt/conda/envs/env/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.13.1.so: cannot open shared object file: No such file or directory\n\nUsing backend: pytorch\n\n/opt/conda/envs/env/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'aa': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n\n  warnings.warn(msg, UserWarning)\n\n[2024-10-23 14:48:58,471][inference.model_runners][INFO] - Reading checkpoint from RFDiffusionAA_paper_weights.pt\n\nloading RFDiffusionAA_paper_weights.pt\n\nloaded RFDiffusionAA_paper_weights.pt\n\nOVERRIDING: You are changing diffuser.T from the value this model was trained with.\n\n[2024-10-23 14:49:04,423][inference.model_runners][INFO] - Loading checkpoint.\n\n[2024-10-23 14:49:04,728][diffusion][INFO] - No IGSO3 cache found at /content/rf_diffusion_all_atom/cached_schedules/T_100_omega_1000_min_sigma_0_02_min_b_1_5_max_b_2_5_schedule_linear.pkl.\n\n[2024-10-23 14:49:04,729][diffusion][INFO] - Calculating IGSO3.\n\n[2024-10-23 14:51:26,713][__main__][INFO] - Making design output/ligand_only/sample_0\n\n[2024-10-23 14:51:26,714][__main__][INFO] - making design 0 of 0:1\n\n[2024-10-23 14:51:26,714][inference.model_runners][INFO] - Using contig: ['150-150']\n\n==============================\n\n*** Open Babel Error  in openLib\n\n  /usr/lib/openbabel/3.1.0/png2format.so did not load properly.\n\n Error: libXrender.so.1: cannot open shared object file: No such file or directory\n\nWith this beta schedule (linear schedule, beta_0 = 0.02, beta_T = 0.14), alpha_bar_T = 0.00022250170877669007\n\nCalculating chi_beta_T dictionary...\n\nDone calculating chi_beta_T dictionaries. They are now cached.\n\nDone calculating chi_beta_T, chi_alphas_T, and chi_abars_T dictionaries.\n\n[2024-10-23 14:53:57,019][inference.model_runners][INFO] - 14:53:57: Timestep 100, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:55:40,755][inference.model_runners][INFO] - 14:55:40: Timestep 99, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:57:26,554][inference.model_runners][INFO] - 14:57:26: Timestep 98, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:59:08,414][inference.model_runners][INFO] - 14:59:08: Timestep 97, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:00:54,301][inference.model_runners][INFO] - 15:00:54: Timestep 96, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:02:35,334][inference.model_runners][INFO] - 15:02:35: Timestep 95, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:04:12,369][inference.model_runners][INFO] - 15:04:12: Timestep 94, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:05:48,541][inference.model_runners][INFO] - 15:05:48: Timestep 93, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:07:32,936][inference.model_runners][INFO] - 15:07:32: Timestep 92, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:09:16,928][inference.model_runners][INFO] - 15:09:16: Timestep 91, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:11:03,153][inference.model_runners][INFO] - 15:11:03: Timestep 90, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n\n\n\n\n\nCode\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Execution time: {elapsed_time:.2f} seconds\")\n\n\n\n\nCode\nfrom google.colab import files\n\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/sample_0.pdb')\n\n\n\n\nCode\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/traj/sample_0_Xt-1_traj.pdb')\n\n\n\n\nCode\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/traj/sample_0_pX0_traj.pdb')\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget for displaying the full question\nquestion_label = widgets.Label(\n    value='Q1: RFAA extends RoseTTAFold2 by adding a _______ architecture that processes proteins, nucleic acids, small molecules, and covalent modifications.'\n)\n\n# Create fill-in-the-blank input field with wider text box\nquestion_1_input = widgets.Text(\n    placeholder='Enter your answer here',\n    layout=widgets.Layout(width='600px')  # Adjust the width here\n)\n\n# Create submit button for fill-in-the-blank\nsubmit_button_1 = widgets.Button(description=\"Submit Answer\")\n\n# Define what happens when the button is clicked\ndef on_button_clicked_1(b):\n    if question_1_input.value.lower() == 'three-track':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button_1.on_click(on_button_clicked_1)\n\n# Display the question label, input field, and submit button\ndisplay(question_label, question_1_input, submit_button_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget to display the full question\nquestion_label = widgets.Label(\n    value=\"Q2: How does RFdiffusionAA generate binding pockets for small molecules?\"\n)\n\n# Create a multiple-choice question\nquestion_2 = widgets.RadioButtons(\n    options=['A) By modifying pre-existing protein structures.',\n             'B) By using random selection of protein scaffolds.',\n             'C) By iteratively refining random residue distributions around target small molecules.',\n             'D) By relying on classical docking methods.'],\n    disabled=False,\n    layout=widgets.Layout(width='90%')  # Adjusting width for better display\n)\n\n# Create a submit button\nsubmit_button = widgets.Button(description=\"Submit Answer\", layout=widgets.Layout(width='30%'))\n\n# Define what happens when the button is clicked\ndef on_button_clicked(b):\n    if question_2.value == 'C) By iteratively refining random residue distributions around target small molecules.':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button.on_click(on_button_clicked)\n\n# Group the question, answer options, and button in a VBox layout\nquiz_layout = widgets.VBox([question_label, question_2, submit_button])\n\n# Display the question, options, and button\ndisplay(quiz_layout)\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget for displaying the full question\nquestion_label = widgets.Label(\n    value='Q3: One of the key advantages of RFdiffusionAA is that it generates proteins from _______ rather than modifying existing scaffolds.'\n)\n\n# Create fill-in-the-blank input field with wider text box\nquestion_3_input = widgets.Text(\n    placeholder='Enter your answer here',\n    layout=widgets.Layout(width='600px')  # Adjust the width here\n)\n\n# Create submit button for fill-in-the-blank\nsubmit_button_3 = widgets.Button(description=\"Submit Answer\")\n\n# Define what happens when the button is clicked\ndef on_button_clicked_3(b):\n    if question_3_input.value.lower() == 'scratch':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button_3.on_click(on_button_clicked_3)\n\n# Display the question label, input field, and submit button\ndisplay(question_label, question_3_input, submit_button_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget to display the full question\nquestion_label = widgets.Label(\n    value=\"Q4: What do the encodings represent for the 2D track in RFAA?\"\n)\n\n# Create a multiple-choice question\nquestion_2 = widgets.RadioButtons(\n    options=['A) A 3D voxel grid representing the 3D spatial arrangement of atoms in the molecule.',\n             'B) A sequence of residues encoded using one-hot encoding for amino acids.',\n             'C) The pairwise relationship between inputs such as chemical bonds and distances between atoms.',\n             'D) A graph representation that captures molecular interactions in a graph neural network framework.'],\n    disabled=False,\n    layout=widgets.Layout(width='90%')  # Adjusting width for better display\n)\n\n# Create a submit button\nsubmit_button = widgets.Button(description=\"Submit Answer\", layout=widgets.Layout(width='30%'))\n\n# Define what happens when the button is clicked\ndef on_button_clicked(b):\n    if question_2.value == 'C) The pairwise relationship between inputs such as chemical bonds and distances between atoms.':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button.on_click(on_button_clicked)\n\n# Group the question, answer options, and button in a VBox layout\nquiz_layout = widgets.VBox([question_label, question_2, submit_button])\n\n# Display the question, options, and button\ndisplay(quiz_layout)"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "##Return to HomePage\nTutorial adapted and images copied from Harrison Kinsley of sentdex\nOBJECTIVES: By the end of this workshop you should be able to understand the following concepts:\n\n Neuron (in the context of machine learning) \nForward pass / backward pass\nReLU activation\n Softmax activation loss and categorical cross entropy\nAdam optimizer\nWeights and biases and updating each\nTraining and epochs\n\nTo save your work, please save a copy of this notebook into your personal google drive.\n Table of Contents \nSection 1: Introduction and coding our first neuron\n\nWhat is a neuron?\nCalculating the inputs and outputs of a neuron\n\nSection 2: Coding a layer of neurons * Using lists of lists to calculate the output * Concept of Dot Protduct * Using batches in neural networks\nSection 3: Hidden Layer Activation Functions * Step Function * Sigmoid Function * Rectified Linear Unit\nSection 4: Output layer activation function * Softmax * Overflow Prevention\nSection 5: Calculating and implementing loss * Categorical Cross Entropy * Calculating Loss for a Batch of Outputs * Error: Log(0) * Formula for Accuracy\nSection 6: Backpropogation * Partial Derivative and Gradients * Chain Rule\nSection 7: Optimizers * Stochastic Gradient Descent\nSection 8: Learning Rate and Momentum * Learning Rate Decay * Momentum\nSection 9: Complete Neural Network from scratch\n\n\n\n\nCode\n# package for creating our dataset\n!pip install nnfs\n\n\nCollecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n\n\n\n\nCode\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport math\n\nnnfs.init()\n\n\n\n\n\n\nWhy are these called neural networks?\nBecause they visually look like a network - neurons are the blue circles connected by orange lines. In this case we have an input layer, two hidden layers of 4 neurons each, and an output layer.\nData is passed forward through this starting from the left (in this image you only have 2 pieces of data coming in as input), which is eventually passed to the output layer that returns something we want. An example output could be the classification of an image as cat or dog, where one of the output neurons is associated with the cat prediction and the other for dog, and the prediction made by the network is defined by which neuron was more heavily fired.\nWe can optimize the predictive accuracy of a neural network by tuning it’s weights and biases during training. We’ll discuss this further later…\n\n\n\nimage.png\n\n\nLet’s start by coding a neuron from a fully connected neural network like the one above. In a fully conncted neural network, every neuron has a unique connection to every single previous neuron.\nLet’s say we have 3 neurons from the previous layer feeding into the neuron we are building.\nConsider the single neuron below:\n\n\n\nimage.png\n\n\nLet’s calculate the outputs from the previous layer (which had 3 neurons)\n\n\nCode\n\n# this will be the input to our current neuron\ninputs = np.array([1, 2, 3])\n\n# every unique input will have a unique weight associated with it\n# since we have three inputs, we have 3 weights\nweights = np.array([0.2, 0.8, -0.5])\n\n# every unique neuron has a unique bias\nbias = 2\n\n# output from our neuron is the input*weight + bias\noutput = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\nprint(output)\n\n\n2.3\n\n\nNow let’s model one of the output layer neurons (layer 4, top neuron)\n\n\n\nimage.png\n\n\nWhat’s going to change? You have one more unique weight, but do you have any other biases? Nope! We are still only modeling a single neuron which has one associated bias.\n  Can you calculate the output of the neuron above using the same methodology of the previous cell?\n\n\nCode\n### PSEUDO CODE ###\n\n# define input array variable\n# define weight array variable\n# define bias variable\n\n# output = input * weight + bias\n# print(output)\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; Use this code if you're stuck\n\ninputs = np.array([1.0,2.0,3.0,2.5])\nweights = [0.2, 0.8, -0.5, 1.0]\nbias =[2.0]\noutput = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\nprint(output)\n\n\n\n\n\nHow about modeling 3 neurons, each with 3 inputs?\n\n\n\nimage.png\n\n\nSince there are three neurons, there’s going to be 3 unique weight sets. Each weight set is going to have 4 values (because there are 4 inputs). We’ll also need 3 unique biases.\n\n\nCode\n# inputs are from hidden layer\ninputs = np.array([1, 2, 3, 2.5])\n\n# neuron 1 (top)\nweights1 = np.array([0.2, 0.8, -0.5, 1.0])\nbias1 = 2\n\n# neuron 2 (middle)\nweights2 = np.array([0.5, -0.91, 0.26, -0.5])\nbias2 = 3\n\n# neuron 3 (bottom)\nweights3 = np.array([-0.26, -0.27, 0.17, 0.87])\nbias3 = 0.5\n\n# 3 neurons, with 4 inputs each\n# each neuron has unique set of weights and a separate bias\n# since our layer has 3 neurons, the layer output should be 3 values (one value for each of the neurons)\noutput = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3]+ bias1,\n          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3]+ bias2,\n          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3]+ bias3]\nprint(output)\n\n\n[4.8, 1.21, 2.385]\n\n\nWhat if we wanted the output to be something different? How would you achieve that? We can’t change the input of a neuron since that’s already provided from the previous layer (or it’s from the input layer). What about the weights and bias? Are they fixed?\nIt should start to become a little clearer that the weights and biases are like knobs in an neural network which we can tinker with to change the communication between all the other neurons.\nLets simplify the code above with a list of lists for weights (still 3 neurons). This is a cleaner way of doing inputs*weights + bias for a layer\n\n\nCode\ninputs = np.array([1, 2, 3, 2.5])\n\n# lets format the weights as a list of lists\nweights = np.array([[0.2, 0.8, -0.5, 1.0],\n                    [0.5, -0.91, 0.26, -0.5],\n                    [-0.26, -0.27, 0.17, 0.87]])\n\nprint(\"weights = \", weights)\n\n# and a list for the biases\nbiases = np.array([2,\n                   3,\n                   0.5])\n\nprint(\"bias =\", bias)\n\n# output of current layer\nlayer_outputs = []\n\n# zip together weights and biases (combines 2 lists into list of lists element-wise)\nfor neuron_weights, neuron_bias in zip(weights, biases):\n  # output of given neuron\n  neuron_output = 0\n\n  for n_input, weight in zip(inputs, neuron_weights):\n    # sum the inputs * weights\n    neuron_output += n_input*weight\n\n  # add the bias term\n  neuron_output += neuron_bias\n  # output from neurons\n  layer_outputs.append(neuron_output)\n\nprint(\"output =\", layer_outputs)\n\n\n[4.8, 1.21, 2.385]\n\n\nWhy do we need both weights and biases to tune/tweak? Hypothetically you don’t need both. But having both help diversify your possible outputs since they both offer different types of modifications. This is exactly like conceptualizing the graph y = mx + b. Changing your slope (weight) affects the steepness of your graph, and changing your b (bias) changes the location of your plot.\n\n\n\nimage.png\n\n\nWe’ll be working with arrays of many shapes and sizes. Consider the three below:\n\n\nCode\n# type: list, 1D array, vector\nl = np.array([1, 5, 6, 2])\n\n# type: list of lists, 2D array, matrix\nlol = np.array([[1, 5, 6, 2],\n                [3, 2, 1, 3]])\n\n# type: list of list of lists, 3D array\nlolol = np.array([[[1, 5, 6, 2],\n                   [3, 2, 1, 3]],\n                  [[5, 2, 1, 2],\n                   [6, 4, 8, 4]],\n                  [[2, 8, 5, 3],\n                   [1, 1, 9, 4]]])\n\n\n Print the shape of each below using the .shape numpy method\n\n\nCode\n### PSEUDOCODE ###\n# print( shape of l, lol, and lolol )\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Double Click to See Answer!&lt;/font&gt;\nprint(l.shape, lol.shape, lolol.shape)\n\n\n Note that arrays have to be homologous: At each dimension, they need to have the same size\n## Introduction to dot product\n(feel free to skip if you already have a firm grasp)\nHow do we multiply inputs (vector) by weights (matrix of vectors)? Dot product! Can you perform the dot product between a and b?\nRecall:\n\\(a \\cdot b= \\sum \\limits_ {i=1}^n a_i b_i\\)\n  Write the code for a dot product in the code box below \n\n\nCode\na = np.array([1, 2, 3])\nb = np.array([2, 3, 4])\n\n### PSUEDO CODE ###\n# dot product is multiplying element-wise the values of the vectors and adding them together\n# dot_product = ?\n\n# results in single, scalar value\n# print dot_product\n\n\n  Write the code to calculate the output for a single neuron connected to 3 neurons in the previous layer \n\n\nCode\n# We can write this more succinctly with numpy\ninputs = np.array([1, 2, 3, 2.5])\nweights = np.array([0.2, 0.8, -0.5, 1.0])\n\nbias = 2\n\n# here order doesnt matter, but why does weight normally come first?\noutput = np.dot(weights, inputs) + bias\nprint(output)\n\n\n4.8\n\n\n  Write the code to calculate the output for a dotlayer of neurons in the code box below \n\n\nCode\n# dot product in numpy for a layer of neurons\ninputs = np.array([1, 2, 3, 2.5])\n\nweights = np.array([[0.2, 0.8, -0.5, 1.0],\n                    [0.5, -0.91, 0.26, -0.5],\n                    [-0.26, -0.27, 0.17, 0.87]])\n\nbiases = np.array([2, 3, 0.5])\n\noutput = np.dot(weights, inputs) + biases # now order DOES matter\n\n# here we perform the dot product 3 times:\n# [np.dot(weights[0], inputs),\n#  np.dot(weights[1], inputs),\n#  np.dot(weights[2], inputs)] = [2.8, -1.79, 1.885]\n\n# then add biases:\n# np.dot(weights, inputs) + biases = [2.8, -1.79, 1.885] + [2.0, 3.0, 0.5] = [4.8, 1.21, 2.385]\n\nprint(output)\n\n\n[4.79999995 1.21000004 2.38499999]\n\n\n\n\nBatches allow calculating things in parallel. The bigger the batch, the more parallel operations we can run. This is also why we tend to do neural network training on GPUs which have 100s-1000s of cores that we can run calculations on (rather than 4-8 cores on CPUs). Batches also help with generalization. Exposing our network with multiple datapoints at a time, rather than one at a time or all points at once, is widely understood to help a neural network to generalize. Generally we will work with batch sizes of 32, 64, maybe 128\n\n\n\n\nActivation functions are used for a multitude of reasons in the hidden layers (layers other than the input or output layer). They help normalize the output of each neuron which prevents values from growing too large or too small as they pass through the network.\nEach neuron will have an activation function, which comes into play after calculating the inputs * weights + bias.\nThus the output = Activation Function (inputs * weights + bias)\nHere we will talk about 3 activations functions 1. Step function 2. Sigmoid function 3. Rectified linear (ReLU) function.\nIn the   step function  if the input is greater than 0 the output is 1, otherwise the output is a 0.\n\n\n\nimage.png\n\n\nTherefore in the case of the step function, the output of your neuron is literally a 0 or a 1.\n\n\n\nimage.png\n\n\nWhie this is theoretically okay, it quickly becomes obvious that using something with more granularity would lead to better training for a neural network. For example in the step function, if the output is 0, how do we know how close input value was from reaching an output of 1? Knowing this will help the model to correct itself better and comes into play when computing loss and optimizing our weights and biases.\nOne such function would be the sigmoid function. We can readily see how close (or far away) the output is to 0 (or 1). However, even the sigmoid functions are not perfect but their drawback of vanishing gradients won’t make too much sense until we get to gradients.\n\n\n\nimage.png\n\n\nThis brings us to the rectified linear unit (ReLU) activation function.\nIf x is greater than 0, the output is whatever x is, and otherwise the output is 0. One of the main reasons we use the ReLU is that it’s fast: If x less than 0 output 0, if x is greater than 0 output x.\nYou may question why we don’t use a linear activation function (like y=x), but modeling non-linear data with a linear function is just not possible :( So we do need non-linear activation functions\n\n\n\nimage.png\n\n\nLet’s recall the different parameters for tweaking the output of a neural network we have learnt so far..\nRevisiting how tweaking the weights and biases will affect the output of our neuron… changing the weights will change the strength (slope) of the activation, and we can offset the activation point with the bias. You could also negate the weight to instead see what value of x is needed to deactivate your neuron\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Food For Thought! Run to Show Question &lt;/b&gt;\n\nfrom IPython.display import HTML, display\ndef match_the_following(questions, answers):\n    print(\"Match the following:\")\n\n    for q in questions:\n        print(q)\n    for a in answers:\n        print(a)\n\n    # Get user inputs\n    response = {\n        \"1\": input(\"Match for 1 (enter A, B, or C): \").upper(),\n        \"2\": input(\"Match for 2 (enter A, B, or C): \").upper(),\n        \"3\": input(\"Match for 3 (enter A, B, or C): \").upper(),\n    }\n\n    # Define correct answers\n    correct_answers = {\"1\": \"B\", \"2\": \"A\", \"3\": \"C\"}\n\n    # Check and provide feedback\n    for key, value in correct_answers.items():\n        if response[key] == value:\n            print(f\"Correct match for {questions[int(key)-1]}\")\n        else:\n            print(f\"Incorrect match for {questions[int(key)-1]}. The correct answer is {value}.\")\n\n# Call the function\nquestions = [\"1. What controls the offset of the output\", \"2. What controls the strength/slope of the output?\", \"3. What do we use to normalize the output?\"]\nanswers = [\"A. Weights\", \"B. Bias\", \"C. Activation Function\"]\n\nmatch_the_following(questions, answers)\n\n\n\nMatch the following:\n1. What controls the offset of the output\n2. What controls the strength/slope of the output?\n3. What do we use to normalize the output?\nA. Weights\nB. Bias\nC. Activation Function\nMatch for 1 (enter A, B, or C): B\nMatch for 2 (enter A, B, or C): A\nMatch for 3 (enter A, B, or C): C\nCorrect match for 1. What controls the offset of the output\nCorrect match for 2. What controls the strength/slope of the output?\nCorrect match for 3. What do we use to normalize the output?\n\n\n  Write an algorithm in the coding cell below that accepts each value from the inputs array and computes the output of the ReLU function for each value\n\n\nCode\ninputs = np.array([0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100])\noutput = []\n\n### PSEUDO CODE ###\n# for i in inputs:\n#   if i is greater than 0, ReLU(i) = i\n#   if i is less than or equal to 0, ReLU(i) = 0\n\n# print output\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Double Click to See Answer!&lt;/font&gt;\n\ninputs = np.array([0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100])\noutput = []\nfor i in inputs:\n  if i &gt; 0:\n    output.append(i)\n  else:\n    output.append(0)\nprint(output)\n\n\n\n\n\nWhy another activation function? We want a function to help quantify relative “correct-ness”. In the two output layers below, which is relatively more correct? Assuming the first node in the array is the correct prediction, the layer_outputs1 is probably more correct since it is relatively larger than the other array values.\n\n\nCode\nlayer_outputs1 = np.array([4.8, 1.21, 2.385])\nlayer_outputs2 = np.array([4.8, 4.79, 4.25])\n\n\nThe softmax function is commonly used in machine learning for multiclass classification tasks. It takes a vector of arbitrary real-valued scores (often called logits) as input and normalizes them into a probability distribution over multiple classes.\nThe softmax function for a vector \\(z\\) with \\(K\\) elements is defined as:\n\\[ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\]\nwhere: - $ (z)_i $ is the \\(i\\)-th element of the resulting probability distribution. - \\(e\\) is the base of the natural logarithm (Euler’s number). - \\(z_i\\) is the \\(i\\)-th element of the input vector \\(z\\). - The denominator is the sum of exponentiated scores over all classes.\nThe softmax function ensures that the output probabilities sum to 1, making it suitable for classification tasks where the model needs to output class probabilities. Can you calculate the softmax of layer_outputs1 and layer_outputs2? After computing the softmax of each layer, also print the sum of the softmax’ed array to confirm they sum to 1\n\n\nCode\n## PSEUDO CODE ##\n\n# layer outputs 1\n# softmax(layer_outputs1) = e^(layer_outputs1) / sum( e^(layer_outputs1) )\n\n# layer outputs 2\n# softmax(layer_outputs2) = e^(layer_outputs2) / sum( e^(layer_outputs2) )\n\n# print sum( softmax1 ), sum( softmax2)\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; Double click if you're stuck &lt;/font&gt;\n\n#since we want to calculate the softmax for 2 outputs, instead of writing the code twice we can write a function instead\ndef calculate_softmax(layer_output):\n  softmax = np.exp(layer_output)/ np.sum(np.exp(layer_output))\n  return softmax\n\ns1 = calculate_softmax(layer_outputs1)\nsum1 = np.sum(s1)\ns2 = calculate_softmax(layer_outputs2)\nsum2 = np.sum(s2)\nprint(s1, sum1, s2, sum2)\n\n\nLets move from a single vector of a layer’s outputs, in reality we’ll have a batch of outputs\n\n\nCode\n# batch of 3 outputs\nlayer_outputs = np.array([[4.8, 1.21, 2.385], # output 1\n                          [8.9, -1.81, 0.2], # output 2\n                          [1.41, 1.051, 0.026]]) # output 3\n#overflow prevention\nlayer_outputs = layer_outputs - layer_outputs.max()\nexp_values = np.exp(layer_outputs)\n\n# now, how do we do a sum? we use axis\n# axis=1 is sum of rows, which is what we want\nnorm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\nprint(norm_values)\n\n\n[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n\n\nWhat’s overflow prevention?\nWhen working with exponentials it is easy to reach really large values that the processor cannot calculate and becomes a roadblock.\n\n\nCode\n# an example overflow which will become a roadblock in NN calculations\nnp.exp(10000)\n\n\nRuntimeWarning: overflow encountered in exp\n  np.exp(10000)\n\n\ninf\n\n\nWe can prevent this in our neural network by taking all of our outputs, and subtracting the largest value in the layer from all values in the layer. Now the largest value is 0 and everything else is less than 0.\n\n\n\nHow can we optimize our model? Should we optimize for accuracy of the predicted label vs true label? This is a good strategy, but you will be throwing away a lot of useful information since your output is a probability distribution not a single number.\nCategorical cross-entropy is a commonly used loss function in neural network classification models, especially for multiclass classification tasks. It measures the dissimilarity between the true probability distribution of the labels and the predicted probability distribution outputted by the model. This loss penalizes large errors between the true and predicted probabilities, encouraging the model to output higher probabilities for the correct class labels.\n\\[ L(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij}) \\]\nwhere: - $ L(y, ) $ is the categorical cross-entropy loss. - $ N $ is the number of samples in the dataset. - $ K $ is the number of classes. - $ y $ is the true one-hot encoded label matrix of shape $ (N, K) $. - $ $ is the predicted probability distribution matrix outputted by the model of shape $ (N, K) $. - $ y_{ij} $ is the true probability that sample \\(i\\) belongs to class $ j $. - \\(\\hat{y}_{ij}\\) is the predicted probability that sample \\(i\\) belongs to class \\(j\\).\n\n\nCode\nsoftmax_output = np.array([0.7, 0.1, 0.2])\ntarget_output = np.array([1, 0, 0]) # means target class is 1\n\ntarget_class = 0\n\n# cce = - Σ y_true * log( y_pred)\nloss = -(math.log(softmax_output[0])*target_output[0] +\n         math.log(softmax_output[1])*target_output[1] +\n         math.log(softmax_output[2])*target_output[2])\n\nprint(loss)\n\n\n0.35667494393873245\n\n\n\n\nCode\n# since our true distribution has only one entry that equals 1 and the others are zero (one hot encoding), we can simply apply to the non-zero term\nloss = -math.log(softmax_output[0])\nprint(loss)\n\n\n0.35667494393873245\n\n\n\n\nCode\n# consider if prediction was 0.5 for the true class, not 0.7\n# you can now see that the loss is higher\nprint(-math.log(0.7),-math.log(0.5))\n\n\n0.35667494393873245 0.6931471805599453\n\n\n Let’s calculate the loss for a batch of three images\n\n\nCode\nsoftmax_outputs =np.array([[0.7, 0.1, 0.2], # prob distribution for image 1 (0.7)\n                          [0.1, 0.5, 0.4], # ...image 2 (0.5)\n                          [0.02, 0.9, 0.08]]) # ... image 3 (0.9)\n\n# 0 = dog, 1 = cat, 2 = human\n# image 1 dog, image 2 cat, image 3 cat\nclass_targets = [0, 1, 1]\n\nprint(softmax_outputs[[0, 1, 2], class_targets])\n\n\n[0.7 0.5 0.9]\n\n\n\n\nCode\n# Apply loss function to our softmaxed values\nneg_log = -np.log(softmax_outputs[\n    range(len(softmax_outputs)), class_targets\n  ])\n\naverage_loss = np.mean(neg_log)\nprint(neg_log) # all cce's\nprint(average_loss) # average over batch\n\n\n[0.35667494 0.69314718 0.10536052]\n0.38506088005216804\n\n\nWe will eventually hit a problem…  log(0)\n\n\nCode\nsoftmax_outputs =np.array([[0.0, 0.1, 0.2], # consider prob 0 for true class\n                          [0.1, 0.5, 0.4],\n                          [0.02, 0.9, 0.08]])\n\nclass_targets = [0, 1, 1]\n\nprint(-np.log(softmax_outputs[[0, 1, 2], class_targets]))\n\n\n[       inf 0.69314718 0.10536052]\n\n\nRuntimeWarning: divide by zero encountered in log\n  print(-np.log(softmax_outputs[[0, 1, 2], class_targets]))\n\n\n\n\nCode\n# now the batch loss is infinite!\nprint(np.mean(-np.log(softmax_outputs[[0, 1, 2], class_targets])))\n\n\ninf\n\n\nRuntimeWarning: divide by zero encountered in log\n  print(np.mean(-np.log(softmax_outputs[[0, 1, 2], class_targets])))\n\n\nOne option is to clip the values by some insigificant amount\n\n\nCode\n# clip 1e-7 from 0\nprint(-np.log(1e-7))\n\n\n16.11809565095832\n\n\nClipping the predicted values ensures we don’t wind up with this infinite problem\n\n\nCode\n# y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n\nAnd how do we calculate accuracy?\n\n\nCode\npredictions = np.argmax(softmax_outputs, axis=1)\nprint(predictions)\n\n# average of the amount of times prediction matches target class\naccuracy = np.mean(predictions == class_targets)\nprint(accuracy)\n\n\n[2 1 1]\n0.6666666666666666\n\n\n\n\n\nRandomly searching for weights and biases is inefficient. In order to know how to adjust weights and biases, we first need to understand their impact on loss. To learn the impact of all the inputs, weights, and biases to the neuron output and at the end of the loss function, we need to calculate the derivative of each operation performed during the forward pass in a neuron and whole model\n\n\n\nimage.png\n\n\nDerivatives are not limited to addition/multiplication. We need to derive them for other functions used in forward pass, like max()\n\n\n\nimage.png\n\n\nThe partial derivative measures how much impact a single input has on a function’s output.\nWhen you combine the partial derivative of all the parameters, the resulting set of equations are called the gradient.\n\n\n\nimage.png\n\n\nSimilar to the image below, the forward pass is a chain of functions. We pass input data to the first layer where we have the layer’s weights and biases and the resultant flows through the ReLU activation function. We repeat this calculation in another layer, all the way to the output layer and softmax activation\n\n\n\nimage.png\n\n\nWe need to learn how each weight and bias impacts loss to improve loss. To do this for a chain of functions, use the chain rule, which says that the derivative of a function chain is a product of derivatives of all the functions in this chain.\nTo calculate how a parameter (like a weight) affects the loss, we:\n\nStart by finding the derivative of the outermost function with respect to the next function.\nMultiply this by the derivative of the next function with respect to the one before it.\nContinue multiplying the derivatives of each function in the chain until you reach the parameter (weight or bias).\n\nThe chain rule is essential for figuring out how a single input impacts the final output, which in this case is the loss.\n\n\n\nimage.png\n\n\nLet’s backpropogate the ReLU function for a single neuron and act as if we intend to minimise the output for this single neuron. We will leverage the chain rule with derivatives and partial derivatives to calculate the impact of each variable on the ReLU activated output.\n\n\n\nimage.png\n\n\n\n\nCode\n# neuron with 3 inputs\nx = np.array([1.0, -2.0, 3.0]) # inputs\nw = np.array([-3.0, -1.0, 2.0]) # weights\nb = 1.0 # bias\n\n## forward pass\n# input*weights\nxw0 = x[0] * w[0]\nxw1 = x[1] * w[1]\nxw2 = x[2] * w[2]\n\n# add bias\nz = xw0 + xw1 + xw2 + b\n\n# ReLU activation function\ny = max(z, 0)\nprint(y)\n\n\n6.0\n\n\nFirst step is to backpropogate our gradients by calculating derivatives and partial derivatives wrt each of our parameters and inputs using the chain rule. The nested function we are faced with can be written as below.\n\n\n\nimage.png\n\n\nThe derivatives wrt the weights and a bias will inform us about their impact and will be used to update their weights and bias. The derivative wrt the inputs are used to chain more layers by passing them to the previous function in the chain.\nWe will have to calculate the derivative of the loss function and apply the chain rule with the derivatives of all activation functions and neurons in all of the consecutive layers. The derivative wrt weights and biases is used to update any parameters. The derivative wrt layer’s inputs is used to chain to another layer (which is why we backpropogate to the previous layer in the chain)\nBackward pass\nDerivative from next layer:\n\\(\\frac{d y}{d y} = 1.0\\)\nDerivative of ReLU and chain rule:\n\\((1.0) * \\frac{d}{d z}ReLU(z) = 1(z&gt;0)=1(6&gt;0)= 1.0 * 1.0 = 1.0\\)\nPartial derivatives of summation and chain rule:\n\\(1.0 * \\frac{\\delta}{\\delta (x_0 w_0)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta (x_1 w_1)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta (x_2 w_2)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta b} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\nPartial derivatives of multiplication and chain rule:\n\\((1.0) * \\frac{\\delta}{\\delta x_0}(x_0w_0) = (1.0) * w_0 = -3.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_0}(x_0w_0) = (1.0) * x_0 = 1.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta x_1}(x_0w_0) = (1.0) * w_1 = -1.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_1}(x_0w_0) = (1.0) * x_1 = -2.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta x_2}(x_0w_0) = (1.0) * w_2 = 2.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_2}(x_0w_0) = (1.0) * x_2 = 3.0\\)\n\n\nCode\n## backward pass\n# derivative from the next layer\ndvalue = 1.0 # ∂y / ∂y = 1\nprint(dvalue)\n\n# derivative of ReLU and chain rule\n# (∂y / ∂y) * (∂ ReLU(z) / ∂\ndrelu_dz = dvalue * (1.0 if z &gt; 0 else 0.)\nprint(drelu_dz)\n\n# partial derivatives of summation, the chain rule\ndsum_dxw0 = 1\ndsum_dxw1 = 1\ndsum_dxw2 = 1\ndsum_db = 1\ndrelu_dxw0 = drelu_dz * dsum_dxw0\ndrelu_dxw1 = drelu_dz * dsum_dxw1\ndrelu_dxw2 = drelu_dz * dsum_dxw2\ndrelu_db = drelu_dz * dsum_db\nprint(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n\n# Partial derivatives of the multiplication and chain rule\ndmul_dx0 = w[0]\ndmul_dx1 = w[1]\ndmul_dx2 = w[2]\ndmul_dw0 = x[0]\ndmul_dw1 = x[1]\ndmul_dw2 = x[2]\ndrelu_dx0 = drelu_dxw0 * dmul_dx0\ndrelu_dw0 = drelu_dxw0 * dmul_dw0\ndrelu_dx1 = drelu_dxw1 * dmul_dx1\ndrelu_dw1 = drelu_dxw1 * dmul_dw1\ndrelu_dx2 = drelu_dxw2 * dmul_dx2\ndrelu_dw2 = drelu_dxw2 * dmul_dw2\nprint(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n\n\n1.0\n1.0\n1.0 1.0 1.0 1.0\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n\n\nWorking backward by taking the ReLU() derivative, taking the summing operation’s derivative, multiplying both, so on… is backpropogating using the chain rule. The resulting output function’s gradients are passed back through NN, using multiplication of the gradient of subsequent functions from later layers with the current one.\nThis is the complete set of the activated neuron’s partial derivatives wrt the inputs, weights, and a bias. If you’d like to see a visualization of this backpropogation process, check out this video\n\n\n\nWe want to use the calculated gradient to adjust weights and biases, to ultimately decrease the measure of loss. We can successfully decrease a neuron’s activation function’s (ReLU) output in this manner. In our previous approach, we subtracted a fraction of the gradient from each weight and bias to achieve this adjustment. When we have millions, billions, or even more dimensions, stochastic gradient descent (SGD) is the best known way to search for global minimum.\n\n\n\nEven though we want our model to be as accurate as possible, the optimizer’s goal is to minimize loss, not directly improve accuracy. Loss is the average of individual sample losses, so while some losses may decrease significantly, others might only increase slightly, which can lead to incorrect predictions. This can lower the overall loss but but increase the number of incorrectly predicted samples, thus lowering the accuracy.\nEarly in training, larger updates help explore different areas of the solution space, but as training progresses, we want smaller, more refined updates. To achieve this we implement a learning rate decay. By dividing the initial learning rate by the step count, we ensure the learning rate drops quickly at first and slows down later. This allows the model to make fine adjustments as it nears the optimal solution.\nIn the last section we introduced SGD as a way to optimize the model. While it can be effective, it’s a basic method that follows the gradient without additional logic that could potentially help the model find the global minimum to the loss function.\nMomentum addresses this by creating a rolling average of gradients over several updates, combining this average with the current gradient. This helps the model move past local minima and further reduce loss.\n\n\n\nLet’s implement the concepts we learnt into a simple neural network. The intention of this workshop was to introduce some of the key concepts behind neural networks - not cover every aspect of machine learning (a strong grasp will require multiple workshops and experience playing with several architectures and datasets). Athough a first pass might be intimidating, please run each of the cells below to observe instantiating and training our model. After doing so, feel free to take a second pass through the code.\n\n\nBelow is a slew of classes we will be using to create our model. You are not expected to to understand every class, but skim through the comments to see the general purpose of each.\n\n\nCode\n# Dense layer\nclass Layer_Dense:\n  \"\"\"\n  Dense layer of a neural network\n  Facilitates:\n  - Forward propogation of data throught layer\n  - Backward propogation of gradients during training\n  \"\"\"\n\n  # Layer initialization\n  def __init__(self, n_inputs, n_neurons):\n    # Initialize weights and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1, n_neurons))\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs, weights and biases\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Gradients on parameters\n    self.dweights = np.dot(self.inputs.T, dvalues)\n    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n    # Gradient on values\n    self.dinputs = np.dot(dvalues, self.weights.T)\n\n\n# ReLU activation\nclass Activation_ReLU:\n  \"\"\"\n  Rectified linear unit activation function\n  Applied to input of neural network layer\n  Introduces non-linearity into the network\n  \"\"\"\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs\n    self.output = np.maximum(0, inputs)\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Since we need to modify original variable,\n    # let’s make a copy of values first\n    self.dinputs = dvalues.copy()\n    # Zero gradient where input values were negative\n    self.dinputs[self.inputs &lt;= 0] = 0\n\n\n# Softmax classifier - combined Softmax activation\n# and cross-entropy loss for faster backward step\nclass Activation_Softmax_Loss_CategoricalCrossentropy():\n  \"\"\"\n  Combination of softmax activation function and categorical cross entropy loss function\n  Commonly used in classification tasks\n  We minimize loss by adjustng model parameters to improve performance\n  \"\"\"\n\n  # create activation and loss function objectives\n  def __init__(self):\n    self.activation = Activation_Softmax()\n    self.loss = Loss_CategoricalCrossentropy()\n\n  # forward pass\n  def forward(self, inputs, y_true):\n    # output layer's activation function\n    self.activation.forward(inputs)\n    # set the output\n    self.output = self.activation.output\n    # calculate and return loss value\n    return self.loss.calculate(self.output, y_true)\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n\n    # if labels one-hot encoded, turn into discrete values\n    if len(y_true.shape) == 2:\n      y_true = np.argmax(y_true, axis=1)\n\n    # copy so we can safely modify\n    self.dinputs = dvalues.copy()\n    # Calculate gradient\n    self.dinputs[range(samples), y_true] -= 1\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n# Adam optimizer\nclass Optimizer_Adam:\n\n  \"\"\"\n  Adam optimization algorithm to optimize parameters of neural network\n  Initalize with learning rate, decay, epsilon, momentum\n  Pre-update params: Adjust learning rate based on decay\n  Update params: Update params using momentum and cache corrections\n  Post-update params: Track number of optimization steps performed\n  \"\"\"\n\n  # Initialize optimizer - set settings\n  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n    self.learning_rate = learning_rate\n    self.current_learning_rate = learning_rate\n    self.decay = decay\n    self.iterations = 0\n    self.epsilon = epsilon\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n\n  # Call once before any parameter updates\n  def pre_update_params(self):\n    if self.decay:\n      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n\n  # Update parameters\n  def update_params(self, layer):\n\n    # If layer does not contain cache arrays, create them filled with zeros\n    if not hasattr(layer, 'weight_cache'):\n      layer.weight_momentums = np.zeros_like(layer.weights)\n      layer.weight_cache = np.zeros_like(layer.weights)\n      layer.bias_momentums = np.zeros_like(layer.biases)\n      layer.bias_cache = np.zeros_like(layer.biases)\n\n    # Update momentum with current gradients\n    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n\n    # Get corrected momentum\n    # self.iteration is 0 at first pass\n    # and we need to start with 1 here\n    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n\n    # update cache with squared current gradients\n    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n\n    # get corrected cache\n    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n\n    # Vanilla SGD parameter update + normalization with square root cache\n    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n\n  # call once after any parameter updates\n  def post_update_params(self):\n    self.iterations += 1\n\n\n# Softmax activation\nclass Activation_Softmax:\n\n  \"\"\"\n  Softmax activation function for multi-class classification\n  Compute probabilities for each class\n  \"\"\"\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Get unnormalized probabilities\n    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n\n    # Normalize them for each sample\n    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\n    self.output = probabilities\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Create uninitialized array\n    self.dinputs = np.empty_like(dvalues)\n\n    # Enumerate outputs and gradients\n    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n      # Flatten output array\n      single_output = single_output.reshape(-1, 1)\n\n      # Calculate Jacobian matrix of the output\n      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n\n      # Calculate sample-wise gradient and add it to the array of sample gradients\n      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n\n\n# Common loss class\nclass Loss:\n\n  # calculates data and regularization losses, given model output and ground truth values\n  def calculate(self, output, y):\n\n    # calculate sample losses\n    sample_losses = self.forward(output, y)\n\n    # calculate mean losses\n    data_loss = np.mean(sample_losses)\n\n    # return loss\n    return data_loss\n\n\n# cross entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n  \"\"\"\n  Computes categorical cross entropy\n  Quantifies discrepency between predicted and true class probabilities\n  \"\"\"\n\n  # forward pass\n  def forward(self, y_pred, y_true):\n\n    # number samples in batch\n    samples = len(y_pred)\n\n    # clip data to prevent division by 0\n    # clip both sides to not drag mean towards any value\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n    # probabilities for target values (only if categorical labels)\n    if len(y_true.shape) == 1:\n      correct_confidences = y_pred_clipped[ range(samples), y_true ]\n\n    # mask values (only for one-hot encoded labels)\n    elif len(y_true.shape) == 2:\n      correct_confidences = np.sum( y_pred_clipped * y_true, axis=1 )\n\n    # losses\n    negative_log_likelihoods = -np.log(correct_confidences)\n    return negative_log_likelihoods\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n    # Number of labels in every sample\n    # We’ll use the first sample to count them\n    labels = len(dvalues[0])\n\n    if len(y_true.shape) == 1:\n      y_true = np.eye(labels)[y_true]\n\n    # calculate gradient\n    self.dinputs = -y_true / dvalues\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n\n\n\nWe will be training a neural network to predict the color of a point (y \\(ϵ\\) \\(\\{\\)green, red, or blue \\(\\}\\) ) given an input \\(X\\) of two descripitive features (the x and y coordinates)\n\n\nCode\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# an example feature set (x,y) and it's label (color)\nX[5], y[5]\n\n\n\n\n\nWe will be using a feedforward neural network consisting of two dense (fully connected) layers with ReLU activation, where the first layer takes 2 input features and outputs 64 neurons, and the second layer takes 64 neurons and outputs 3 neurons.\n\n\n\nimage.png\n\n\n\n\nCode\n# Create Dense layer with 2 input features and 64 output values\ndense1 = Layer_Dense(2, 64)\n\n# Create ReLU activation (to be used with Dense layer):\nactivation1 = Activation_ReLU()\n\n# Create second Dense layer with 64 input features (as we take output\n# of previous layer here) and 3 output values (output values)\ndense2 = Layer_Dense(64, 3)\n\n# Create Softmax classifier’s combined loss and activation\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n\n# Create optimizer\noptimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n\n\n\n\n\n\n\nCode\n# Train in loop\nfor epoch in range(10001):\n  # Perform a forward pass of our training data through this layer\n  dense1.forward(X)\n\n  # Perform a forward pass through activation function\n  # takes the output of first dense layer here\n  activation1.forward(dense1.output)\n\n  # Perform a forward pass through second Dense layer\n  # takes outputs of activation function of first layer as inputs\n  dense2.forward(activation1.output)\n\n  # Perform a forward pass through the activation/loss function\n  # takes the output of second dense layer here and returns loss\n  loss = loss_activation.forward(dense2.output, y)\n\n  # Calculate accuracy from output of activation2 and targets\n  # calculate values along first axis\n  predictions = np.argmax(loss_activation.output, axis=1)\n\n  if len(y.shape) == 2:\n    y = np.argmax(y, axis=1)\n\n  accuracy = np.mean(predictions==y)\n\n  if not epoch % 100:\n    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n\n  # backward pass\n  loss_activation.backward(loss_activation.output, y)\n  dense2.backward(loss_activation.dinputs)\n  activation1.backward(dense2.dinputs)\n  dense1.backward(activation1.dinputs)\n\n  # update weights and biases\n  optimizer.pre_update_params()\n  optimizer.update_params(dense1)\n  optimizer.update_params(dense2)\n  optimizer.post_update_params()\n\n\n\nepoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.670, loss: 0.705, lr: 0.04999752512250644\nepoch: 200, acc: 0.797, loss: 0.522, lr: 0.04999502549496326\nepoch: 300, acc: 0.847, loss: 0.430, lr: 0.049992526117345455\nepoch: 400, acc: 0.887, loss: 0.344, lr: 0.04999002698961558\nepoch: 500, acc: 0.910, loss: 0.303, lr: 0.049987528111736124\nepoch: 600, acc: 0.907, loss: 0.276, lr: 0.049985029483669646\nepoch: 700, acc: 0.917, loss: 0.252, lr: 0.049982531105378675\nepoch: 800, acc: 0.920, loss: 0.245, lr: 0.04998003297682575\nepoch: 900, acc: 0.930, loss: 0.228, lr: 0.049977535097973466\nepoch: 1000, acc: 0.940, loss: 0.217, lr: 0.049975037468784345\nepoch: 1100, acc: 0.937, loss: 0.205, lr: 0.049972540089220974\nepoch: 1200, acc: 0.947, loss: 0.192, lr: 0.04997004295924593\nepoch: 1300, acc: 0.947, loss: 0.184, lr: 0.04996754607882181\nepoch: 1400, acc: 0.943, loss: 0.183, lr: 0.049965049447911185\nepoch: 1500, acc: 0.943, loss: 0.189, lr: 0.04996255306647668\nepoch: 1600, acc: 0.943, loss: 0.165, lr: 0.049960056934480884\nepoch: 1700, acc: 0.943, loss: 0.161, lr: 0.04995756105188642\nepoch: 1800, acc: 0.943, loss: 0.158, lr: 0.049955065418655915\nepoch: 1900, acc: 0.943, loss: 0.155, lr: 0.04995257003475201\nepoch: 2000, acc: 0.947, loss: 0.151, lr: 0.04995007490013731\nepoch: 2100, acc: 0.943, loss: 0.148, lr: 0.0499475800147745\nepoch: 2200, acc: 0.953, loss: 0.145, lr: 0.0499450853786262\nepoch: 2300, acc: 0.957, loss: 0.142, lr: 0.0499425909916551\nepoch: 2400, acc: 0.957, loss: 0.138, lr: 0.04994009685382384\nepoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\nepoch: 2600, acc: 0.960, loss: 0.132, lr: 0.049935109325431604\nepoch: 2700, acc: 0.957, loss: 0.130, lr: 0.049932615934796004\nepoch: 2800, acc: 0.937, loss: 0.159, lr: 0.04993012279315098\nepoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\nepoch: 3000, acc: 0.960, loss: 0.123, lr: 0.049925137256683606\nepoch: 3100, acc: 0.960, loss: 0.121, lr: 0.04992264486178666\nepoch: 3200, acc: 0.960, loss: 0.119, lr: 0.04992015271573119\nepoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\nepoch: 3400, acc: 0.960, loss: 0.115, lr: 0.049915169169995596\nepoch: 3500, acc: 0.950, loss: 0.131, lr: 0.049912677770240964\nepoch: 3600, acc: 0.957, loss: 0.118, lr: 0.049910186619178794\nepoch: 3700, acc: 0.960, loss: 0.113, lr: 0.04990769571677183\nepoch: 3800, acc: 0.960, loss: 0.111, lr: 0.04990520506298287\nepoch: 3900, acc: 0.960, loss: 0.110, lr: 0.04990271465777467\nepoch: 4000, acc: 0.960, loss: 0.109, lr: 0.049900224501110035\nepoch: 4100, acc: 0.960, loss: 0.107, lr: 0.04989773459295174\nepoch: 4200, acc: 0.960, loss: 0.106, lr: 0.04989524493326262\nepoch: 4300, acc: 0.960, loss: 0.104, lr: 0.04989275552200545\nepoch: 4400, acc: 0.960, loss: 0.103, lr: 0.04989026635914307\nepoch: 4500, acc: 0.960, loss: 0.102, lr: 0.04988777744463829\nepoch: 4600, acc: 0.960, loss: 0.101, lr: 0.049885288778453954\nepoch: 4700, acc: 0.960, loss: 0.100, lr: 0.049882800360552884\nepoch: 4800, acc: 0.960, loss: 0.099, lr: 0.04988031219089794\nepoch: 4900, acc: 0.960, loss: 0.098, lr: 0.049877824269451976\nepoch: 5000, acc: 0.960, loss: 0.098, lr: 0.04987533659617785\nepoch: 5100, acc: 0.967, loss: 0.107, lr: 0.04987284917103844\nepoch: 5200, acc: 0.960, loss: 0.097, lr: 0.04987036199399661\nepoch: 5300, acc: 0.960, loss: 0.096, lr: 0.04986787506501525\nepoch: 5400, acc: 0.960, loss: 0.095, lr: 0.04986538838405724\nepoch: 5500, acc: 0.960, loss: 0.094, lr: 0.049862901951085496\nepoch: 5600, acc: 0.960, loss: 0.094, lr: 0.049860415766062906\nepoch: 5700, acc: 0.960, loss: 0.093, lr: 0.0498579298289524\nepoch: 5800, acc: 0.960, loss: 0.092, lr: 0.04985544413971689\nepoch: 5900, acc: 0.960, loss: 0.092, lr: 0.049852958698319315\nepoch: 6000, acc: 0.960, loss: 0.091, lr: 0.04985047350472258\nepoch: 6100, acc: 0.960, loss: 0.090, lr: 0.04984798855888967\nepoch: 6200, acc: 0.960, loss: 0.089, lr: 0.049845503860783506\nepoch: 6300, acc: 0.960, loss: 0.089, lr: 0.049843019410367055\nepoch: 6400, acc: 0.960, loss: 0.088, lr: 0.04984053520760327\nepoch: 6500, acc: 0.953, loss: 0.126, lr: 0.049838051252455155\nepoch: 6600, acc: 0.960, loss: 0.090, lr: 0.049835567544885655\nepoch: 6700, acc: 0.960, loss: 0.088, lr: 0.04983308408485778\nepoch: 6800, acc: 0.960, loss: 0.088, lr: 0.0498306008723345\nepoch: 6900, acc: 0.960, loss: 0.087, lr: 0.04982811790727884\nepoch: 7000, acc: 0.960, loss: 0.086, lr: 0.04982563518965381\nepoch: 7100, acc: 0.960, loss: 0.086, lr: 0.049823152719422406\nepoch: 7200, acc: 0.960, loss: 0.085, lr: 0.049820670496547675\nepoch: 7300, acc: 0.960, loss: 0.085, lr: 0.04981818852099264\nepoch: 7400, acc: 0.960, loss: 0.084, lr: 0.049815706792720335\nepoch: 7500, acc: 0.960, loss: 0.083, lr: 0.0498132253116938\nepoch: 7600, acc: 0.960, loss: 0.083, lr: 0.04981074407787611\nepoch: 7700, acc: 0.960, loss: 0.082, lr: 0.049808263091230306\nepoch: 7800, acc: 0.960, loss: 0.082, lr: 0.04980578235171948\nepoch: 7900, acc: 0.963, loss: 0.081, lr: 0.04980330185930667\nepoch: 8000, acc: 0.963, loss: 0.081, lr: 0.04980082161395499\nepoch: 8100, acc: 0.963, loss: 0.080, lr: 0.04979834161562752\nepoch: 8200, acc: 0.963, loss: 0.084, lr: 0.04979586186428736\nepoch: 8300, acc: 0.963, loss: 0.080, lr: 0.04979338235989761\nepoch: 8400, acc: 0.963, loss: 0.079, lr: 0.04979090310242139\nepoch: 8500, acc: 0.963, loss: 0.079, lr: 0.049788424091821805\nepoch: 8600, acc: 0.963, loss: 0.078, lr: 0.049785945328062006\nepoch: 8700, acc: 0.963, loss: 0.078, lr: 0.0497834668111051\nepoch: 8800, acc: 0.963, loss: 0.077, lr: 0.049780988540914256\nepoch: 8900, acc: 0.963, loss: 0.077, lr: 0.0497785105174526\nepoch: 9000, acc: 0.963, loss: 0.076, lr: 0.04977603274068329\nepoch: 9100, acc: 0.963, loss: 0.076, lr: 0.04977355521056952\nepoch: 9200, acc: 0.963, loss: 0.075, lr: 0.049771077927074414\nepoch: 9300, acc: 0.963, loss: 0.075, lr: 0.0497686008901612\nepoch: 9400, acc: 0.707, loss: 1.982, lr: 0.04976612409979302\nepoch: 9500, acc: 0.963, loss: 0.079, lr: 0.0497636475559331\nepoch: 9600, acc: 0.963, loss: 0.076, lr: 0.049761171258544616\nepoch: 9700, acc: 0.963, loss: 0.076, lr: 0.0497586952075908\nepoch: 9800, acc: 0.963, loss: 0.075, lr: 0.04975621940303483\nepoch: 9900, acc: 0.963, loss: 0.074, lr: 0.049753743844839965\nepoch: 10000, acc: 0.967, loss: 0.074, lr: 0.04975126853296942\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Assuming you have access to the weights and biases of your trained model\n# For the weights and biases of dense1 and dense2 layers\nW1, b1 = dense1.weights, dense1.biases\nW2, b2 = dense2.weights, dense2.biases\n\n# Create a meshgrid of points covering the feature space\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Flatten the meshgrid points and apply the first dense layer and ReLU activation\npoints = np.c_[xx.ravel(), yy.ravel()]\nz1 = np.dot(points, W1) + b1\na1 = np.maximum(0, z1)\n\n# Apply the second dense layer\nz2 = np.dot(a1, W2) + b2\n\n# Apply softmax activation to get probabilities\nexp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n# Predictions\npredictions = np.argmax(probs, axis=1)\nZ = predictions.reshape(xx.shape)\n\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\nThat was training a full neural network from scratch in Numpy (phew!). Although this was meticulous, it should give you an appreciation of the amount of work done under the hood in deep learning packages like PyTorch. And although this was not an extensive overview of all aspects of the anatomy of a neural network architecture and training, you (hopefully) have a slightly better understanding of some of the key introductory concepts. When ready, also make sure to check out a reimplementation of the same architecture in PyTorch in WS4_IntroToNNs_PyTorch.ipynb."
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#a-comment-on-batches",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#a-comment-on-batches",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "Batches allow calculating things in parallel. The bigger the batch, the more parallel operations we can run. This is also why we tend to do neural network training on GPUs which have 100s-1000s of cores that we can run calculations on (rather than 4-8 cores on CPUs). Batches also help with generalization. Exposing our network with multiple datapoints at a time, rather than one at a time or all points at once, is widely understood to help a neural network to generalize. Generally we will work with batch sizes of 32, 64, maybe 128"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#defining-the-model",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#defining-the-model",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "Below is a slew of classes we will be using to create our model. You are not expected to to understand every class, but skim through the comments to see the general purpose of each.\n\n\nCode\n# Dense layer\nclass Layer_Dense:\n  \"\"\"\n  Dense layer of a neural network\n  Facilitates:\n  - Forward propogation of data throught layer\n  - Backward propogation of gradients during training\n  \"\"\"\n\n  # Layer initialization\n  def __init__(self, n_inputs, n_neurons):\n    # Initialize weights and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1, n_neurons))\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs, weights and biases\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Gradients on parameters\n    self.dweights = np.dot(self.inputs.T, dvalues)\n    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n    # Gradient on values\n    self.dinputs = np.dot(dvalues, self.weights.T)\n\n\n# ReLU activation\nclass Activation_ReLU:\n  \"\"\"\n  Rectified linear unit activation function\n  Applied to input of neural network layer\n  Introduces non-linearity into the network\n  \"\"\"\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs\n    self.output = np.maximum(0, inputs)\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Since we need to modify original variable,\n    # let’s make a copy of values first\n    self.dinputs = dvalues.copy()\n    # Zero gradient where input values were negative\n    self.dinputs[self.inputs &lt;= 0] = 0\n\n\n# Softmax classifier - combined Softmax activation\n# and cross-entropy loss for faster backward step\nclass Activation_Softmax_Loss_CategoricalCrossentropy():\n  \"\"\"\n  Combination of softmax activation function and categorical cross entropy loss function\n  Commonly used in classification tasks\n  We minimize loss by adjustng model parameters to improve performance\n  \"\"\"\n\n  # create activation and loss function objectives\n  def __init__(self):\n    self.activation = Activation_Softmax()\n    self.loss = Loss_CategoricalCrossentropy()\n\n  # forward pass\n  def forward(self, inputs, y_true):\n    # output layer's activation function\n    self.activation.forward(inputs)\n    # set the output\n    self.output = self.activation.output\n    # calculate and return loss value\n    return self.loss.calculate(self.output, y_true)\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n\n    # if labels one-hot encoded, turn into discrete values\n    if len(y_true.shape) == 2:\n      y_true = np.argmax(y_true, axis=1)\n\n    # copy so we can safely modify\n    self.dinputs = dvalues.copy()\n    # Calculate gradient\n    self.dinputs[range(samples), y_true] -= 1\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n# Adam optimizer\nclass Optimizer_Adam:\n\n  \"\"\"\n  Adam optimization algorithm to optimize parameters of neural network\n  Initalize with learning rate, decay, epsilon, momentum\n  Pre-update params: Adjust learning rate based on decay\n  Update params: Update params using momentum and cache corrections\n  Post-update params: Track number of optimization steps performed\n  \"\"\"\n\n  # Initialize optimizer - set settings\n  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n    self.learning_rate = learning_rate\n    self.current_learning_rate = learning_rate\n    self.decay = decay\n    self.iterations = 0\n    self.epsilon = epsilon\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n\n  # Call once before any parameter updates\n  def pre_update_params(self):\n    if self.decay:\n      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n\n  # Update parameters\n  def update_params(self, layer):\n\n    # If layer does not contain cache arrays, create them filled with zeros\n    if not hasattr(layer, 'weight_cache'):\n      layer.weight_momentums = np.zeros_like(layer.weights)\n      layer.weight_cache = np.zeros_like(layer.weights)\n      layer.bias_momentums = np.zeros_like(layer.biases)\n      layer.bias_cache = np.zeros_like(layer.biases)\n\n    # Update momentum with current gradients\n    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n\n    # Get corrected momentum\n    # self.iteration is 0 at first pass\n    # and we need to start with 1 here\n    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n\n    # update cache with squared current gradients\n    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n\n    # get corrected cache\n    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n\n    # Vanilla SGD parameter update + normalization with square root cache\n    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n\n  # call once after any parameter updates\n  def post_update_params(self):\n    self.iterations += 1\n\n\n# Softmax activation\nclass Activation_Softmax:\n\n  \"\"\"\n  Softmax activation function for multi-class classification\n  Compute probabilities for each class\n  \"\"\"\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Get unnormalized probabilities\n    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n\n    # Normalize them for each sample\n    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\n    self.output = probabilities\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Create uninitialized array\n    self.dinputs = np.empty_like(dvalues)\n\n    # Enumerate outputs and gradients\n    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n      # Flatten output array\n      single_output = single_output.reshape(-1, 1)\n\n      # Calculate Jacobian matrix of the output\n      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n\n      # Calculate sample-wise gradient and add it to the array of sample gradients\n      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n\n\n# Common loss class\nclass Loss:\n\n  # calculates data and regularization losses, given model output and ground truth values\n  def calculate(self, output, y):\n\n    # calculate sample losses\n    sample_losses = self.forward(output, y)\n\n    # calculate mean losses\n    data_loss = np.mean(sample_losses)\n\n    # return loss\n    return data_loss\n\n\n# cross entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n  \"\"\"\n  Computes categorical cross entropy\n  Quantifies discrepency between predicted and true class probabilities\n  \"\"\"\n\n  # forward pass\n  def forward(self, y_pred, y_true):\n\n    # number samples in batch\n    samples = len(y_pred)\n\n    # clip data to prevent division by 0\n    # clip both sides to not drag mean towards any value\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n    # probabilities for target values (only if categorical labels)\n    if len(y_true.shape) == 1:\n      correct_confidences = y_pred_clipped[ range(samples), y_true ]\n\n    # mask values (only for one-hot encoded labels)\n    elif len(y_true.shape) == 2:\n      correct_confidences = np.sum( y_pred_clipped * y_true, axis=1 )\n\n    # losses\n    negative_log_likelihoods = -np.log(correct_confidences)\n    return negative_log_likelihoods\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n    # Number of labels in every sample\n    # We’ll use the first sample to count them\n    labels = len(dvalues[0])\n\n    if len(y_true.shape) == 1:\n      y_true = np.eye(labels)[y_true]\n\n    # calculate gradient\n    self.dinputs = -y_true / dvalues\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#creating-dataset",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#creating-dataset",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "We will be training a neural network to predict the color of a point (y \\(ϵ\\) \\(\\{\\)green, red, or blue \\(\\}\\) ) given an input \\(X\\) of two descripitive features (the x and y coordinates)\n\n\nCode\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# an example feature set (x,y) and it's label (color)\nX[5], y[5]"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#instantiating-model",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#instantiating-model",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "We will be using a feedforward neural network consisting of two dense (fully connected) layers with ReLU activation, where the first layer takes 2 input features and outputs 64 neurons, and the second layer takes 64 neurons and outputs 3 neurons.\n\n\n\nimage.png\n\n\n\n\nCode\n# Create Dense layer with 2 input features and 64 output values\ndense1 = Layer_Dense(2, 64)\n\n# Create ReLU activation (to be used with Dense layer):\nactivation1 = Activation_ReLU()\n\n# Create second Dense layer with 64 input features (as we take output\n# of previous layer here) and 3 output values (output values)\ndense2 = Layer_Dense(64, 3)\n\n# Create Softmax classifier’s combined loss and activation\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n\n# Create optimizer\noptimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#training",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#training",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "Code\n# Train in loop\nfor epoch in range(10001):\n  # Perform a forward pass of our training data through this layer\n  dense1.forward(X)\n\n  # Perform a forward pass through activation function\n  # takes the output of first dense layer here\n  activation1.forward(dense1.output)\n\n  # Perform a forward pass through second Dense layer\n  # takes outputs of activation function of first layer as inputs\n  dense2.forward(activation1.output)\n\n  # Perform a forward pass through the activation/loss function\n  # takes the output of second dense layer here and returns loss\n  loss = loss_activation.forward(dense2.output, y)\n\n  # Calculate accuracy from output of activation2 and targets\n  # calculate values along first axis\n  predictions = np.argmax(loss_activation.output, axis=1)\n\n  if len(y.shape) == 2:\n    y = np.argmax(y, axis=1)\n\n  accuracy = np.mean(predictions==y)\n\n  if not epoch % 100:\n    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n\n  # backward pass\n  loss_activation.backward(loss_activation.output, y)\n  dense2.backward(loss_activation.dinputs)\n  activation1.backward(dense2.dinputs)\n  dense1.backward(activation1.dinputs)\n\n  # update weights and biases\n  optimizer.pre_update_params()\n  optimizer.update_params(dense1)\n  optimizer.update_params(dense2)\n  optimizer.post_update_params()\n\n\n\nepoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.670, loss: 0.705, lr: 0.04999752512250644\nepoch: 200, acc: 0.797, loss: 0.522, lr: 0.04999502549496326\nepoch: 300, acc: 0.847, loss: 0.430, lr: 0.049992526117345455\nepoch: 400, acc: 0.887, loss: 0.344, lr: 0.04999002698961558\nepoch: 500, acc: 0.910, loss: 0.303, lr: 0.049987528111736124\nepoch: 600, acc: 0.907, loss: 0.276, lr: 0.049985029483669646\nepoch: 700, acc: 0.917, loss: 0.252, lr: 0.049982531105378675\nepoch: 800, acc: 0.920, loss: 0.245, lr: 0.04998003297682575\nepoch: 900, acc: 0.930, loss: 0.228, lr: 0.049977535097973466\nepoch: 1000, acc: 0.940, loss: 0.217, lr: 0.049975037468784345\nepoch: 1100, acc: 0.937, loss: 0.205, lr: 0.049972540089220974\nepoch: 1200, acc: 0.947, loss: 0.192, lr: 0.04997004295924593\nepoch: 1300, acc: 0.947, loss: 0.184, lr: 0.04996754607882181\nepoch: 1400, acc: 0.943, loss: 0.183, lr: 0.049965049447911185\nepoch: 1500, acc: 0.943, loss: 0.189, lr: 0.04996255306647668\nepoch: 1600, acc: 0.943, loss: 0.165, lr: 0.049960056934480884\nepoch: 1700, acc: 0.943, loss: 0.161, lr: 0.04995756105188642\nepoch: 1800, acc: 0.943, loss: 0.158, lr: 0.049955065418655915\nepoch: 1900, acc: 0.943, loss: 0.155, lr: 0.04995257003475201\nepoch: 2000, acc: 0.947, loss: 0.151, lr: 0.04995007490013731\nepoch: 2100, acc: 0.943, loss: 0.148, lr: 0.0499475800147745\nepoch: 2200, acc: 0.953, loss: 0.145, lr: 0.0499450853786262\nepoch: 2300, acc: 0.957, loss: 0.142, lr: 0.0499425909916551\nepoch: 2400, acc: 0.957, loss: 0.138, lr: 0.04994009685382384\nepoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\nepoch: 2600, acc: 0.960, loss: 0.132, lr: 0.049935109325431604\nepoch: 2700, acc: 0.957, loss: 0.130, lr: 0.049932615934796004\nepoch: 2800, acc: 0.937, loss: 0.159, lr: 0.04993012279315098\nepoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\nepoch: 3000, acc: 0.960, loss: 0.123, lr: 0.049925137256683606\nepoch: 3100, acc: 0.960, loss: 0.121, lr: 0.04992264486178666\nepoch: 3200, acc: 0.960, loss: 0.119, lr: 0.04992015271573119\nepoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\nepoch: 3400, acc: 0.960, loss: 0.115, lr: 0.049915169169995596\nepoch: 3500, acc: 0.950, loss: 0.131, lr: 0.049912677770240964\nepoch: 3600, acc: 0.957, loss: 0.118, lr: 0.049910186619178794\nepoch: 3700, acc: 0.960, loss: 0.113, lr: 0.04990769571677183\nepoch: 3800, acc: 0.960, loss: 0.111, lr: 0.04990520506298287\nepoch: 3900, acc: 0.960, loss: 0.110, lr: 0.04990271465777467\nepoch: 4000, acc: 0.960, loss: 0.109, lr: 0.049900224501110035\nepoch: 4100, acc: 0.960, loss: 0.107, lr: 0.04989773459295174\nepoch: 4200, acc: 0.960, loss: 0.106, lr: 0.04989524493326262\nepoch: 4300, acc: 0.960, loss: 0.104, lr: 0.04989275552200545\nepoch: 4400, acc: 0.960, loss: 0.103, lr: 0.04989026635914307\nepoch: 4500, acc: 0.960, loss: 0.102, lr: 0.04988777744463829\nepoch: 4600, acc: 0.960, loss: 0.101, lr: 0.049885288778453954\nepoch: 4700, acc: 0.960, loss: 0.100, lr: 0.049882800360552884\nepoch: 4800, acc: 0.960, loss: 0.099, lr: 0.04988031219089794\nepoch: 4900, acc: 0.960, loss: 0.098, lr: 0.049877824269451976\nepoch: 5000, acc: 0.960, loss: 0.098, lr: 0.04987533659617785\nepoch: 5100, acc: 0.967, loss: 0.107, lr: 0.04987284917103844\nepoch: 5200, acc: 0.960, loss: 0.097, lr: 0.04987036199399661\nepoch: 5300, acc: 0.960, loss: 0.096, lr: 0.04986787506501525\nepoch: 5400, acc: 0.960, loss: 0.095, lr: 0.04986538838405724\nepoch: 5500, acc: 0.960, loss: 0.094, lr: 0.049862901951085496\nepoch: 5600, acc: 0.960, loss: 0.094, lr: 0.049860415766062906\nepoch: 5700, acc: 0.960, loss: 0.093, lr: 0.0498579298289524\nepoch: 5800, acc: 0.960, loss: 0.092, lr: 0.04985544413971689\nepoch: 5900, acc: 0.960, loss: 0.092, lr: 0.049852958698319315\nepoch: 6000, acc: 0.960, loss: 0.091, lr: 0.04985047350472258\nepoch: 6100, acc: 0.960, loss: 0.090, lr: 0.04984798855888967\nepoch: 6200, acc: 0.960, loss: 0.089, lr: 0.049845503860783506\nepoch: 6300, acc: 0.960, loss: 0.089, lr: 0.049843019410367055\nepoch: 6400, acc: 0.960, loss: 0.088, lr: 0.04984053520760327\nepoch: 6500, acc: 0.953, loss: 0.126, lr: 0.049838051252455155\nepoch: 6600, acc: 0.960, loss: 0.090, lr: 0.049835567544885655\nepoch: 6700, acc: 0.960, loss: 0.088, lr: 0.04983308408485778\nepoch: 6800, acc: 0.960, loss: 0.088, lr: 0.0498306008723345\nepoch: 6900, acc: 0.960, loss: 0.087, lr: 0.04982811790727884\nepoch: 7000, acc: 0.960, loss: 0.086, lr: 0.04982563518965381\nepoch: 7100, acc: 0.960, loss: 0.086, lr: 0.049823152719422406\nepoch: 7200, acc: 0.960, loss: 0.085, lr: 0.049820670496547675\nepoch: 7300, acc: 0.960, loss: 0.085, lr: 0.04981818852099264\nepoch: 7400, acc: 0.960, loss: 0.084, lr: 0.049815706792720335\nepoch: 7500, acc: 0.960, loss: 0.083, lr: 0.0498132253116938\nepoch: 7600, acc: 0.960, loss: 0.083, lr: 0.04981074407787611\nepoch: 7700, acc: 0.960, loss: 0.082, lr: 0.049808263091230306\nepoch: 7800, acc: 0.960, loss: 0.082, lr: 0.04980578235171948\nepoch: 7900, acc: 0.963, loss: 0.081, lr: 0.04980330185930667\nepoch: 8000, acc: 0.963, loss: 0.081, lr: 0.04980082161395499\nepoch: 8100, acc: 0.963, loss: 0.080, lr: 0.04979834161562752\nepoch: 8200, acc: 0.963, loss: 0.084, lr: 0.04979586186428736\nepoch: 8300, acc: 0.963, loss: 0.080, lr: 0.04979338235989761\nepoch: 8400, acc: 0.963, loss: 0.079, lr: 0.04979090310242139\nepoch: 8500, acc: 0.963, loss: 0.079, lr: 0.049788424091821805\nepoch: 8600, acc: 0.963, loss: 0.078, lr: 0.049785945328062006\nepoch: 8700, acc: 0.963, loss: 0.078, lr: 0.0497834668111051\nepoch: 8800, acc: 0.963, loss: 0.077, lr: 0.049780988540914256\nepoch: 8900, acc: 0.963, loss: 0.077, lr: 0.0497785105174526\nepoch: 9000, acc: 0.963, loss: 0.076, lr: 0.04977603274068329\nepoch: 9100, acc: 0.963, loss: 0.076, lr: 0.04977355521056952\nepoch: 9200, acc: 0.963, loss: 0.075, lr: 0.049771077927074414\nepoch: 9300, acc: 0.963, loss: 0.075, lr: 0.0497686008901612\nepoch: 9400, acc: 0.707, loss: 1.982, lr: 0.04976612409979302\nepoch: 9500, acc: 0.963, loss: 0.079, lr: 0.0497636475559331\nepoch: 9600, acc: 0.963, loss: 0.076, lr: 0.049761171258544616\nepoch: 9700, acc: 0.963, loss: 0.076, lr: 0.0497586952075908\nepoch: 9800, acc: 0.963, loss: 0.075, lr: 0.04975621940303483\nepoch: 9900, acc: 0.963, loss: 0.074, lr: 0.049753743844839965\nepoch: 10000, acc: 0.967, loss: 0.074, lr: 0.04975126853296942"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#visualizing-predictions",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#visualizing-predictions",
    "title": "  Section 1: Introduction and coding our first neuron",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\n\n# Assuming you have access to the weights and biases of your trained model\n# For the weights and biases of dense1 and dense2 layers\nW1, b1 = dense1.weights, dense1.biases\nW2, b2 = dense2.weights, dense2.biases\n\n# Create a meshgrid of points covering the feature space\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Flatten the meshgrid points and apply the first dense layer and ReLU activation\npoints = np.c_[xx.ravel(), yy.ravel()]\nz1 = np.dot(points, W1) + b1\na1 = np.maximum(0, z1)\n\n# Apply the second dense layer\nz2 = np.dot(a1, W2) + b2\n\n# Apply softmax activation to get probabilities\nexp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n# Predictions\npredictions = np.argmax(probs, axis=1)\nZ = predictions.reshape(xx.shape)\n\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\nThat was training a full neural network from scratch in Numpy (phew!). Although this was meticulous, it should give you an appreciation of the amount of work done under the hood in deep learning packages like PyTorch. And although this was not an extensive overview of all aspects of the anatomy of a neural network architecture and training, you (hopefully) have a slightly better understanding of some of the key introductory concepts. When ready, also make sure to check out a reimplementation of the same architecture in PyTorch in WS4_IntroToNNs_PyTorch.ipynb."
  }
]