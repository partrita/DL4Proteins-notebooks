[
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html",
    "href": "notebooks/WS06_IntroductionToAF.html",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "##홈페이지로 돌아가기\nAlphaFold2의 구현은 ColabFold에서 각색되었습니다.\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\n최신 단백질 접힘 및 멀티트랙 아키텍처 식별 및 적용\nAF2의 ColabFold 구현 실행\npLDDT 점수, coverage, pae 점수를 포함한 AF 출력 해석\nAlphaFold의 다양한 단백질 표현 구별\nAF2의 기본 아키텍처 구성 요소 열거\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n\n\n\n\n\n소개: 딥러닝의 등장!\n소개: 마법 상자로서의 Alphafold\n\n\n\n\n\n실용적인 Alphafold: 서열 접기\n실용적인 Alphafold: 결과 해석\n다중 서열 정렬(MSA) Coverage\npLDDT\n예측 정렬 오류(PAE)\n\n\n\n\n\nAlphafold 아키텍처: 입력 서열 표현\nAlphafold 아키텍처: Evoformer\nAlphafold 아키텍처: 구조 모듈\n\n\n\n\n\n\nCode\n#@markdown **Please run this cell as you read the introduction**\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport os\n\ncustom_template_path = None\nuse_templates = False\n\n\nUSE_AMBER = False\nUSE_TEMPLATES = use_templates\nPYTHON_VERSION = python_version\n\nif not os.path.isfile(\"COLABFOLD_READY\"):\n  print(\"installing colabfold...\")\n  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n  if os.environ.get('TPU_NAME', False) != False:\n    os.system(\"pip uninstall -y jax jaxlib\")\n    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n  os.system(\"touch COLABFOLD_READY\")\n\nif USE_AMBER or USE_TEMPLATES:\n  if not os.path.isfile(\"CONDA_READY\"):\n    print(\"installing conda...\")\n    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\")\n    os.system(\"bash Mambaforge-Linux-x86_64.sh -bfp /usr/local\")\n    os.system(\"mamba config --set auto_update_conda false\")\n    os.system(\"touch CONDA_READY\")\n\nif USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n  print(\"installing hhsuite and amber...\")\n  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n  os.system(\"touch HH_READY\")\n  os.system(\"touch AMBER_READY\")\nelse:\n  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n    print(\"installing hhsuite...\")\n    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n    os.system(\"touch HH_READY\")\n  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n    print(\"installing amber...\")\n    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n    os.system(\"touch AMBER_READY\")\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\njobname = 'import_test'\nquery_sequence = 'A'\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\nmodel_type = \"auto\"\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\ndownload_alphafold_params(model_type, Path(\".\"))\n\n\ninstalling colabfold...\n\n\nDownloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [02:01&lt;00:00, 30.6MB/s]\n\n\n\n\n\n\n\n단백질의 서열을 안다면 어떤 3D 구조로 접힐까요? 이 단백질 접힘 문제는 생물물리학자, 생물학자, 단백질 공학자들의 오랜 꿈이었습니다. 이를 해결하는 것은 연구 및 약물 설계를 위한 많은 새로운 발견과 혁신의 열쇠를 쥐고 있습니다!\n\n\n\n\n\n하지만 단백질 접힘 뒤에 있는 개별 물리적 개념의 대부분을 이해했음에도 불구하고 단백질이 무엇으로 접힐지 확실하게 예측할 수 있는 방정식 레시피는 없었습니다.\n3D 실험 단백질 구조의 예가 증가함에 따라 데이터셋은 결국 신경망이 단백질이 접히는 방식을 예측하도록 훈련할 수 있을 만큼 커졌습니다.\n\n\n\n\n\n가장 유명한 것 중 하나가 AlphaFold입니다! 매년 열리는 단백질 접힘 대회인 CASP에서 모든 경쟁자를 압도적인 차이로 능가하며 단백질 접힘 혁명을 일으켰습니다.\n\n\n힌트: 이전 점수는 약 30-40 Global Distance Score(100점 만점)였습니다.\n\n\nCode\n# @markdown Take a guess!\nAF1 = 70 # @param {type:\"slider\", min:0, max:100, step:10}\nAF2 = 50 # @param {type:\"slider\", min:0, max:100, step:10}\n\nyear = [2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020]\ncasp_scores = [35, 41, 34, 30, 31, 41, 57, 88]\n\n# make bar chart\nfig = plt.figure(figsize=(10, 5))\nplt.bar(year, casp_scores)\nplt.xlabel('Year')\nplt.ylabel('CASP Score')\n\n# add prediction as scatter\nplt.scatter(2018, AF1, label='AF1', color='purple')\nplt.scatter(2020, AF2, label='AF2', color='magenta')\nplt.text(2018, AF1, f'{AF1}', ha='right')\nplt.text(2020, AF2, f'{AF2}', ha='right')\nplt.title('CASP Median Free Modeling Accuracy')\n\n# Set y axis to 0-100\nplt.ylim(0, 100)\n\n\nplt.show()\n\n\n\n\n\n\nAlphaFold는 입력으로 서열과 몇 가지 선택적인 다른 정보를 사용합니다:\n\nMSAs – 다중 서열 정렬, 데이터베이스에서 추론된 유전적으로 유사한 아미노산 서열.\nPair Features – 가능한 잔기 상호 작용의 쌍별 특징 표현.\nTemplates – 알려지지 않은 단백질과 구조가 유사할 수 있는 템플릿 단백질. 이들은 모델의 주요 모듈에 들어가기 전에 쌍 특징과 결합됩니다.\n\n간단히 말해서, 이 세 가지 입력이 모델에 공급되면 모델은 단백질과 AF가 구조가 얼마나 그럴듯하다고 생각하는지에 대한 몇 가지 지표를 뱉어냅니다. 자세한 내용은 AF 아키텍처: AF는 어떻게 작동합니까? 하위 섹션을 참조하세요.\n\n\n\nScreenshot 2024-06-26 000206.png\n\n\n\n\n\n\n\n바로 들어가 봅시다! 단백질 서열이 있습니다. AlphaFold로 구조를 어떻게 찾을까요?\n여기서는 AF2의 ColabFold 구현(Mirdita et al. 2022)을 사용할 것입니다. 일반적으로 MSA 정렬을 제공해야 하지만 ColabFold는 MMseqs2를 사용하여 자동으로 수행합니다. 아래 셀을 실행하고 서열(및 선택적으로 템플릿)을 제공하기만 하면 짠, 단백질이 접힙니다!\n참고: 드노보(de novo) 단백질을 설계할 때는 MSA를 끄는 것이 좋습니다. 드노보 단백질은 일반적으로 천연 단백질과 구조적 유사성이 높지 않기 때문입니다.\n각색된 내용입니다. 더 고급 용도의 경우 ColabFold Github 페이지를 방문하여 더 많은 옵션과 매개변수가 있는 노트북을 찾으세요. 이 노트북은 주로 교육 목적으로 제작되었습니다!\n\n\n\n\nCode\n#@markdown Input your favorite protein sequence, then RUN! (Note: Make sure you are connected to a GPU instance. Should take around 2-5 minutes)\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\nquery_sequence = 'MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH' #@param {type:\"string\"}\njobname = 'learning_af'\nnum_relax = 0\ntemplate_mode = \"none\"\n\nuse_amber = num_relax &gt; 0\n\n# remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nif template_mode == \"pdb100\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n\nprint(\"jobname\",jobname)\nprint(\"sequence\",query_sequence)\nprint(\"length\",len(query_sequence.replace(\":\",\"\")))\n\n\n\nimport os\n\nmsa_mode = \"mmseqs2_uniref_env\"\npair_mode = \"unpaired_paired\"\n\n# decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n\n\nmodel_type = \"auto\"\nnum_recycles = \"3\"\nrecycle_early_stop_tolerance = \"auto\"\nrelax_max_iterations = 200\npairing_strategy = \"greedy\"\n\n\n\nmax_msa = \"auto\"\nnum_seeds = 1\nuse_dropout = False\n\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\n\nsave_all = False\nsave_recycles = False\nsave_to_google_drive = False\ndpi = 200\nif save_to_google_drive:\n  from pydrive2.drive import GoogleDrive\n  from pydrive2.auth import GoogleAuth\n  from google.colab import auth\n  from oauth2client.client import GoogleCredentials\n  auth.authenticate_user()\n  gauth = GoogleAuth()\n  gauth.credentials = GoogleCredentials.get_application_default()\n  drive = GoogleDrive(gauth)\n  print(\"You are logged into Google Drive and are good to go!\")\n\n\ndisplay_images = True\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n\n\ndisplay_images = False\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nlog_filename = os.path.join(jobname,\"log.txt\")\nsetup_logging(Path(log_filename))\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,\n    model_type=model_type,\n    num_models=2,\n    num_recycles=2,\n    relax_max_iterations=relax_max_iterations,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2], # edited\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    pairing_strategy=pairing_strategy,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n    user_agent=\"colabfold/google-colab-main\",\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n\n\njobname learning_af_e64d1\nsequence MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH\nlength 92\n2024-10-09 17:38:47,646 Running on GPU\n2024-10-09 17:38:48,066 Found 5 citations for tools or databases\n2024-10-09 17:38:48,067 Query 1/1: learning_af_e64d1 (length 92)\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:01 remaining: ?]\n\n\n2024-10-09 17:38:49,136 Sleeping for 5s. Reason: PENDING\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:07 remaining: ?]\n\n\n2024-10-09 17:38:55,066 Sleeping for 6s. Reason: PENDING\n\n\nRUNNING:   4%|▍         | 6/150 [elapsed: 00:13 remaining: 05:34]\n\n\n2024-10-09 17:39:02,012 Sleeping for 5s. Reason: RUNNING\n\n\nRUNNING:   7%|▋         | 11/150 [elapsed: 00:19 remaining: 03:56]\n\n\n2024-10-09 17:39:07,932 Sleeping for 8s. Reason: RUNNING\n\n\nRUNNING:  13%|█▎        | 19/150 [elapsed: 00:28 remaining: 03:01]\n\n\n2024-10-09 17:39:16,910 Sleeping for 7s. Reason: RUNNING\n\n\nRUNNING:  17%|█▋        | 26/150 [elapsed: 00:36 remaining: 02:38]\n\n\n2024-10-09 17:39:24,835 Sleeping for 8s. Reason: RUNNING\n\n\nCOMPLETE: 100%|██████████| 150/150 [elapsed: 00:47 remaining: 00:00]\n\n\n2024-10-09 17:39:36,982 Setting max_seq=512, max_extra_seq=5120\n2024-10-09 17:40:15,080 alphafold2_ptm_model_1_seed_000 recycle=0 pLDDT=80 pTM=0.667\n2024-10-09 17:40:21,715 alphafold2_ptm_model_1_seed_000 recycle=1 pLDDT=82.1 pTM=0.692 tol=0.583\n2024-10-09 17:40:28,410 alphafold2_ptm_model_1_seed_000 recycle=2 pLDDT=82.7 pTM=0.702 tol=0.111\n2024-10-09 17:40:28,412 alphafold2_ptm_model_1_seed_000 took 37.7s (2 recycles)\n2024-10-09 17:40:35,204 alphafold2_ptm_model_2_seed_000 recycle=0 pLDDT=84.9 pTM=0.742\n2024-10-09 17:40:42,062 alphafold2_ptm_model_2_seed_000 recycle=1 pLDDT=85.2 pTM=0.742 tol=0.834\n2024-10-09 17:40:48,939 alphafold2_ptm_model_2_seed_000 recycle=2 pLDDT=85.8 pTM=0.749 tol=0.14\n2024-10-09 17:40:48,940 alphafold2_ptm_model_2_seed_000 took 20.5s (2 recycles)\n2024-10-09 17:40:48,961 reranking models by 'plddt' metric\n2024-10-09 17:40:48,961 rank_001_alphafold2_ptm_model_2_seed_000 pLDDT=85.8 pTM=0.749\n2024-10-09 17:40:48,962 rank_002_alphafold2_ptm_model_1_seed_000 pLDDT=82.7 pTM=0.702\n2024-10-09 17:40:49,709 Done\n\n\n0\n\n\n\n\nCode\n## **&lt;font color='#569098'&gt;파트 2: 결과를 어떻게 해석할까요?**&lt;/font&gt;\n\n출력을 다운로드하면(실제 ColabFold 노트북에 편리하게 패키지되어 있거나 이 노트북의 파일에 덜 편리하게 있음) 관심 있는 여러 파일을 얻을 수 있습니다:\n\n1. &lt;font color='#e59454'&gt;\"순위가 매겨진(ranked)\" 3D pdb 모델&lt;/font&gt; 몇 개\n2. &lt;font color='#e59454'&gt;\"coverage\"&lt;/font&gt;를 자세히 설명하는 이미지\n3. &lt;font color='#e59454'&gt;\"pLDDT\"&lt;/font&gt;를 자세히 설명하는 이미지\n4. &lt;font color='#e59454'&gt;\"PAE\"&lt;/font&gt;를 자세히 설명하는 이미지\n\n\n\n각각이 무엇을 의미하는지 살펴봅시다!\n\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages.\n        \n\n\n\n\n\n\n\n\n\n\n\n\n“Coverage”는 꽤 간단합니다. AF가 서열 데이터베이스를 살펴볼 때 입력 서열의 부분에 대해 얼마나 많은 일치를 할당할 수 있었는지 보여줍니다.\n\n\n\n\n\nCode\n#@markdown Run to show the MSA coverage of your protein - Is there a lot or a little coverage? (hint: more colors = more coverage)\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\npLDDT는 AF가 예측에 대해 갖는 잔기별 신뢰도로 해석될 수 있습니다. pLDDT가 높을수록 예측이 더 정확합니다.\nDeepmind 웹사이트에 따르면,\n\n&gt;90 = 높은 정확도\n70-90 = 잘 모델링됨\n50-70 = 낮은 신뢰도\n&lt;50 = 아마도 무질서한(disordered) 영역 \n\n전체 pLDDT는 출력 순위를 매기는 데 사용됩니다(그리고 우리가 시각화한 구조는 pLDDT로 색칠됩니다 – 파란색이 더 높고 빨간색이 더 낮음).\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n### **&lt;font color='#569098'&gt;PAE - 예측 정렬 오류 (Predicted Aligned Error)**\n\n단백질 도메인 및 잔기 상호 작용에 대한 AF의 신뢰도를 나타냅니다. 모든 잔기 간의 접촉 맵(contact map)으로 제공됩니다.\n\n(x, y)의 색상은 예측과 실제 구조가 잔기 y에 정렬될 때 잔기 x의 위치에서 예상되는 거리 오류에 해당합니다.\n\n낮을수록 좋습니다:\n* 낮음 = 잘 정의된 위치/방향 (파란색)\n* 높음 = 불확실한 상대적 위치 (빨간색)\n\n예: 나노바디가 항원에 잘 결합하면 두 사슬 사이의 상호 작용 영역에서 더 많은 \"파란색\"을 볼 수 있습니다(예: 아래 질문 참조).\n\n\nWhich one has more coverage?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\n\nCode\n#@markdown Run to display your pLDDT score - Is it high confidence? Where might AF2 be less confident?\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n아래는 AlphaFold 실행 후 생성된 모든 파일 요약입니다.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which one is AF more confident in? Why?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Alphafold has more confidence in the right one! This is because the plddt is higher (there are less \"spikes\")')\n\n\nWhich one is AF more confident in? Why?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\n\nAF는 멀티트랙 아키텍처, 다중 서열 정렬, 엔드투엔드(end-to-end) 학습, 어텐션(attention), 자체 신뢰도 추정기 등 많은 혁신을 활용했습니다. 이 용어들을 이해하지 못해도 걱정하지 마세요. 자세히 알아보겠습니다!\n아래는 이전에 이야기했던 AF2 아키텍처의 세 가지 주요 구성 요소입니다:\n\n\nCode\n#@markdown Run to view PAE charts - take time to understand what the x and y axes mean!\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\npae = \"\"\npae_file = os.path.join(jobname,f\"{jobname}{jobname_prefix}_pae.png\")\nif os.path.isfile(pae_file):\n    pae = image_to_data_url(pae_file)\n\ndisplay(HTML(f\"\"\"\n&lt;div style=\"max-width:90%; padding:2em;\"&gt;\n  { '&lt;!--' if pae == '' else '' }&lt;img src=\"{pae}\" class=\"full\" /&gt;{ '--&gt;' if pae == '' else '' }\n&lt;/div&gt;\n\"\"\"))\n\n\n\n\n  \n\n\n\n\n\n\n\nAlphaFold 2에 입력되는 단백질 서열 정보는 fasta 파일로 저장됩니다. 다음은 아래의 예입니다:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD\n\n\nCode\n### &lt;font color='#569098'&gt;다중 서열 정렬 및 템플릿 구조 구축\n\n입력 아미노산 서열은 단백질 서열의 여러 데이터베이스(일반적으로 UniRef)에 대한 쿼리로 사용되어 *다중 서열 정렬*(MSA)을 구성합니다. MSA는 살아있는 유기체에서 확인된 유사한(그러나 동일하지는 않은) 서열을 식별합니다. 정렬에서 구조로 어떻게 갈까요? 이론은 함께 진화하는 잔기가 일반적으로 단백질의 접힌 상태에서 서로 가깝다는 것입니다.\n\n유사한 서열에서도 구조적 정보를 얻을 수 있습니다. 이러한 템플릿은 잔기 간의 거리를 결정하기 위해 거리 행렬로 변환될 수 있습니다.\n\n\n\nWhich quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nBelow is a summary of all the files produced after running AlphaFold\n여기서 우리는 또한 템플릿을 식별하려고 노력하는데, 이는 입력 서열과 유사한 구조를 가질 수 있는 단백질입니다. 이것은 쌍 표현(pair representation)이라고도 하는 구조의 초기 표현을 구성하는 데 사용됩니다. 보존된 조각을 식별함으로써 모델은 이를 구조 구성의 가이드로 사용합니다.\n\n\n\n\n여기서 MSA와 템플릿을 가져와 한 쌍의 트랜스포머에 통과시킵니다. 이렇게 하면 (1) 어떤 정보 조각이 더 유익한지 식별하고, (2) MSA와 쌍 표현 간에 정보를 반복적으로 교환하고, (3) MSA와 쌍 상호 작용 모두에 대한 표현을 정제합니다.\n아래는 Evoformer(MSA 트랜스포머와 쌍 트랜스포머로 구성됨)를 자세히 살펴본 것입니다. 왼쪽 이미지: MSA 트랜스포머는 각각 잔기에 해당하는 MSA의 두 열 사이의 상관 관계를 식별합니다. 이 정보는 쌍 표현으로 전달되며, 그 후 쌍 표현은 또 다른 가능한 상호 작용을 식별합니다. 오른쪽 이미지: 정보가 다시 MSA로 전달됩니다. MSA 트랜스포머는 쌍 표현으로부터 입력을 받고 유의미한 상관 관계를 나타내는 또 다른 열 쌍을 관찰합니다.\n\n\n\nimage.png\n\n\n트랜스포머 쌍은 두 변의 합이 세 번째 변보다 크거나 같아야 한다는 삼각형 부등식의 원리에 따라 작동합니다. 어텐션은 잔기의 삼각형 측면에서 배열됩니다. 이 정리를 사용하여 우리는 세 점 사이의 거리가 이 정리를 결코 깨뜨릴 수 없기 때문에 잔기가 서로 떨어져 있을 가능성이 있는 거리를 결정할 수 있습니다.\n\n\nProtein sequence information fed to AlphaFold 2 are stored as a fasta file. Here’s an example below:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD\n\n\n\n\n구조 모듈은 정제된 MSA 표현과 정제된 쌍 표현을 사용하여 구조의 3D 모델을 구성합니다. 최종 결과는 단백질의 각 원자(곁사슬 포함) 위치를 나타내는 직카 좌표의 긴 목록입니다. 단백질은 잔기 가스(residue gas)로 표현되는데, 모든 아미노산은 질소, R 그룹 탄소 및 알파 탄소에 점이 있는 삼각형으로 모델링됩니다. 구조 모듈의 시작 부분에서 모든 잔기는 좌표의 원점에 배치됩니다. 모든 단계에서 모델은 잔기를 공간에서 변위/회전시키는 일련의 행렬을 생성합니다. 이 표현은 물리적 또는 기하학적 가정을 반영하지 않으며 결과적으로 네트워크는 구조적 위반을 생성하는 경향이 있는데, 이는 이 비디오에서 볼 수 있습니다.\n\n\n\nimage.png\n\n\n\n\nCode\n중요한 점은 이 모델이 *반복적으로* 작동한다는 것입니다. 구조를 생성한 후 모델은 모든 정보(MSA 표현, 쌍 표현 및 예측된 구조)를 가져와 Evoformer 블록의 시작 부분으로 다시 전달합니다. 여러 반복에 걸쳐 블록 전체에서 예측을 정제하는 모델에 대한 [이 비디오](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM5_ESM.mp4)를 확인하세요.\n\n\n\n    \n      \n    \n        \n          \n          An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\n        \n        \n        \n          \n          An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\n        \n        \n        \n          \n          An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\n        \n        \n        \n          \n          An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\n\nAlphafold의 아키텍처와 개념은 RosettaTTAFold2 및 ESMFold와 같은 다른 딥러닝 기반 단백질 접힘 알고리즘에도 영감을 주었습니다. 각 모델은 장단점이 있으며, 이는 다양한 단백질 유형에 대한 정확도에 따라 달라지며, 종종 실행 시간(더 정확한 모델은 실행하는 데 더 오래 걸림)과 상충 관계에 있습니다. 드노보 단백질 설계이든 천연 복합체 조사이든 원하는 작업에 따라 특정 모델을 선택하는 것이 유리할 수 있습니다. 그러나 그것은 이 튜토리얼의 범위를 벗어납니다.\n또 다른 활발한 진전 분야는 리간드 및 번역 후 수정(post-translational modifications)을 포함한 접힘 예측입니다. 작성 시점을 기준으로 최신 기술에는 RosettaTTAFold All Atom, AF3, HelixFold3, Chai1이 포함됩니다. 이 모델들은 DNA 또는 억제제와 같은 리간드를 접힘 고려 대상에 포함할 수 있었습니다. 특히 HelixFold-single과 Chai1은 대신 단백질 언어 모델을 사용하여 MSA 사용을 우회하려고 합니다! 자유롭게 확인해 보세요!\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the concept of “pair representation” in AF2?\"\ncorrect_answer = \"Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\"\ndecoy_answers = [\"Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\",\n                 \"Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\",\n                 \"Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\n        \n        \n        \n          \n          Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\n        \n        \n        \n          \n          Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\n        \n        \n        \n          \n          Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\n\nHere, we take the MSA and templates and pass them through a pair of transformers. Doing so (1) identifies which pieces of information are more informative, (2) iteratively exchanges information between MSA and pair representations, and (3) refines the representations for both the MSA and pair interactions.\nBelow is a deeper look into the Evoformer (which is composed of an MSA transformer and pair transformer). Left image: The MSA transformer identifies a correlation between the two columns of the MSA, each corresponding to a residue. This information is passed to the pair representation, where subsequently the pair representation identifies another possible interaction. Right image: Information is passed back to the MSA. The MSA transformer receives an input from the pair representation and observes another pair of columns that exhibit a significant correlation.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the role of the Evoformer in the AF2 architecture?\"\ncorrect_answer = \"The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\"\ndecoy_answers = [\"The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\",\n                 \"The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\",\n                 \"The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\n        \n        \n        \n          \n          The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\n        \n        \n        \n          \n          The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\n        \n        \n        \n          \n          The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nThe pair of transformers works on the principle of Triangle inequality, where the sum of two sides must be greater than or equal to the third side. Attention is arranged in terms of triangles of residues. Using this theorem, we can determine the likely distance residues have from one another because the distance between three points can never break that theorem\n\n\n\nimage.png\n\n\n\n\n\nThe structure module takes a refined MSA representation and refined pair representation, and uses this to construct a 3D model of the structure. The end result is a long list of cartesian coordinates representing the position of each atom of the protein (including side chains). Proteins are represented as a residue gas, where every amino acid is modeled as a triangle with points at the Nitrogen, R group Carbon and the Alpha Carbon. At the beginning of the structure module, all residues are placed at the origin of coordinates. At every step, the model produces a set of matrices that displace/rotate the residues in space. This representation does not reflect any physical or geometric assumptions, and as a result the network has a tendency to generate structural violations, which you can see in this video\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"How does the structure module of AF2 generate the final protein structure?\"\ncorrect_answer = \"The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\"\ndecoy_answers = [\"The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\",\n                 \"The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\",\n                 \"The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\"]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\n        \n        \n        \n          \n          The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\n        \n        \n        \n          \n          The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\n        \n        \n        \n          \n          The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is invariant point attention (IPA) and why is it important in AF2?\"\ncorrect_answer = \"IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\"\ndecoy_answers = [\"IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\",\n                 \"IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\",\n                 \"IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\n        \n        \n        \n          \n          IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\n        \n        \n        \n          \n          IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\n        \n        \n        \n          \n          IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the Frame Aligned Point Error (FAPE) and how is it used in AF2?\"\ncorrect_answer = \"FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\"\ndecoy_answers = [\"FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\",\n                 \"FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\",\n                 \"FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\n        \n        \n        \n          \n          FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\n        \n        \n        \n          \n          FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\n        \n        \n        \n          \n          FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nAn important note is that this model works iteratively. After generating a structure, the model takes all the information (MSA representation, pair representation, and predicted structure) and passes it back to the beginning of the Evoformer blocks. Check this video out of the model refining it’s predictions across blocks over multiple iteractions.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\n# Define the question and answers\nquestion = \"How does AF2 refine its predictions iteratively?\"\ncorrect_answer = \"Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\"\ndecoy_answers = [\"AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\",\n                 \"AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\",\n                 \"AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\"]\n\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\n        \n        \n        \n          \n          AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\n        \n        \n        \n          \n          AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\n        \n        \n        \n          \n          Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\n\nAlphafold’s architecture and concepts have inspired other deep-learning based protein folding algorithms as well, such as RosettaTTAFold2 and ESMFold. Each model has its advantages and disadvantages, dependent on its accuracy (with respect to various protein types), which oftentimes trades off with runtime (more accurate models take longer to run). Based on your desired task, whether it be designing a de novo protein or investigating a natural complex, it may be beneficial to choose a particular model to use. However, that is beyond the scope of this tutorial.\nAnother active field of progress is predicting folding with ligands and post-translational modifications. At the time of writing, the state of the art includes RosettaTTAFold All Atom, AF3, HelixFold3, and Chai1. These models are been able to account for ligands in folding, such as DNA or inhibitors. In particular, HelixFold-single and Chai1 look to circumvent MSA usage by using protein language models instead! Feel free to check all of them out!"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#목차",
    "href": "notebooks/WS06_IntroductionToAF.html#목차",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "소개: 딥러닝의 등장!\n소개: 마법 상자로서의 Alphafold\n\n\n\n\n\n실용적인 Alphafold: 서열 접기\n실용적인 Alphafold: 결과 해석\n다중 서열 정렬(MSA) Coverage\npLDDT\n예측 정렬 오류(PAE)\n\n\n\n\n\nAlphafold 아키텍처: 입력 서열 표현\nAlphafold 아키텍처: Evoformer\nAlphafold 아키텍처: 구조 모듈\n\n\n\n\n\n\nCode\n#@markdown **Please run this cell as you read the introduction**\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\nimport os\n\ncustom_template_path = None\nuse_templates = False\n\n\nUSE_AMBER = False\nUSE_TEMPLATES = use_templates\nPYTHON_VERSION = python_version\n\nif not os.path.isfile(\"COLABFOLD_READY\"):\n  print(\"installing colabfold...\")\n  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n  if os.environ.get('TPU_NAME', False) != False:\n    os.system(\"pip uninstall -y jax jaxlib\")\n    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n  os.system(\"touch COLABFOLD_READY\")\n\nif USE_AMBER or USE_TEMPLATES:\n  if not os.path.isfile(\"CONDA_READY\"):\n    print(\"installing conda...\")\n    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh\")\n    os.system(\"bash Mambaforge-Linux-x86_64.sh -bfp /usr/local\")\n    os.system(\"mamba config --set auto_update_conda false\")\n    os.system(\"touch CONDA_READY\")\n\nif USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n  print(\"installing hhsuite and amber...\")\n  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n  os.system(\"touch HH_READY\")\n  os.system(\"touch AMBER_READY\")\nelse:\n  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n    print(\"installing hhsuite...\")\n    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n    os.system(\"touch HH_READY\")\n  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n    print(\"installing amber...\")\n    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")\n    os.system(\"touch AMBER_READY\")\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\njobname = 'import_test'\nquery_sequence = 'A'\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\nmodel_type = \"auto\"\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\ndownload_alphafold_params(model_type, Path(\".\"))\n\n\ninstalling colabfold...\n\n\nDownloading alphafold2_ptm weights to .: 100%|██████████| 3.47G/3.47G [02:01&lt;00:00, 30.6MB/s]"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#등장-딥러닝",
    "href": "notebooks/WS06_IntroductionToAF.html#등장-딥러닝",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "하지만 단백질 접힘 뒤에 있는 개별 물리적 개념의 대부분을 이해했음에도 불구하고 단백질이 무엇으로 접힐지 확실하게 예측할 수 있는 방정식 레시피는 없었습니다.\n3D 실험 단백질 구조의 예가 증가함에 따라 데이터셋은 결국 신경망이 단백질이 접히는 방식을 예측하도록 훈련할 수 있을 만큼 커졌습니다.\n\n\n\n\n\n가장 유명한 것 중 하나가 AlphaFold입니다! 매년 열리는 단백질 접힘 대회인 CASP에서 모든 경쟁자를 압도적인 차이로 능가하며 단백질 접힘 혁명을 일으켰습니다.\n\n\n힌트: 이전 점수는 약 30-40 Global Distance Score(100점 만점)였습니다.\n\n\nCode\n# @markdown Take a guess!\nAF1 = 70 # @param {type:\"slider\", min:0, max:100, step:10}\nAF2 = 50 # @param {type:\"slider\", min:0, max:100, step:10}\n\nyear = [2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020]\ncasp_scores = [35, 41, 34, 30, 31, 41, 57, 88]\n\n# make bar chart\nfig = plt.figure(figsize=(10, 5))\nplt.bar(year, casp_scores)\nplt.xlabel('Year')\nplt.ylabel('CASP Score')\n\n# add prediction as scatter\nplt.scatter(2018, AF1, label='AF1', color='purple')\nplt.scatter(2020, AF2, label='AF2', color='magenta')\nplt.text(2018, AF1, f'{AF1}', ha='right')\nplt.text(2020, AF2, f'{AF2}', ha='right')\nplt.title('CASP Median Free Modeling Accuracy')\n\n# Set y axis to 0-100\nplt.ylim(0, 100)\n\n\nplt.show()"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#마법-상자로서의-alphafold",
    "href": "notebooks/WS06_IntroductionToAF.html#마법-상자로서의-alphafold",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "AlphaFold는 입력으로 서열과 몇 가지 선택적인 다른 정보를 사용합니다:\n\nMSAs – 다중 서열 정렬, 데이터베이스에서 추론된 유전적으로 유사한 아미노산 서열.\nPair Features – 가능한 잔기 상호 작용의 쌍별 특징 표현.\nTemplates – 알려지지 않은 단백질과 구조가 유사할 수 있는 템플릿 단백질. 이들은 모델의 주요 모듈에 들어가기 전에 쌍 특징과 결합됩니다.\n\n간단히 말해서, 이 세 가지 입력이 모델에 공급되면 모델은 단백질과 AF가 구조가 얼마나 그럴듯하다고 생각하는지에 대한 몇 가지 지표를 뱉어냅니다. 자세한 내용은 AF 아키텍처: AF는 어떻게 작동합니까? 하위 섹션을 참조하세요.\n\n\n\nScreenshot 2024-06-26 000206.png"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#파트-1-서열-접기",
    "href": "notebooks/WS06_IntroductionToAF.html#파트-1-서열-접기",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "Code\n#@markdown Input your favorite protein sequence, then RUN! (Note: Make sure you are connected to a GPU instance. Should take around 2-5 minutes)\nfrom google.colab import files\nimport os\nimport re\nimport hashlib\nimport random\n\nfrom sys import version_info\npython_version = f\"{version_info.major}.{version_info.minor}\"\n\ndef add_hash(x,y):\n  return x+\"_\"+hashlib.sha1(y.encode()).hexdigest()[:5]\n\nquery_sequence = 'MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH' #@param {type:\"string\"}\njobname = 'learning_af'\nnum_relax = 0\ntemplate_mode = \"none\"\n\nuse_amber = num_relax &gt; 0\n\n# remove whitespaces\nquery_sequence = \"\".join(query_sequence.split())\n\nbasejobname = \"\".join(jobname.split())\nbasejobname = re.sub(r'\\W+', '', basejobname)\njobname = add_hash(basejobname, query_sequence)\n\n# check if directory with jobname exists\ndef check(folder):\n  if os.path.exists(folder):\n    return False\n  else:\n    return True\nif not check(jobname):\n  n = 0\n  while not check(f\"{jobname}_{n}\"): n += 1\n  jobname = f\"{jobname}_{n}\"\n\n# make directory to save results\nos.makedirs(jobname, exist_ok=True)\n\n# save queries\nqueries_path = os.path.join(jobname, f\"{jobname}.csv\")\nwith open(queries_path, \"w\") as text_file:\n  text_file.write(f\"id,sequence\\n{jobname},{query_sequence}\")\n\nif template_mode == \"pdb100\":\n  use_templates = True\n  custom_template_path = None\nelif template_mode == \"custom\":\n  custom_template_path = os.path.join(jobname,f\"template\")\n  os.makedirs(custom_template_path, exist_ok=True)\n  uploaded = files.upload()\n  use_templates = True\n  for fn in uploaded.keys():\n    os.rename(fn,os.path.join(custom_template_path,fn))\nelse:\n  custom_template_path = None\n  use_templates = False\n\nprint(\"jobname\",jobname)\nprint(\"sequence\",query_sequence)\nprint(\"length\",len(query_sequence.replace(\":\",\"\")))\n\n\n\nimport os\n\nmsa_mode = \"mmseqs2_uniref_env\"\npair_mode = \"unpaired_paired\"\n\n# decide which a3m to use\nif \"mmseqs2\" in msa_mode:\n  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n\nelif msa_mode == \"custom\":\n  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n  if not os.path.isfile(a3m_file):\n    custom_msa_dict = files.upload()\n    custom_msa = list(custom_msa_dict.keys())[0]\n    header = 0\n    import fileinput\n    for line in fileinput.FileInput(custom_msa,inplace=1):\n      if line.startswith(\"&gt;\"):\n         header = header + 1\n      if not line.rstrip():\n        continue\n      if line.startswith(\"&gt;\") == False and header == 1:\n         query_sequence = line.rstrip()\n      print(line, end='')\n\n    os.rename(custom_msa, a3m_file)\n    queries_path=a3m_file\n    print(f\"moving {custom_msa} to {a3m_file}\")\n\nelse:\n  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n  with open(a3m_file, \"w\") as text_file:\n    text_file.write(\"&gt;1\\n%s\" % query_sequence)\n\n\nmodel_type = \"auto\"\nnum_recycles = \"3\"\nrecycle_early_stop_tolerance = \"auto\"\nrelax_max_iterations = 200\npairing_strategy = \"greedy\"\n\n\n\nmax_msa = \"auto\"\nnum_seeds = 1\nuse_dropout = False\n\nnum_recycles = None if num_recycles == \"auto\" else int(num_recycles)\nrecycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\nif max_msa == \"auto\": max_msa = None\n\nsave_all = False\nsave_recycles = False\nsave_to_google_drive = False\ndpi = 200\nif save_to_google_drive:\n  from pydrive2.drive import GoogleDrive\n  from pydrive2.auth import GoogleAuth\n  from google.colab import auth\n  from oauth2client.client import GoogleCredentials\n  auth.authenticate_user()\n  gauth = GoogleAuth()\n  gauth.credentials = GoogleCredentials.get_application_default()\n  drive = GoogleDrive(gauth)\n  print(\"You are logged into Google Drive and are good to go!\")\n\n\ndisplay_images = True\n\nimport sys\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom Bio import BiopythonDeprecationWarning\nwarnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\nfrom pathlib import Path\nfrom colabfold.download import download_alphafold_params, default_data_dir\nfrom colabfold.utils import setup_logging\nfrom colabfold.batch import get_queries, run, set_model_type\nfrom colabfold.plot import plot_msa_v2\n\nimport os\nimport numpy as np\ntry:\n  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\nexcept:\n  K80_chk = \"0\"\n  pass\nif \"1\" in K80_chk:\n  print(\"WARNING: found GPU Tesla K80: limited to total length &lt; 1000\")\n  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:\n    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]\n  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:\n    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]\n\nfrom colabfold.colabfold import plot_protein\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# For some reason we need that to get pdbfixer to import\nif use_amber and f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:\n    sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")\n\n\ndisplay_images = False\n\ndef input_features_callback(input_features):\n  if display_images:\n    plot_msa_v2(input_features)\n    plt.show()\n    plt.close()\n\ndef prediction_callback(protein_obj, length,\n                        prediction_result, input_features, mode):\n  model_name, relaxed = mode\n  if not relaxed:\n    if display_images:\n      fig = plot_protein(protein_obj, Ls=length, dpi=150)\n      plt.show()\n      plt.close()\n\nresult_dir = jobname\nlog_filename = os.path.join(jobname,\"log.txt\")\nsetup_logging(Path(log_filename))\n\nqueries, is_complex = get_queries(queries_path)\nmodel_type = set_model_type(is_complex, model_type)\n\nif \"multimer\" in model_type and max_msa is not None:\n  use_cluster_profile = False\nelse:\n  use_cluster_profile = True\n\ndownload_alphafold_params(model_type, Path(\".\"))\nresults = run(\n    queries=queries,\n    result_dir=result_dir,\n    use_templates=use_templates,\n    custom_template_path=custom_template_path,\n    num_relax=num_relax,\n    msa_mode=msa_mode,\n    model_type=model_type,\n    num_models=2,\n    num_recycles=2,\n    relax_max_iterations=relax_max_iterations,\n    recycle_early_stop_tolerance=recycle_early_stop_tolerance,\n    num_seeds=num_seeds,\n    use_dropout=use_dropout,\n    model_order=[1,2], # edited\n    is_complex=is_complex,\n    data_dir=Path(\".\"),\n    keep_existing_results=False,\n    rank_by=\"auto\",\n    pair_mode=pair_mode,\n    pairing_strategy=pairing_strategy,\n    stop_at_score=float(100),\n    prediction_callback=prediction_callback,\n    dpi=dpi,\n    zip_results=False,\n    save_all=save_all,\n    max_msa=max_msa,\n    use_cluster_profile=use_cluster_profile,\n    input_features_callback=input_features_callback,\n    save_recycles=save_recycles,\n    user_agent=\"colabfold/google-colab-main\",\n)\nresults_zip = f\"{jobname}.result.zip\"\nos.system(f\"zip -r {results_zip} {jobname}\")\n\n\njobname learning_af_e64d1\nsequence MTWVRQAPGKGPEWVSGINPDGSSTYYADSVKGRFTISRDNAKNTLYLQMNSLKSEDTALYKCATGAAPRIPTTLRGQGTQVTVSSHHHHHH\nlength 92\n2024-10-09 17:38:47,646 Running on GPU\n2024-10-09 17:38:48,066 Found 5 citations for tools or databases\n2024-10-09 17:38:48,067 Query 1/1: learning_af_e64d1 (length 92)\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:01 remaining: ?]\n\n\n2024-10-09 17:38:49,136 Sleeping for 5s. Reason: PENDING\n\n\nPENDING:   0%|          | 0/150 [elapsed: 00:07 remaining: ?]\n\n\n2024-10-09 17:38:55,066 Sleeping for 6s. Reason: PENDING\n\n\nRUNNING:   4%|▍         | 6/150 [elapsed: 00:13 remaining: 05:34]\n\n\n2024-10-09 17:39:02,012 Sleeping for 5s. Reason: RUNNING\n\n\nRUNNING:   7%|▋         | 11/150 [elapsed: 00:19 remaining: 03:56]\n\n\n2024-10-09 17:39:07,932 Sleeping for 8s. Reason: RUNNING\n\n\nRUNNING:  13%|█▎        | 19/150 [elapsed: 00:28 remaining: 03:01]\n\n\n2024-10-09 17:39:16,910 Sleeping for 7s. Reason: RUNNING\n\n\nRUNNING:  17%|█▋        | 26/150 [elapsed: 00:36 remaining: 02:38]\n\n\n2024-10-09 17:39:24,835 Sleeping for 8s. Reason: RUNNING\n\n\nCOMPLETE: 100%|██████████| 150/150 [elapsed: 00:47 remaining: 00:00]\n\n\n2024-10-09 17:39:36,982 Setting max_seq=512, max_extra_seq=5120\n2024-10-09 17:40:15,080 alphafold2_ptm_model_1_seed_000 recycle=0 pLDDT=80 pTM=0.667\n2024-10-09 17:40:21,715 alphafold2_ptm_model_1_seed_000 recycle=1 pLDDT=82.1 pTM=0.692 tol=0.583\n2024-10-09 17:40:28,410 alphafold2_ptm_model_1_seed_000 recycle=2 pLDDT=82.7 pTM=0.702 tol=0.111\n2024-10-09 17:40:28,412 alphafold2_ptm_model_1_seed_000 took 37.7s (2 recycles)\n2024-10-09 17:40:35,204 alphafold2_ptm_model_2_seed_000 recycle=0 pLDDT=84.9 pTM=0.742\n2024-10-09 17:40:42,062 alphafold2_ptm_model_2_seed_000 recycle=1 pLDDT=85.2 pTM=0.742 tol=0.834\n2024-10-09 17:40:48,939 alphafold2_ptm_model_2_seed_000 recycle=2 pLDDT=85.8 pTM=0.749 tol=0.14\n2024-10-09 17:40:48,940 alphafold2_ptm_model_2_seed_000 took 20.5s (2 recycles)\n2024-10-09 17:40:48,961 reranking models by 'plddt' metric\n2024-10-09 17:40:48,961 rank_001_alphafold2_ptm_model_2_seed_000 pLDDT=85.8 pTM=0.749\n2024-10-09 17:40:48,962 rank_002_alphafold2_ptm_model_1_seed_000 pLDDT=82.7 pTM=0.702\n2024-10-09 17:40:49,709 Done\n\n\n0\n\n\n\n\nCode\n## **&lt;font color='#569098'&gt;파트 2: 결과를 어떻게 해석할까요?**&lt;/font&gt;\n\n출력을 다운로드하면(실제 ColabFold 노트북에 편리하게 패키지되어 있거나 이 노트북의 파일에 덜 편리하게 있음) 관심 있는 여러 파일을 얻을 수 있습니다:\n\n1. &lt;font color='#e59454'&gt;\"순위가 매겨진(ranked)\" 3D pdb 모델&lt;/font&gt; 몇 개\n2. &lt;font color='#e59454'&gt;\"coverage\"&lt;/font&gt;를 자세히 설명하는 이미지\n3. &lt;font color='#e59454'&gt;\"pLDDT\"&lt;/font&gt;를 자세히 설명하는 이미지\n4. &lt;font color='#e59454'&gt;\"PAE\"&lt;/font&gt;를 자세히 설명하는 이미지\n\n\n\n각각이 무엇을 의미하는지 살펴봅시다!\n\n\n\n        3Dmol.js failed to load for some reason.  Please check your browser console for error messages.\n        \n\n\n\n\n\n\n\n\n\n\n\n\n“Coverage”는 꽤 간단합니다. AF가 서열 데이터베이스를 살펴볼 때 입력 서열의 부분에 대해 얼마나 많은 일치를 할당할 수 있었는지 보여줍니다.\n\n\n\n\n\nCode\n#@markdown Run to show the MSA coverage of your protein - Is there a lot or a little coverage? (hint: more colors = more coverage)\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_coverage.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\npLDDT는 AF가 예측에 대해 갖는 잔기별 신뢰도로 해석될 수 있습니다. pLDDT가 높을수록 예측이 더 정확합니다.\nDeepmind 웹사이트에 따르면,\n\n&gt;90 = 높은 정확도\n70-90 = 잘 모델링됨\n50-70 = 낮은 신뢰도\n&lt;50 = 아마도 무질서한(disordered) 영역 \n\n전체 pLDDT는 출력 순위를 매기는 데 사용됩니다(그리고 우리가 시각화한 구조는 pLDDT로 색칠됩니다 – 파란색이 더 높고 빨간색이 더 낮음).\n\n\n\n\n\n\nimage.png\n\n\n\n\nCode\n### **&lt;font color='#569098'&gt;PAE - 예측 정렬 오류 (Predicted Aligned Error)**\n\n단백질 도메인 및 잔기 상호 작용에 대한 AF의 신뢰도를 나타냅니다. 모든 잔기 간의 접촉 맵(contact map)으로 제공됩니다.\n\n(x, y)의 색상은 예측과 실제 구조가 잔기 y에 정렬될 때 잔기 x의 위치에서 예상되는 거리 오류에 해당합니다.\n\n낮을수록 좋습니다:\n* 낮음 = 잘 정의된 위치/방향 (파란색)\n* 높음 = 불확실한 상대적 위치 (빨간색)\n\n예: 나노바디가 항원에 잘 결합하면 두 사슬 사이의 상호 작용 영역에서 더 많은 \"파란색\"을 볼 수 있습니다(예: 아래 질문 참조).\n\n\nWhich one has more coverage?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\n\nCode\n#@markdown Run to display your pLDDT score - Is it high confidence? Where might AF2 be less confident?\nfrom IPython.display import display, HTML, Image\nimport base64\nfrom html import escape\n\n# see: https://stackoverflow.com/a/53688522\ndef image_to_data_url(filename):\n  ext = filename.split('.')[-1]\n  prefix = f'data:image/{ext};base64,'\n  with open(filename, 'rb') as f:\n    img = f.read()\n  return prefix + base64.b64encode(img).decode('utf-8')\n\ncov = image_to_data_url(os.path.join(jobname,f\"{jobname}{jobname_prefix}_plddt.png\"))\ndisplay(Image(url=cov, width=800, height=500))\n\n\n\n\n\n\n\n\n아래는 AlphaFold 실행 후 생성된 모든 파일 요약입니다.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    # Create widgets\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    # Set up the layout\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Add button click event\n    submit_button.on_click(display_answer)\n\n    # Display the widget\n    display(HTML(\"&lt;b&gt;Which one is AF more confident in? Why?&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom CSS for text wrapping\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Alphafold has more confidence in the right one! This is because the plddt is higher (there are less \"spikes\")')\n\n\nWhich one is AF more confident in? Why?"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#파트-1-입력-서열의-표현",
    "href": "notebooks/WS06_IntroductionToAF.html#파트-1-입력-서열의-표현",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "AlphaFold 2에 입력되는 단백질 서열 정보는 fasta 파일로 저장됩니다. 다음은 아래의 예입니다:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD\n\n\nCode\n### &lt;font color='#569098'&gt;다중 서열 정렬 및 템플릿 구조 구축\n\n입력 아미노산 서열은 단백질 서열의 여러 데이터베이스(일반적으로 UniRef)에 대한 쿼리로 사용되어 *다중 서열 정렬*(MSA)을 구성합니다. MSA는 살아있는 유기체에서 확인된 유사한(그러나 동일하지는 않은) 서열을 식별합니다. 정렬에서 구조로 어떻게 갈까요? 이론은 함께 진화하는 잔기가 일반적으로 단백질의 접힌 상태에서 서로 가깝다는 것입니다.\n\n유사한 서열에서도 구조적 정보를 얻을 수 있습니다. 이러한 템플릿은 잔기 간의 거리를 결정하기 위해 거리 행렬로 변환될 수 있습니다.\n\n\n\nWhich quadrants represent intermolecular interactions? Is the figure mirrored? Why or why not?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\nBelow is a summary of all the files produced after running AlphaFold\n여기서 우리는 또한 템플릿을 식별하려고 노력하는데, 이는 입력 서열과 유사한 구조를 가질 수 있는 단백질입니다. 이것은 쌍 표현(pair representation)이라고도 하는 구조의 초기 표현을 구성하는 데 사용됩니다. 보존된 조각을 식별함으로써 모델은 이를 구조 구성의 가이드로 사용합니다."
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#파트-2-evoformer",
    "href": "notebooks/WS06_IntroductionToAF.html#파트-2-evoformer",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "여기서 MSA와 템플릿을 가져와 한 쌍의 트랜스포머에 통과시킵니다. 이렇게 하면 (1) 어떤 정보 조각이 더 유익한지 식별하고, (2) MSA와 쌍 표현 간에 정보를 반복적으로 교환하고, (3) MSA와 쌍 상호 작용 모두에 대한 표현을 정제합니다.\n아래는 Evoformer(MSA 트랜스포머와 쌍 트랜스포머로 구성됨)를 자세히 살펴본 것입니다. 왼쪽 이미지: MSA 트랜스포머는 각각 잔기에 해당하는 MSA의 두 열 사이의 상관 관계를 식별합니다. 이 정보는 쌍 표현으로 전달되며, 그 후 쌍 표현은 또 다른 가능한 상호 작용을 식별합니다. 오른쪽 이미지: 정보가 다시 MSA로 전달됩니다. MSA 트랜스포머는 쌍 표현으로부터 입력을 받고 유의미한 상관 관계를 나타내는 또 다른 열 쌍을 관찰합니다.\n\n\n\nimage.png\n\n\n트랜스포머 쌍은 두 변의 합이 세 번째 변보다 크거나 같아야 한다는 삼각형 부등식의 원리에 따라 작동합니다. 어텐션은 잔기의 삼각형 측면에서 배열됩니다. 이 정리를 사용하여 우리는 세 점 사이의 거리가 이 정리를 결코 깨뜨릴 수 없기 때문에 잔기가 서로 떨어져 있을 가능성이 있는 거리를 결정할 수 있습니다.\n\n\nProtein sequence information fed to AlphaFold 2 are stored as a fasta file. Here’s an example below:\n&gt;sp|P46598|HSP90_CANAL Heat shock protein 90 homolog OS=Candida albicans\n(strain SC5314 / ATCC MYA-2876) OX=237561 GN=HSP90 PE=1 SV=1\n\nMADAKVETHEFTAEISQLMSLIINTVYSNKEIFLRELISNASDALDKIRYQALSDPSQLE\nSEPELFIRIIPQKDQKVLEIRDSGIGMTKADLVNNLGTIAKSGTKSFMEALSAGADVSMI\nGQFGVGFYSLFLVADHVQVISKHNDDEQYVWESNAGGKFTVTLDETNERLGRGTMLRLFL\nKEDQLEYLEEKRIKEVVKKHSEFVAYPIQLVVTKEVEKEVPETEEEDKAAEEDDKKPKLE\nEVKDEEDEKKEKKTKTVKEEVTETEELNKTKPLWTRNPSDITQDEYNAFYKSISNDWEDP\nLAVKHFSVEGQLEFRAILFVPKRAPFDAFESKKKKNNIKLYVRRVFITDDAEELIPEWLS\nFIKGVVDSEDLPLNLSREMLQQNKILKVIRKNIVKKMIETFNEISEDQEQFNQFYTAFSK\nNIKLGIHEDAQNRQSLAKLLRFYSTKSSEEMTSLSDYVTRMPEHQKNIYYITGESIKAVE\nKSPFLDALKAKNFEVLFMVDPIDEYAMTQLKEFEDKKLVDITKDFELEESDEEKAAREKE\nIKEYEPLTKALKDILGDQVEKVVVSYKLVDAPAAIRTGQFGWSANMERIMKAQALRDTTM\nSSYMSSKKTFEISPSSPIIKELKKKVETDGAEDKTVKDLTTLLFDTALLTSGFTLDEPSN\nFAHRINRLIALGLNIDDDSEETAVEPEATTTASTDEPAGESAMEEVD"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#파트-3-구조-모듈",
    "href": "notebooks/WS06_IntroductionToAF.html#파트-3-구조-모듈",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "구조 모듈은 정제된 MSA 표현과 정제된 쌍 표현을 사용하여 구조의 3D 모델을 구성합니다. 최종 결과는 단백질의 각 원자(곁사슬 포함) 위치를 나타내는 직카 좌표의 긴 목록입니다. 단백질은 잔기 가스(residue gas)로 표현되는데, 모든 아미노산은 질소, R 그룹 탄소 및 알파 탄소에 점이 있는 삼각형으로 모델링됩니다. 구조 모듈의 시작 부분에서 모든 잔기는 좌표의 원점에 배치됩니다. 모든 단계에서 모델은 잔기를 공간에서 변위/회전시키는 일련의 행렬을 생성합니다. 이 표현은 물리적 또는 기하학적 가정을 반영하지 않으며 결과적으로 네트워크는 구조적 위반을 생성하는 경향이 있는데, 이는 이 비디오에서 볼 수 있습니다.\n\n\n\nimage.png\n\n\n\n\nCode\n중요한 점은 이 모델이 *반복적으로* 작동한다는 것입니다. 구조를 생성한 후 모델은 모든 정보(MSA 표현, 쌍 표현 및 예측된 구조)를 가져와 Evoformer 블록의 시작 부분으로 다시 전달합니다. 여러 반복에 걸쳐 블록 전체에서 예측을 정제하는 모델에 대한 [이 비디오](https://static-content.springer.com/esm/art%3A10.1038%2Fs41586-021-03819-2/MediaObjects/41586_2021_3819_MOESM5_ESM.mp4)를 확인하세요.\n\n\n\n    \n      \n    \n        \n          \n          An MSA is a method for annotating sequences based on their biochemical properties. It’s essential for AF2 as it helps classify sequences into functional categories, which can then be used to infer protein structure.\n        \n        \n        \n          \n          An MSA is a sequence alignment of protein sequences that identifies similar, but not identical, sequences across different organisms. It is important because it helps determine which parts of the sequence are more likely to mutate and allows the detection of correlations between amino acids, providing crucial evolutionary information for predicting protein structure.\n        \n        \n        \n          \n          An MSA is a multi-sequence alignment used to determine the secondary structure of proteins. It is crucial for AF2 because it provides a visual representation of how protein structures align with one another, aiding in the prediction of tertiary structures.\n        \n        \n        \n          \n          An MSA is a matrix of sequence alignments that highlights the most conserved regions in a protein sequence. For AF2, it’s important because it helps in quantifying the stability of different protein folds by analyzing these conserved regions.\n        \n        \n      Submit"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#앞으로",
    "href": "notebooks/WS06_IntroductionToAF.html#앞으로",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "Alphafold의 아키텍처와 개념은 RosettaTTAFold2 및 ESMFold와 같은 다른 딥러닝 기반 단백질 접힘 알고리즘에도 영감을 주었습니다. 각 모델은 장단점이 있으며, 이는 다양한 단백질 유형에 대한 정확도에 따라 달라지며, 종종 실행 시간(더 정확한 모델은 실행하는 데 더 오래 걸림)과 상충 관계에 있습니다. 드노보 단백질 설계이든 천연 복합체 조사이든 원하는 작업에 따라 특정 모델을 선택하는 것이 유리할 수 있습니다. 그러나 그것은 이 튜토리얼의 범위를 벗어납니다.\n또 다른 활발한 진전 분야는 리간드 및 번역 후 수정(post-translational modifications)을 포함한 접힘 예측입니다. 작성 시점을 기준으로 최신 기술에는 RosettaTTAFold All Atom, AF3, HelixFold3, Chai1이 포함됩니다. 이 모델들은 DNA 또는 억제제와 같은 리간드를 접힘 고려 대상에 포함할 수 있었습니다. 특히 HelixFold-single과 Chai1은 대신 단백질 언어 모델을 사용하여 MSA 사용을 우회하려고 합니다! 자유롭게 확인해 보세요!\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the concept of “pair representation” in AF2?\"\ncorrect_answer = \"Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\"\ndecoy_answers = [\"Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\",\n                 \"Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\",\n                 \"Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          Pair representation in AF2 is a model of the likelihood that pairs of amino acids will be in contact with each other. It is an intermediate representation that is refined through the Evoformer module and used to inform the final structure prediction of the protein.\n        \n        \n        \n          \n          Pair representation in AF2 denotes the alignment of protein sequences based on evolutionary relationships. It allows AF2 to identify homologous pairs of sequences and infer structural similarities.\n        \n        \n        \n          \n          Pair representation in AF2 is a method for visualizing the three-dimensional arrangement of amino acid side chains. It provides AF2 with a detailed spatial map of side chains, crucial for understanding the overall protein shape.\n        \n        \n        \n          \n          Pair representation in AF2 is a technique for quantifying the hydrophobic interactions between amino acids. It’s used to estimate the internal energy of the protein structure and refine predictions of protein folding.\n        \n        \n      Submit"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-2-evoformer",
    "href": "notebooks/WS06_IntroductionToAF.html#part-2-evoformer",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "Here, we take the MSA and templates and pass them through a pair of transformers. Doing so (1) identifies which pieces of information are more informative, (2) iteratively exchanges information between MSA and pair representations, and (3) refines the representations for both the MSA and pair interactions.\nBelow is a deeper look into the Evoformer (which is composed of an MSA transformer and pair transformer). Left image: The MSA transformer identifies a correlation between the two columns of the MSA, each corresponding to a residue. This information is passed to the pair representation, where subsequently the pair representation identifies another possible interaction. Right image: Information is passed back to the MSA. The MSA transformer receives an input from the pair representation and observes another pair of columns that exhibit a significant correlation.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the role of the Evoformer in the AF2 architecture?\"\ncorrect_answer = \"The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\"\ndecoy_answers = [\"The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\",\n                 \"The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\",\n                 \"The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The Evoformer is a post-processing step that refines the final protein structure by adjusting predicted coordinates to fit known experimental data. It fine-tunes the structure predictions based on empirical measurements.\n        \n        \n        \n          \n          The Evoformer is a data preprocessing module that normalizes and filters raw protein sequences before they are input into the main prediction model. It ensures that only high-quality sequences are used for structure prediction.\n        \n        \n        \n          \n          The Evoformer is responsible for extracting and refining information from the MSA and templates. It uses a transformer architecture to iteratively exchange information between the MSA and pairwise interactions, improving the model's characterization of protein structure through multiple cycles.\n        \n        \n        \n          \n          The Evoformer functions as a classifier that categorizes amino acid sequences into predefined structural classes. It uses a supervised learning approach to label sequences based on known protein structures.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nThe pair of transformers works on the principle of Triangle inequality, where the sum of two sides must be greater than or equal to the third side. Attention is arranged in terms of triangles of residues. Using this theorem, we can determine the likely distance residues have from one another because the distance between three points can never break that theorem\n\n\n\nimage.png"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#part-3-structure-module",
    "href": "notebooks/WS06_IntroductionToAF.html#part-3-structure-module",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "The structure module takes a refined MSA representation and refined pair representation, and uses this to construct a 3D model of the structure. The end result is a long list of cartesian coordinates representing the position of each atom of the protein (including side chains). Proteins are represented as a residue gas, where every amino acid is modeled as a triangle with points at the Nitrogen, R group Carbon and the Alpha Carbon. At the beginning of the structure module, all residues are placed at the origin of coordinates. At every step, the model produces a set of matrices that displace/rotate the residues in space. This representation does not reflect any physical or geometric assumptions, and as a result the network has a tendency to generate structural violations, which you can see in this video\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"How does the structure module of AF2 generate the final protein structure?\"\ncorrect_answer = \"The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\"\ndecoy_answers = [\"The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\",\n                 \"The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\",\n                 \"The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\"]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          The structure module of AlphaFold 2 utilizes a lattice-based approach to construct the protein model, where amino acids are placed on a fixed grid. It then applies optimization algorithms to minimize structural energy and achieve the final protein structure.\n        \n        \n        \n          \n          The structure module generates the final protein structure by combining sequence motifs with a template library of known protein structures. It matches the input sequence to the closest template and adjusts the model accordingly.\n        \n        \n        \n          \n          The structure module uses a generative adversarial network (GAN) to create the protein structure. It generates multiple possible structures and selects the best one based on adversarial feedback from a discriminator network.\n        \n        \n        \n          \n          The structure module of AlphaFold 2 takes the refined MSA and pair representations and constructs a three-dimensional model of the protein structure. It treats the protein as a 'residue gas,' where amino acids are represented as triangles. The module uses Invariant Point Attention (IPA) to ensure the structure is invariant to translations and rotations.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is invariant point attention (IPA) and why is it important in AF2?\"\ncorrect_answer = \"IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\"\ndecoy_answers = [\"IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\",\n                 \"IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\",\n                 \"IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          IPA is a preprocessing step that aligns protein sequences with known structural templates. It ensures that the input sequences are appropriately matched with template structures before being processed by the main prediction model.\n        \n        \n        \n          \n          IPA is used in the structure module to ensure the model's predictions are invariant to translations and rotations of the protein. This is important because it allows the model to generalize better and requires less data to learn accurate protein structures.\n        \n        \n        \n          \n          IPA is used to predict several possible orientations of the given protein, that way, more detailed information can be captured about its structure as it undergoes iterative refinement.\n        \n        \n        \n          \n          IPA is a technique used in AF2 to perform dimensionality reduction on protein data. It simplifies the representation of the protein structure by projecting it onto a lower-dimensional space, which helps in speeding up the computation.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport random\nfrom IPython.display import display, HTML\n\n\nquestion = \"What is the Frame Aligned Point Error (FAPE) and how is it used in AF2?\"\ncorrect_answer = \"FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\"\ndecoy_answers = [\"FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\",\n                 \"FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\",\n                 \"FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\"]\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          FAPE is a regularization term added to the loss function to penalize large deviations in predicted bond angles. It helps ensure that the model predicts more physically realistic protein structures by constraining bond angles.\n        \n        \n        \n          \n          FAPE is an evaluation metric that measures the similarity between predicted protein structures and a library of experimental structures. It helps assess the accuracy of predictions by comparing them to known, experimentally determined structures.\n        \n        \n        \n          \n          FAPE is a loss function that measures the accuracy of predicted atomic positions in the protein structure. It is similar to RMSD, but includes additional properties to prevent creating proteins with incorrect chirality\n        \n        \n        \n          \n          FAPE is a normalization technique used to adjust the scale of protein structures. It ensures that all predicted structures are on the same scale, which helps in comparing predictions across different models.\n        \n        \n      Submit\n      \n    \n    \n    \n\n\nAn important note is that this model works iteratively. After generating a structure, the model takes all the information (MSA representation, pair representation, and predicted structure) and passes it back to the beginning of the Evoformer blocks. Check this video out of the model refining it’s predictions across blocks over multiple iteractions.\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\n# Define the question and answers\nquestion = \"How does AF2 refine its predictions iteratively?\"\ncorrect_answer = \"Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\"\ndecoy_answers = [\"AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\",\n                 \"AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\",\n                 \"AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\"]\n\n\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n\n    \n      \n    \n        \n          \n          AF2 leverages ensemble learning, combining outputs from various independent models, and iteratively refines predictions by giving more weight to models that better align with known protein structures.\n        \n        \n        \n          \n          AF2 utilizes a hierarchical neural network architecture, where different levels of the network specialize in distinct aspects of protein structure, refining predictions through a top-down feedback loop.\n        \n        \n        \n          \n          AF2 applies a genetic algorithm approach, where multiple variations of protein structures are generated and selectively combined in each iteration, akin to evolutionary selection, to improve predictions.\n        \n        \n        \n          \n          Predictions are iteratively refined by taking the predicted structure, MSA representation, and pair representation from one cycle and using them as input for another round of processing through the Evoformer module. The iterative process allows the model to continuously improve its predictions.\n        \n        \n      Submit"
  },
  {
    "objectID": "notebooks/WS06_IntroductionToAF.html#onwards",
    "href": "notebooks/WS06_IntroductionToAF.html#onwards",
    "title": "섹션 1: 소개: 약간의 역사",
    "section": "",
    "text": "Alphafold’s architecture and concepts have inspired other deep-learning based protein folding algorithms as well, such as RosettaTTAFold2 and ESMFold. Each model has its advantages and disadvantages, dependent on its accuracy (with respect to various protein types), which oftentimes trades off with runtime (more accurate models take longer to run). Based on your desired task, whether it be designing a de novo protein or investigating a natural complex, it may be beneficial to choose a particular model to use. However, that is beyond the scope of this tutorial.\nAnother active field of progress is predicting folding with ligands and post-translational modifications. At the time of writing, the state of the art includes RosettaTTAFold All Atom, AF3, HelixFold3, and Chai1. These models are been able to account for ligands in folding, such as DNA or inhibitors. In particular, HelixFold-single and Chai1 look to circumvent MSA usage by using protein language models instead! Feel free to check all of them out!"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html",
    "title": "섹션 1: 단백질 언어 모델 소개",
    "section": "",
    "text": "##홈페이지로 돌아가기\n이 코드와 튜토리얼은 RosettaCon 2024의 Sergey Ovchinnikov의 강연과 노트북을 각색했습니다.\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다: * 사전 학습된 단백질 언어 모델 미세조정(finetuning)의 이점 설명하기 * LoRA(Low Rank Adaptation)를 사용하여 모델 미세조정하기 * 실제 작업에 미세조정 적용하기: 리간드 결합 부위 예측\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n\n\n\n\n\n소개: 왜 미세조정인가?\n\n\n\n\n\n직접 미세조정: ESM 시각화\n직접 미세조정: 학습\n\n\n\n\n\nLoRA: ESM에 LoRA 부착하기\nLoRA: LoRA란 무엇인가?\nLoRA: LoRA로 학습하기\n\n\n\n\n\n\n대규모 단백질 서열 데이터셋에 대해 학습된 단백질 언어 모델은 단백질 구조와 기능을 이해하고 예측하는 능력을 혁신적으로 변화시켰습니다. 이러한 사전 학습된 모델은 단백질 서열의 일반적인 패턴을 포착하지만, 특정 작업에 맞게 조정해야 하는 경우가 많습니다. 이 과정을 미세조정(finetuning)이라고 합니다.\n\n\n\n작업 특이성(Task Specificity): 사전 학습된 모델은 단백질 서열에 대한 일반적인 이해를 제공하지만 특정 작업에는 더 집중적인 지식이 필요할 수 있습니다.\n제한된 데이터: 많은 단백질 관련 작업에는 레이블이 지정된 데이터가 제한적입니다. 미세조정을 통해 특정 작업에 적응하면서 사전 학습된 모델의 지식을 활용할 수 있습니다.\n효율성: 사전 학습된 모델에서 시작하는 것이 처음부터 학습하는 것보다 종종 더 빠르고 효과적입니다.\n\n그러나 미세조정에는 고유한 과제가 있습니다. 대규모 모델의 모든 파라미터를 미세조정하는 것은 계산 비용이 많이 들 수 있으며, 여전히 데이터셋 크기에 제한을 받습니다. 제한된 작업별 데이터로 전체 미세조정을 수행하면 과적합(overfitting)이 발생할 수 있으며, 모델이 특정 작업에 적응하는 동안 일반적인 지식을 잃을 수 있습니다.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('What might be an example where finetuning is useful',\n                  'Any application with limited data would benefit from finetuning, as it allows us to use transfer learning from more general data.')\n\n\nWhat might be an example where finetuning is useful\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\n미세조정의 중요한 응용 분야 중 하나는 단백질의 어떤 위치가 리간드와 상호 작용할 수 있는지 예측하는 것입니다. 이 정보는 단백질 기능을 이해하고 신약을 설계하는 데 매우 중요합니다.\n이 특정 작업의 경우 ESM 임베딩을 사용하여 각 잔기가 리간드와 결합하는지 여부의 확률을 결정합니다. 레이블이 지정된 결합 포켓이 있는 단백질 세트에서 ESM을 간단히 튜닝합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n#@markdown Let's set up our environment and load the necessary libraries. Take a peak at the code if you are interested in learning how to import large language models using the HuggingFace transformers library!\nmodel_name = \"esm2_t6_8M_UR50D\" # @param [\"esm2_t33_650M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t6_8M_UR50D\"]\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import EsmForSequenceClassification, EsmForTokenClassification, AutoTokenizer\n\ntrainable_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\n\nSome weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\nCode\n#@markdown Let's take a look at our model and how many parameters we are training\nprint(f'NUMBER OF TRAINABLE PARAMETERS: {trainable_params(model)}')\nprint('\\nModel Information\\n----------------------------')\nmodel\n\n\nNUMBER OF TRAINABLE PARAMETERS: 7737722\n\nModel Information\n----------------------------\n\n\nEsmForTokenClassification(\n  (esm): EsmModel(\n    (embeddings): EsmEmbeddings(\n      (word_embeddings): Embedding(33, 320, padding_idx=1)\n      (dropout): Dropout(p=0.15, inplace=False)\n      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n    )\n    (encoder): EsmEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x EsmLayer(\n          (attention): EsmAttention(\n            (self): EsmSelfAttention(\n              (query): Linear(in_features=320, out_features=320, bias=True)\n              (key): Linear(in_features=320, out_features=320, bias=True)\n              (value): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n              (rotary_embeddings): RotaryEmbedding()\n            )\n            (output): EsmSelfOutput(\n              (dense): Linear(in_features=320, out_features=320, bias=True)\n              (dropout): Dropout(p=0.15, inplace=False)\n            )\n            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n          )\n          (intermediate): EsmIntermediate(\n            (dense): Linear(in_features=320, out_features=1280, bias=True)\n          )\n          (output): EsmOutput(\n            (dense): Linear(in_features=1280, out_features=320, bias=True)\n            (dropout): Dropout(p=0.15, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (contact_head): EsmContactPredictionHead(\n      (regression): Linear(in_features=120, out_features=1, bias=True)\n      (activation): Sigmoid()\n    )\n  )\n  (dropout): Dropout(p=0.15, inplace=False)\n  (classifier): Linear(in_features=320, out_features=1, bias=True)\n)\n\n\n\n\n여기서는 대규모 언어 모델(ESM2)을 가져와 리간드 접촉 예측을 위해 튜닝할 추가 레이어를 추가했습니다. 단백질 언어 모델의 모든 파라미터를 직접 미세조정하는 순진한 접근 방식을 시도해 봅시다. 모델이 학습됨에 따라 이 방법의 한계에 대해 생각해 보세요.\n\n\nCode\n%matplotlib inline\nfrom IPython.display import display, clear_output\n#@markdown Run training!!!\n\n# GET DATA\nbatch_size = 32\nmax_crop_len = 512\n\n!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\nimport pickle\nwith open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n  DATA = pickle.load(handle)\n\nimport numpy as np\nimport pickle\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Helper function to pad sequences\ndef pad_sequence(seq, max_len, pad_value=0):\n    pad_size = max(0, max_len - len(seq))\n    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n\nclass CustomProteinDataset(Dataset):\n    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n        self.inputs = inputs\n        self.attention_masks = attention_masks\n        self.outputs = outputs\n        self.masks = masks\n        self.max_crop_len = max_crop_len\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]\n        attention_mask = self.attention_masks[idx]\n        output = self.outputs[idx]\n        mask = self.masks[idx]\n\n        # Calculate the true length of the sequence (where attention_mask == 1)\n        true_len = int(np.sum(attention_mask))\n\n        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n        crop_len = min(self.max_crop_len, true_len)\n\n        # Randomly sample a crop starting index\n        if true_len &gt; crop_len:\n            start_idx = np.random.randint(0, true_len - crop_len + 1)\n        else:\n            start_idx = 0\n\n        # Crop the sequences\n        input_ids = input_ids[start_idx:start_idx + crop_len]\n        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n\n        # Pad the cropped sequences to max_crop_len\n        input_ids = pad_sequence(input_ids, self.max_crop_len)\n        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n        output = pad_sequence(output, self.max_crop_len)\n        mask = pad_sequence(mask, self.max_crop_len)\n\n        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n\n# Create DataLoaders\ndataloaders = []\nfor v in range(3):  # train/test/validation\n    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n                                   max_crop_len=max_crop_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n    dataloaders.append(dataloader)\n\n\n# Training Code\n\ndef compute_loss(logits, labels, mask):\n  \"\"\"Compute masked loss.\"\"\"\n  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n  masked_loss = loss * mask\n  mean_loss = masked_loss.sum() / mask.sum()\n  return mean_loss\n\ndef train_one_epoch(model, dataloader, optimizer):\n  \"\"\"Train the model for one epoch.\"\"\"\n  model.train()\n  total_loss = 0\n\n  for batch in dataloader:\n    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n    # Forward pass\n    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n    logits = outputs.logits.squeeze(-1)\n\n    # Compute loss\n    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    mean_loss.backward()\n    optimizer.step()\n\n    total_loss += mean_loss.item()\n\n  average_loss = total_loss / len(dataloader)\n  return average_loss\n\ndef validate(model, dataloader):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n      for batch in dataloader:\n        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n        # Forward pass\n        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n        logits = outputs.logits.squeeze(-1)\n\n        # Compute loss\n        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n        total_loss += mean_loss.item()\n\n    average_loss = total_loss / len(dataloader)\n    return average_loss\n\n\n\n# Training loop\ndef train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n    \"\"\"Train and validate the model.\"\"\"\n    epoch_plot = []\n    train_plot = []\n    test_plot = []\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n        test_loss = validate(model, test_dataloader)\n        #print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n        # clear matplotlib plot if already exists\n        plt.clf()\n        clear_output(wait=True)\n        # visualize with matplotlib\n        epoch_plot.append(epoch + 1)\n        train_plot.append(train_loss)\n        test_plot.append(test_loss)\n        plt.plot(epoch_plot, train_plot, 'b.-', label='Train')\n        plt.plot(epoch_plot, test_plot, 'r.-', label='Test')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss (Lower is Better)')\n        plt.title('Training and Validation Loss')\n        # Set ticks\n        plt.yticks([])\n        plt.xlim(1, num_epochs)\n        plt.xticks(range(1, num_epochs+1))\n        plt.legend()\n        plt.show()\n\n    print(\"Training complete.\")\n\n# Actually run training\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Why might directly finetuning be suboptimal?',\n                  '1. Training is slow since we need to train lots of parameters\\n2. Direct finetuning leads to overfitting when working with smaller datasets.')\n\n\nWhy might directly finetuning be suboptimal?\n\n\n\n\n\n\n    \n    \n\n\n\n\n\n\nLoRA는 대규모 모델 미세조정과 관련된 많은 문제를 해결하는 효율적인 미세조정 기술입니다. 의존성을 다시 설정해 봅시다. 그러나 모델과 훈련 가능한 파라미터를 살펴보면 뭔가 다릅니다. 무슨 일이 일어났는지 알 수 있나요?\n\n\nCode\n#@markdown Set up dependencies (Re-run for each time you run the training below)\n\n# https://github.com/huggingface/peft\n!pip -q install --no-dependencies peft\nfrom peft import LoraConfig, get_peft_model, TaskType\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\"query\", \"key\", \"value\"],\n    r=4,\n    lora_dropout=0.15,\n)\nmodel = get_peft_model(model, config)\n\n\nSome weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\n\nCode\n#@markdown Show trainable parameters and model\nprint(f'NUMBER OF TRAINABLE PARAMETERS: {trainable_params(model)}')\nprint('\\nModel Information\\n----------------------------')\nmodel\n\n\nNUMBER OF TRAINABLE PARAMETERS: 46401\n\nModel Information\n----------------------------\n\n\nPeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): EsmForTokenClassification(\n      (esm): EsmModel(\n        (embeddings): EsmEmbeddings(\n          (word_embeddings): Embedding(33, 320, padding_idx=1)\n          (dropout): Dropout(p=0.15, inplace=False)\n          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n        )\n        (encoder): EsmEncoder(\n          (layer): ModuleList(\n            (0-5): 6 x EsmLayer(\n              (attention): EsmAttention(\n                (self): EsmSelfAttention(\n                  (query): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (value): lora.Linear(\n                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.15, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=320, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=320, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.0, inplace=False)\n                  (rotary_embeddings): RotaryEmbedding()\n                )\n                (output): EsmSelfOutput(\n                  (dense): Linear(in_features=320, out_features=320, bias=True)\n                  (dropout): Dropout(p=0.15, inplace=False)\n                )\n                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n              )\n              (intermediate): EsmIntermediate(\n                (dense): Linear(in_features=320, out_features=1280, bias=True)\n              )\n              (output): EsmOutput(\n                (dense): Linear(in_features=1280, out_features=320, bias=True)\n                (dropout): Dropout(p=0.15, inplace=False)\n              )\n              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (contact_head): EsmContactPredictionHead(\n          (regression): Linear(in_features=120, out_features=1, bias=True)\n          (activation): Sigmoid()\n        )\n      )\n      (dropout): Dropout(p=0.15, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=320, out_features=1, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=320, out_features=1, bias=True)\n        )\n      )\n    )\n  )\n)\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is different? Select all the correct answers.\"\ncorrect_answers = [\"The model now has less trainable parameters\", \"The model has new connections at each layer\", \"The model has more parameters\"]\ndecoy_answers = [\"The model has become deeper (more layers)\", \"Nothing has changed about the model architecture\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    \n        \n        \n        Nothing has changed about the model architecture\n        \n        \n        \n        The model has new connections at each layer\n        \n        \n        \n        The model now has less trainable parameters\n        \n        \n        \n        The model has become deeper (more layers)\n        \n        \n        \n        The model has more parameters\n        \n    Submit\n    \n    \n    \n\n\n\n\n모든 모델 파라미터를 업데이트하는 대신 LoRA는 은닉층의 출력에 가중치를 다시 부여하여 더 큰 모델을 튜닝하기 위해 소수의 훈련 가능한 파라미터를 도입합니다. 이는 각 레이어(또는 사용자가 지정한 레이어)에 더 작은 추가 은닉 파라미터 세트를 추가하여 수행됩니다. 그러면 전체 모델이 고정(frozen)되고 추가 은닉 파라미터만 학습됩니다.\n\n\n\n어댑터(adaptor)를 학습시킴으로써 단일 GPU에서 거대한 모델을 학습시킬 수 있을 뿐만 아니라 어댑터의 ’병목(bottleneck)’을 조정하여 과적합을 방지할 수 있습니다.\n\n\n\nimage.png\n\n\n\n\nCode\n%matplotlib inline\n#@markdown We can directly run training using the same code as before, now that we have added LoRA adaptors to our model!\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers):\n    answers = [correct_answer] + decoy_answers\n    random.shuffle(answers)\n\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n    &lt;/script&gt;\n    \"\"\"\n    display(HTML(html_code))\n\nquestion = \"How does LoRA help in finetuning large language models?\"\ncorrect_answer = \"It reduces the number of trainable parameters, allowing for efficient finetuning on smaller datasets and preventing overfitting.\"\ndecoy_answers = [\n    \"It increases the model's capacity by adding more layers.\",\n    \"It completely replaces the original model architecture.\",\n    \"It only trains on new data, ignoring the pre-trained weights.\"\n]\ncreate_quiz(question, correct_answer, decoy_answers)\n\n\n순위(rank)와 (선택적으로 드롭아웃)을 변경하여 모델을 더 잘 튜닝할 수 있습니다. 구체적으로 순위를 편집하면 모델을 미세조정하려는 정도를 제어할 수 있습니다. 어댑터는 모델 파라미터가 변경될 수 있는 범위를 제한하며, 순위를 조정하면 이 범위가 제어됩니다.\n순위가 너무 크면 본질적으로 Raw 미세조정과 동일한 작업을 수행하게 됩니다. 너무 작으면 파라미터가 충분히 변경될 수 없기 때문에 모델이 전혀 미세조정되지 않습니다.\n다양한 값을 시도해보고 학습에 어떤 영향을 미치는지 확인해 보세요!\n\n\nCode\n%matplotlib inline\n# Reloading model so that we don't have to re-run\nmodel = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n                                                  num_labels=1,\n                                                  hidden_dropout_prob=0.15)\n\ntokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\nmodel = model.to(DEVICE)\n\n#@markdown Change rank and dropout\nrank = 4 # @param {\"type\":\"integer\"}\ndropout = 0.15 # @param {\"type\":\"number\"}\n\nconfig = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    target_modules=[\"query\", \"key\", \"value\"],\n    r=rank,\n    lora_dropout=0.15,\n)\nmodel = get_peft_model(model, config)\n\n#@markdown Training Parameters\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n이 노트북에서는 특정 작업을 위해 단백질 언어 모델을 미세조정하는 방법을 살펴보았습니다. 다음에 대해 배웠습니다:  1. 미세조정의 이점. 3. 미세조정 기술, 특히 Low Rank Adaptation (LoRA)에 중점을 둠. 4. 이러한 개념을 실제 작업에 적용: 리간드 결합 부위 예측.\n이러한 강력한 모델과 효율적인 미세조정 기술을 활용하여 단백질 설계의 한계를 넓히고 약물 개발 및 효소 공학과 같은 분야의 발견을 가속화할 수 있습니다.\n추가 탐구를 위해 다음을 고려해 보세요: - 다양한 모델 아키텍처와 크기, 특히 SaProt와 같은 구조 인식 언어 모델로 실험해 보세요! - 최상의 균형을 찾기 위해 LoRA 크기를 조정해 보세요. - 이러한 모델을 단백질-단백질 상호 작용 예측 또는 특정 기능을 가진 새로운 단백질 설계와 같은 다른 단백질 관련 작업에 적용해 보세요.\n단백질 언어 모델 분야는 빠르게 발전하고 있으므로 계속 호기심을 갖고 배우세요!\n\n\n\n\n관련 논문\n\n데이터 효율적인 딥 러닝을 통한 Low-N 단백질 공학, Biswas et al., Nature Methods, 2021\n대조 적합성 학습: 단백질 적합성 랜드스케이프의 Low-N 학습을 위한 단백질 언어 모델 재프로그래밍, Zhao et al., bioRxiv, 2024\n단백질 언어 모델 미세 조정으로 다양한 작업의 예측 향상, Schmirler et al., Nature Communications, 2024\nSeqProFT: 서열 전용 단백질 속성 예측을 위한 LoRA 미세조정 적용, Zhang et al., arXiv, 2024"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#목차",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#목차",
    "title": "섹션 1: 단백질 언어 모델 소개",
    "section": "",
    "text": "소개: 왜 미세조정인가?\n\n\n\n\n\n직접 미세조정: ESM 시각화\n직접 미세조정: 학습\n\n\n\n\n\nLoRA: ESM에 LoRA 부착하기\nLoRA: LoRA란 무엇인가?\nLoRA: LoRA로 학습하기"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#왜-미세조정인가",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#왜-미세조정인가",
    "title": "섹션 1: 단백질 언어 모델 소개",
    "section": "",
    "text": "작업 특이성(Task Specificity): 사전 학습된 모델은 단백질 서열에 대한 일반적인 이해를 제공하지만 특정 작업에는 더 집중적인 지식이 필요할 수 있습니다.\n제한된 데이터: 많은 단백질 관련 작업에는 레이블이 지정된 데이터가 제한적입니다. 미세조정을 통해 특정 작업에 적응하면서 사전 학습된 모델의 지식을 활용할 수 있습니다.\n효율성: 사전 학습된 모델에서 시작하는 것이 처음부터 학습하는 것보다 종종 더 빠르고 효과적입니다.\n\n그러나 미세조정에는 고유한 과제가 있습니다. 대규모 모델의 모든 파라미터를 미세조정하는 것은 계산 비용이 많이 들 수 있으며, 여전히 데이터셋 크기에 제한을 받습니다. 제한된 작업별 데이터로 전체 미세조정을 수행하면 과적합(overfitting)이 발생할 수 있으며, 모델이 특정 작업에 적응하는 동안 일반적인 지식을 잃을 수 있습니다.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('What might be an example where finetuning is useful',\n                  'Any application with limited data would benefit from finetuning, as it allows us to use transfer learning from more general data.')\n\n\nWhat might be an example where finetuning is useful"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#raw-미세조정",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#raw-미세조정",
    "title": "섹션 1: 단백질 언어 모델 소개",
    "section": "",
    "text": "여기서는 대규모 언어 모델(ESM2)을 가져와 리간드 접촉 예측을 위해 튜닝할 추가 레이어를 추가했습니다. 단백질 언어 모델의 모든 파라미터를 직접 미세조정하는 순진한 접근 방식을 시도해 봅시다. 모델이 학습됨에 따라 이 방법의 한계에 대해 생각해 보세요.\n\n\nCode\n%matplotlib inline\nfrom IPython.display import display, clear_output\n#@markdown Run training!!!\n\n# GET DATA\nbatch_size = 32\nmax_crop_len = 512\n\n!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\nimport pickle\nwith open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n  DATA = pickle.load(handle)\n\nimport numpy as np\nimport pickle\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\n\n# Helper function to pad sequences\ndef pad_sequence(seq, max_len, pad_value=0):\n    pad_size = max(0, max_len - len(seq))\n    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n\nclass CustomProteinDataset(Dataset):\n    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n        self.inputs = inputs\n        self.attention_masks = attention_masks\n        self.outputs = outputs\n        self.masks = masks\n        self.max_crop_len = max_crop_len\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_ids = self.inputs[idx]\n        attention_mask = self.attention_masks[idx]\n        output = self.outputs[idx]\n        mask = self.masks[idx]\n\n        # Calculate the true length of the sequence (where attention_mask == 1)\n        true_len = int(np.sum(attention_mask))\n\n        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n        crop_len = min(self.max_crop_len, true_len)\n\n        # Randomly sample a crop starting index\n        if true_len &gt; crop_len:\n            start_idx = np.random.randint(0, true_len - crop_len + 1)\n        else:\n            start_idx = 0\n\n        # Crop the sequences\n        input_ids = input_ids[start_idx:start_idx + crop_len]\n        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n\n        # Pad the cropped sequences to max_crop_len\n        input_ids = pad_sequence(input_ids, self.max_crop_len)\n        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n        output = pad_sequence(output, self.max_crop_len)\n        mask = pad_sequence(mask, self.max_crop_len)\n\n        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n\n# Create DataLoaders\ndataloaders = []\nfor v in range(3):  # train/test/validation\n    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n                                   max_crop_len=max_crop_len)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n    dataloaders.append(dataloader)\n\n\n# Training Code\n\ndef compute_loss(logits, labels, mask):\n  \"\"\"Compute masked loss.\"\"\"\n  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n  masked_loss = loss * mask\n  mean_loss = masked_loss.sum() / mask.sum()\n  return mean_loss\n\ndef train_one_epoch(model, dataloader, optimizer):\n  \"\"\"Train the model for one epoch.\"\"\"\n  model.train()\n  total_loss = 0\n\n  for batch in dataloader:\n    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n    # Forward pass\n    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n    logits = outputs.logits.squeeze(-1)\n\n    # Compute loss\n    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    mean_loss.backward()\n    optimizer.step()\n\n    total_loss += mean_loss.item()\n\n  average_loss = total_loss / len(dataloader)\n  return average_loss\n\ndef validate(model, dataloader):\n    \"\"\"Validate the model.\"\"\"\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n      for batch in dataloader:\n        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n\n        # Forward pass\n        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n        logits = outputs.logits.squeeze(-1)\n\n        # Compute loss\n        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n\n        total_loss += mean_loss.item()\n\n    average_loss = total_loss / len(dataloader)\n    return average_loss\n\n\n\n# Training loop\ndef train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n    \"\"\"Train and validate the model.\"\"\"\n    epoch_plot = []\n    train_plot = []\n    test_plot = []\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n        test_loss = validate(model, test_dataloader)\n        #print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n        # clear matplotlib plot if already exists\n        plt.clf()\n        clear_output(wait=True)\n        # visualize with matplotlib\n        epoch_plot.append(epoch + 1)\n        train_plot.append(train_loss)\n        test_plot.append(test_loss)\n        plt.plot(epoch_plot, train_plot, 'b.-', label='Train')\n        plt.plot(epoch_plot, test_plot, 'r.-', label='Test')\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss (Lower is Better)')\n        plt.title('Training and Validation Loss')\n        # Set ticks\n        plt.yticks([])\n        plt.xlim(1, num_epochs)\n        plt.xticks(range(1, num_epochs+1))\n        plt.legend()\n        plt.show()\n\n    print(\"Training complete.\")\n\n# Actually run training\nlearning_rate = 1e-3 # @param {\"type\":\"number\"}\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nnum_epochs = 20  # @param {\"type\":\"integer\"}\ntrain_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)\n\n\n\n\n\n\n\n\n\nTraining complete.\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Question Time! Run to Show Question\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;&lt;b&gt;{correct_answer}&lt;/b&gt;&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: green;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box('Why might directly finetuning be suboptimal?',\n                  '1. Training is slow since we need to train lots of parameters\\n2. Direct finetuning leads to overfitting when working with smaller datasets.')\n\n\nWhy might directly finetuning be suboptimal?"
  },
  {
    "objectID": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#결론",
    "href": "notebooks/WS05_LanguageModelEmbeddingsTransferLearningForDownstreamTask.html#결론",
    "title": "섹션 1: 단백질 언어 모델 소개",
    "section": "",
    "text": "이 노트북에서는 특정 작업을 위해 단백질 언어 모델을 미세조정하는 방법을 살펴보았습니다. 다음에 대해 배웠습니다:  1. 미세조정의 이점. 3. 미세조정 기술, 특히 Low Rank Adaptation (LoRA)에 중점을 둠. 4. 이러한 개념을 실제 작업에 적용: 리간드 결합 부위 예측.\n이러한 강력한 모델과 효율적인 미세조정 기술을 활용하여 단백질 설계의 한계를 넓히고 약물 개발 및 효소 공학과 같은 분야의 발견을 가속화할 수 있습니다.\n추가 탐구를 위해 다음을 고려해 보세요: - 다양한 모델 아키텍처와 크기, 특히 SaProt와 같은 구조 인식 언어 모델로 실험해 보세요! - 최상의 균형을 찾기 위해 LoRA 크기를 조정해 보세요. - 이러한 모델을 단백질-단백질 상호 작용 예측 또는 특정 기능을 가진 새로운 단백질 설계와 같은 다른 단백질 관련 작업에 적용해 보세요.\n단백질 언어 모델 분야는 빠르게 발전하고 있으므로 계속 호기심을 갖고 배우세요!"
  },
  {
    "objectID": "notebooks/WS08_DenoisingDiffusionProbabilisticModels.html",
    "href": "notebooks/WS08_DenoisingDiffusionProbabilisticModels.html",
    "title": " 섹션 0: 개요 ",
    "section": "",
    "text": "디노이징 확산 확률 모델 (DDPMs) 소개 \n##홈페이지로 돌아가기\n이 코드와 튜토리얼은 Sergey Lyskov의 워크숍에서 각색되었습니다.\n추가 배경 읽기 자료: Ho et al.의 디노이징 확산 확률 모델\n목표: 이 노트북을 마치면 다음을 수행할 수 있습니다:\n\n확산 모델이 데이터를 생성하는 방식 설명\n확산 모델의 두 가지 구성 요소 정의\n높은 수준에서 수학적 형식과 단순화 설명\n코드로 정방향 및 역방향 확산 프로세스 구현\n손실 항(term)과 모델이 학습하는 파라미터 해석\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n  목차 \n섹션 0: 개요\n섹션 1: 정방향 프로세스 * 데이터에 노이즈 추가하기 * 노이즈 추가 스케줄링 * 순차적 vs 병렬 생성 * 사인파를 사용하여 정방향 프로세스 구현하기\n섹션 2: 역방향 프로세스 * 모델 출력: 원본 데이터 생성 * 손실 항\n섹션 3: DDPM 구현 * 시간 임베딩 * 신경망 * 학습 * 생성 * 사후 분산\n섹션 4: 요약 및 질문 * 확산 모델 이해하기 * 수학적 기초 * 질문\n\n\n\n디노이징 확산 모델은 노이즈로부터 데이터 샘플을 생성하는 생성 모델입니다. DALLE-2, Imagen 또는 Stable Diffusion과 같은 도구를 사용하여 텍스트 입력에서 이미지를 생성한 적이 있다면 이미 데이터 생성을 위한 확산 모델의 힘을 알고 있는 것입니다. GAN(Generative Adversarial Networks) 및 VAE(Variational AutoEncoders)와 같은 다른 생성 모델에 비해 DDPM은 더 낮은 계산 비용으로 더 나은 품질의 이미지를 생성합니다.\n확산 아키텍처에는 두 가지 프로세스가 있습니다-\n\n샘플의 데이터를 파괴하고 노이즈로 변환하는 정방향 프로세스(Forward Process).\n추가된 노이즈를 예측하고 원본/유사한 이미지를 반환하는 역방향 프로세스(Reverse Process).\n\n주황색 상자가 가능한 모든 데이터 표현의 전체 공간을 나타낸다고 가정해 봅시다. 이 공간 내에서 우리는 흰색 원 안에 둘러싸인 데이터 포인트의 하위 집합에 특히 관심이 있습니다. 녹색 점은 이 하위 집합 내에 있는 우리가 관심 있는 데이터 종류의 예입니다.\n녹색 점에 확산 과정을 적용하면 데이터가 흰색 원에서 멀어져 데이터 공간의 더 넓은 영역으로 밀려납니다. 확산 모델은 녹색 점을 흰색 원 안의 원래 위치로 다시 안내하도록 학습됩니다.\n\n\n\nimage.png\n\n\n이미지 출처: https://learnopencv.com/denoising-diffusion-probabilistic-models/\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Question Time! Run to Show Question &lt;/b&gt;\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct! Only the reverse process requires a neural network. &lt;br&gt; Since the forward process makes the data noisy, we can use a predefined algorithm to implement the noising!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect. Only the reverse process requires a neural network. &lt;br&gt; Since the forward process makes the data noisy, we can use a predefined algorithm to implement the noising!\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"At which step is the neural network used? Select all the correct answers.\"\ncorrect_answers = [\"Reverse Process\"]\ndecoy_answers = [\"Forward Process\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    At which step is the neural network used? Select all the correct answers.\n        \n        \n        Reverse Process\n        \n        \n        \n        Forward Process\n        \n    Submit\n    \n    \n    \n\n\n정방향 및 역방향 프로세스를 자세히 살펴보겠습니다.\n다음 셀을 실행하여 신경망을 위한 표준 패키지를 로드하고 GPU(사용 가능한 경우)를 활성화하세요.\n\n\nCode\n#Run this cell to load the standard packages for neural networks\nimport random, math\nfrom tqdm import tqdm # fun package to display a status bar during time consuming loops\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nrandom.seed(0);\ntorch.manual_seed(0)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n\n 섹션 1: 정방향 프로세스 \n분포 \\(q\\)로 설명되는 정방향 프로세스는 데이터에 점진적으로 노이즈를 추가합니다. 데이터가 순수한 가우스 노이즈로 변환될 때까지 여러 타임스텝에 걸쳐 수행됩니다. 이미지를 예로 들면, \\(t = 0\\)에서 실제 이미지 \\(x_0\\)로 시작하여 매 타임스텝 \\(t\\)마다 이전 타임스텝의 이미지에 약간의 노이즈를 추가합니다. 노이즈는 일반적으로 평균이 0이고 표준 편차가 1인 가우스 분포에서 샘플링됩니다. 따라서,\n\\[x_t = x_{t-1} + \\text{noise}\\].\n정방향 프로세스는 다음과 같이 시각화할 수 있습니다:   이미지 출처: 확산 모델에 대한 단계별 시각적 소개 [https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models]\n확산 과정을 이해하기 위해 표준 PyTorch 부품으로 간단한 모델을 구축해 보겠습니다. 노이즈를 추가하고 제거할 사인파의 1차원 데이터셋을 만들어 봅시다.\n각 사인파는 위상, 주파수 및 진폭을 무작위로 샘플링하여 생성됩니다. 각 배치(batch_size)에 64개의 사인파가 있는 32개 포인트(input_size) 세트를 샘플링합니다. 가장 간단한 경우로 주파수와 진폭에 대한 가중치 변수를 0으로 설정했습니다.\n이 노트북의 후속 라운드에서 이 변수들을 자유롭게 플레이해 보세요.\n\n\nCode\ninput_size = 32\nbatch_size = 64\n\nhistorical_total_loss = []\n\nclass SineFunctionDataSet(Dataset):\n  def __init__(self, size):\n    super().__init__()\n    self.input_size = input_size\n    data = []\n\n    for i in range(size):\n      phase = 10*random.random() * 2.0 * 3.15 * 10\n      freq = 14 + 0 * random.random()\n      amplitude = 1 + 0*random.random()\n\n      # equation for the sin wave: y = amplitude * sin( x * frequency / input_size + phase)\n      data.append( torch.Tensor([ amplitude * math.sin(t_sin*freq/input_size + phase) for t_sin in range(input_size) ] ))\n\n    self.data = data\n\n  def __len__(self):\n    return len(self.data)\n\n  def __getitem__(self, i):\n    return self.data[i]\n\nsine_data = SineFunctionDataSet(1024*8)\ndata_loader = DataLoader(sine_data, batch_size = batch_size, drop_last=True)\n\n\n\n\nCode\n#visualizing the data\nfor i in sine_data[0:8]:\n  plt.plot(i)\n  plt.xlabel(\"Time\")\n  plt.ylabel(\"Amplitude\")\n  plt.title(\"Input Sine Waves\")\n\n\n\n\n\n\n\n\n\n이 분포에서 샘플링하여 데이터 샘플 \\(x_0\\)를 얻을 수 있습니다.\n타임스텝 \\(t-1\\)의 데이터 \\(x_{t-1}\\)을 입력으로 받아 가우스 노이즈를 추가하여 다음 타임스텝 \\(t\\)의 샘플을 생성하는 정방향 프로세스 \\(q\\)를 정의합니다. 첫 번째 타임스텝의 경우 \\(q_{t_0} \\sim x_0\\)입니다. 추가할 노이즈의 양(또는 가중치)은 분산 스케줄 \\(\\beta_t\\)를 따릅니다.\n따라서 정방향 프로세스는 다음과 같이 정의됩니다: \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\bf{I}),\\]\n여기서 \\(\\mathcal{N}\\)은 평균 \\(\\mu_t = \\sqrt{1-\\beta_t}x_{t-1}\\) 및 분산 \\(\\sigma^2 = \\beta_t\\)의 두 파라미터로 정의되는 정규 분포(일명 가우스 분포)입니다.\n이 방정식은 다음과 같이 재매개변수화(reparameterized)될 수 있습니다. \\[q(x_t | x_{t-1}) = \\sqrt{1-\\beta_t}x_{t-1} + \\sqrt{\\beta_t} \\mathcal{E},\\] 여기서 \\(\\mathcal{E}\\in\\mathcal{N}(0,1)\\)입니다.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Question Time! Run to Show Question &lt;/b&gt;\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect &lt;br&gt; Since we are adding noise at every timestep, the final output is Gaussian Noise!\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\n\n# Example\nquestion = \"What is the final output of the forward diffusion process?\"\ncorrect_answers = [\"Gausian Noise\"]\ndecoy_answers = [\"Denoised Data\", \"Randomized Inputs\", \"Original Features\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the final output of the forward diffusion process?\n        \n        \n        Denoised Data\n        \n        \n        \n        Randomized Inputs\n        \n        \n        \n        Original Features\n        \n        \n        \n        Gausian Noise\n        \n    Submit\n    \n    \n    \n\n\n\\(x_0\\)에서 시작하여 \\(x_1\\), …, \\(x_t\\), …, \\(x_T\\)를 생성합니다. 여기서 \\(x_T\\)는 순수한 가우스 노이즈입니다. 추가되는 노이즈의 양을 제어하려면 스케줄 \\(\\beta_t\\)가 필요합니다. 일반적으로 \\(\\beta_t\\)는 모든 타임스텝마다 증가하여 프로세스를 통해 점진적으로 더 많은 노이즈가 추가됩니다:\n\\[0 &lt; \\beta_1 &lt; \\beta_2 &lt; ... &lt; \\beta_T &lt; 1\\]\n\n\nCode\n#@markdown A simple example of a noise schedule is a linear function with increasing $\\beta$s\n\n#@markdown The max value of $\\beta$ is limited to keep the variance from exploding\nbetas = torch.arange(0, .5, 0.01)\nplt.plot(betas)\nplt.xlabel(\"timestep\")\nplt.ylabel(\"beta\")\n\nplt.annotate('Max Beta', xy=(49, 0.5), xytext=(29, 0.1), fontsize=12,\n            arrowprops=dict(facecolor='green', shrink=0.05));\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Food For Thought! Run to Show Question &lt;/b&gt;\n\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;{correct_answer}&lt;/pre&gt;\"\n\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n    submit_button = widgets.Button(description=\"Submit\")\n    result = widgets.HTML(value=\"\")\n\n    vbox = widgets.VBox([text_area, submit_button, result])\n    submit_button.on_click(display_answer)\n\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: black;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage\ncreate_answer_box(\"What is the problem with an exploding gradient?\", \"Recall how a neural network is trained: \"\n                  \"at the end of the forward pass the derivatives are \"\n                  \"backpropagated and used to update the model weight. When the variance \"\n                  \"is too high these derivatives dominate the model weights, making \"\n                  \"it difficult to converge to a minimum during gradient descent.\"\n                  \"Refresh the concepts from one of the earlier notebooks if this difficult to follow\")\n\n\nWhat is the problem with an exploding gradient?\n\n\n\n\n\n\n    \n    \n\n\n그러나 2021년 OpenAI의 연구원들은 선형 스케줄이 가장 효율적이지 않다는 것을 발견했습니다. 선형 스케줄을 사용하면 전체 단계의 약 절반 후에 원본 이미지의 대부분이 손실됩니다. 남은 데이터는 학습하기에 그리 유용하지 않습니다. 이 단점을 극복하고 학습 시간을 효과적으로 활용하기 위해 연구원들은 노이즈를 더 천천히 추가하는 맞춤형 스케줄을 설계했습니다. 유용한 스케줄 중 하나는 코사인 스케줄입니다. 아래 이미지에서 하단의 코사인 스케줄이 초기 타임스텝에서 더 많은 정보를 보존하는 반면 이후 타임스텝에서는 더 큰 노이즈를 추가하는 것을 볼 수 있습니다:\n\n다양한 스케줄 시각화. 상단 패널: 선형 스케줄 하단 패널: 코사인 스케줄\n이미지 출처: 향상된 디노이징 확산 확률 모델, Alex Nichol, Prafulla Dhariwal 2021\n\n\nCode\n# Cosine schedule\nbetas_linear = torch.arange(0, .5, 0.01)\nbetas = 1 - ( betas_linear * torch.pi / 2 ).cos()\nplt.plot(betas);\nplt.xlabel(\"timestep\");\nplt.ylabel(\"beta\");\n\n\n\n\n\n\n\n\n\n위에서 설명한 대로 정방향 확산 프로세스는 \\(t_0\\)에서 시작하여 \\(t_T\\)까지 순차적으로 이동합니다. 각 타임스텝의 샘플이 이전 샘플에 의존하기 때문입니다. 단계별로 노이즈를 추가하는 것은 비용이 많이 듭니다. 프로세스를 더 빠르게 만들기 위해 여러 단계의 효과를 결합하여 원하는 타임스텝에서의 출력을 정의하는 하나의 공식을 생성할 수 있습니다 (여기에서 자세한 내용 확인).\n\\[q(x_t | x_0) = \\sqrt{\\bar{\\alpha_t}}x_0 + \\sqrt{1 - \\bar{\\alpha_t}} \\mathcal{E}\\] 여기서 \\[\\alpha_t = 1 - \\beta_t\\] \\[\\bar{\\alpha}_t := Π^t_{s=1} a_s\\]입니다.\n\\(\\alpha\\) 값을 계산하고 저장해 봅시다:\n\n\nCode\nalphas = torch.Tensor( [1.0 - b for b in betas] )\nalphas_cumprod = torch.cumprod(alphas, dim=0)\nsqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\nsqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\nsqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n\n# let's also store our previous alphas, which will come in use later\nalphas_prev = F.pad(alphas[:-1], (1, 0), value=1.0)\nalphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n\nplt.plot(sqrt_one_minus_alphas_cumprod, label = 'sqrt one minus alphas cumprod')\nplt.plot(alphas, label ='alpha')\nplt.xlabel(\"timestep\");\nplt.ylabel(r\"alpha, $bar{alpha}$\");\nplt.legend();\n\n\n\n\n\n\n\n\n\n이제 다양한 상수를 얻었습니다. 다음으로 특정 타임스텝 \\(t\\)에서 데이터에 노이즈를 추가하는 함수를 만들어 봅시다.\n\n\nCode\ndef noised_data(x0, t, noise=None):\n    if noise is None: noise = torch.randn_like(x0)\n\n    alpha_t = alphas_cumprod[t]\n\n    x = x0 * torch.sqrt(alpha_t) + torch.sqrt(1.0 - alpha_t) * noise\n\n    return x, noise\n\n\n사인파에 노이즈를 추가하는 예를 살펴봅시다. 이미지는 확산 과정을 통해 진화하며 지정된 스케줄에 따라 시간이 지남에 따라 분산이 감소함에 따라 점차 노이즈가 많아집니다.\n\n\nCode\nimport time\nimport pylab as pl\nfrom IPython.display import display, clear_output\n\nexample = next(iter(data_loader))\nfor i in range(len(betas)):\n  plt.clf()\n  plt.plot( noised_data(example, i)[0][0] )\n  plt.ylabel(\"noised data (x_t)\")\n  plt.ylim(-1.5,1.5)\n  plt.text(0, 1.25, 'time $t = $'+str(i), fontsize = 22)\n  display(pl.gcf())\n  clear_output(wait=True)\n  time.sleep(0.04)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run to Show Question &lt;/b&gt;\n\ncreate_answer_box('Can we have a constant variance schedule? How will it impact the forward process?',\n                  'Yes we can! But it would make the noise uniform and suboptimally utilze the learning capacity of the model. The choice of variance schedule is important in deciding the performance of the model.')\n\n\n\n    \n\n\nCan we have a constant variance schedule? How will it impact the forward process?\n\n\n\n\n\n\n    \n    \n\n\n\n    \n\n\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt; &lt;b&gt; This is a trickier question but give it a try! Run to Show Question &lt;/b&gt;\n\nfrom IPython.display import HTML, display\nfrom ipywidgets import widgets\n\ndef create_answer_box(question, correct_answer):\n    # Function to display the answer when the button is clicked\n    def display_answer(button):\n        user_answer = text_area.value.strip()\n        result.value = f\"&lt;pre class='answer-text'&gt;{correct_answer}&lt;/pre&gt;\"\n        display(HTML(mathjax_script))  # Refresh MathJax to render equations\n\n    # Create a text area for user input\n    text_area = widgets.Textarea(\n        placeholder='Enter your answer here...',\n        layout={'width': '95%', 'height': '100px'}\n    )\n\n    # Create a submit button\n    submit_button = widgets.Button(description=\"Submit\")\n\n    # Create an HTML widget to display the correct answer\n    result = widgets.HTML(value=\"\")\n\n    # Create a vertical box layout for the text area, button, and result\n    vbox = widgets.VBox([text_area, submit_button, result])\n\n    # Bind the button click event to the display_answer function\n    submit_button.on_click(display_answer)\n\n    # Display the question with LaTeX support using MathJax\n    mathjax_script = \"\"\"\n    &lt;script type=\"text/javascript\" async\n      src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML\"&gt;\n      &lt;/script&gt;\"\"\"\n\n    # Display the question and widgets\n    display(HTML(mathjax_script))\n    display(HTML(f\"&lt;b&gt;{question}&lt;/b&gt;\"))\n    display(vbox)\n\n    # Add custom styling for the output box\n    display(HTML(\"\"\"\n    &lt;style&gt;\n    .widget-html-content {\n        width: 95%;\n        white-space: normal;\n        word-wrap: break-word;\n    }\n    .answer-text {\n        width: 90%;\n        color: black;\n        line-height: 1.5;\n        white-space: pre-wrap;\n        word-wrap: break-word;\n        font-family: inherit;\n        font-size: inherit;\n        margin: 0;\n        padding: 0;\n    }\n    &lt;/style&gt;\n    \"\"\"))\n\n# Example usage with LaTeX equations in both question and correct answer\nquestion = r\"Here is the forward process equation: $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\mathbf{I})$$ Why do we add a weight term to the mean? $$ weight = \\sqrt{1-\\beta_t} $$\"\n\n# Correct answer with LaTeX for \\beta and other symbols\ncorrect_answer = r\"\"\"By weighing the x_t term we reduce the contribution of the original signal gradually, ensuring that the final output of the forward process is nearly Gaussian noise. To visualize this, consider a one-dimensional data distribution consisting of a waveform with multiple peaks, as shown below. The goal of the forward process is to destroy the data and convert it to a normal distribution with a single peak (mean) at 0 and unit variance.\n&lt;img src='https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_forward_process_changing_distribution.png' width='400px'&gt;\nImage Source: https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html\n\"\"\"\ncreate_answer_box(question, correct_answer)\n\n\n\n    \n\n\nHere is the forward process equation: $$q(x_t | x_{t-1}) = \\mathcal{N}(x_t ; \\sqrt{1-\\beta_t}x_{t-1},\\beta_t \\mathbf{I})$$ Why do we add a weight term to the mean? $$ weight = \\sqrt{1-\\beta_t} $$\n\n\n\n\n\n\n    \n    \n\n\n\n\n섹션 2: 역방향 프로세스 \n역방향 프로세스에서 모델은 노이즈가 있는 버전에서 원본 이미지를 생성하는 방법을 학습합니다.\n 추가적인 전문적 세부 정보: 모델이 학습 데이터와 유사한 데이터를 생성하기를 원하지만 정확한 복제본을 목표로 하지는 않습니다. 대신 생성된 데이터가 원본 이미지와 동일한 분포에 속하도록 하는 것이 목표입니다. 예를 들어 모델이 노란 해바라기 이미지로 학습된 경우, 단일 고정 색조를 재현하는 대신 자연스러운 변형을 도입하여 다양한 노란색 음영의 해바라기를 생성할 수 있어야 합니다. 이를 통해 모델은 개별 데이터 포인트를 단순히 암기하는 것이 아니라 데이터 분포 내의 다양성을 포착할 수 있습니다. \n조건부 분포 \\(p(x_{t-1}|x_t)\\)를 안다면, 즉 노이즈가 더 많은 이미지 \\(x_t\\)에 대한 노이즈가 덜한 이미지 \\(x_{t-1}\\)의 의존성을 안다면, 가우스 노이즈 \\(x_T\\)에서 샘플링한 다음 점진적으로 디노이징하여 실제 분포 \\(x_0\\)의 샘플로 끝나는 프로세스를 역으로 실행할 수 있습니다.\n그러나 우리는 \\(p(x_{t-1}|x_t)\\)를 모릅니다. 이 조건부 확률을 계산하려면 특정 \\(x_t\\)를 초래할 수 있는 모든 가능한 \\(x_{t-1}\\)의 분포를 알아야 하므로 다루기 힘듭니다. 따라서 우리는 신경망을 사용하여 조건부 분포 \\(p(x_0|x_t)\\)를 예측할 것입니다.\n모델의 예측은 \\(p_\\theta (x_{t-1}|x_t)\\)로 표현되며, 여기서 \\(\\theta\\)는 경사 하강법에 의해 업데이트되는 신경망의 파라미터입니다. 각 단계에서 일반적으로 선택되는 함수 형태는 가우스입니다:\n\\[p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1} ; \\mu_\\theta (x_t, t),\\sigma^2_\\theta(x_t, t ))\\]\n여기서 평균과 분산도 노이즈 레벨 \\(t\\)에 따라 조건부입니다. 이런 식으로 우리의 네트워크는 이제 가우스 분포에 대한 이러한 파라미터, \\(\\mu\\) 및 \\(\\sigma\\)를 예측해야 합니다.\n\n\n\nimage.png\n\n\n이미지 출처: https://erdem.pl/2023/11/step-by-step-visual-introduction-to-diffusion-models\n\n\n 손실 항 (THE LOSS TERM) \n\n\n모든 타임스텝에서 노이즈 이미지와 예측 이미지의 차이는 추가된 노이즈를 정의합니다.\n모델의 총 손실은 모든 타임스텝에서의 손실의 합입니다:\n\\[L = L_0 + L_1 + L_2 + L_3 + L_4 ...  + L_T\\]\n\\(L_T\\) 손실은 무시할 수 있습니다. 해당 타임스텝에서는 학습할 신경망 파라미터가 없으며 우리가 할 수 있는 최선의 방법은 좋은 분산 스케줄러를 정의하고 적절한 크기의 타임스텝을 사용하는 것이기 때문입니다.\n경험적으로 디노이징 확산 확률 모델 논문의 저자들은 모델이 \\(L_0\\) 항 없이 더 나은 성능을 발휘한다는 것을 발견하여 학습 중에 종종 무시됩니다.\n그런 다음 쿨백-라이블러(Kullback–Leibler) 발산을 사용하여 \\(L_t\\)를 추정할 수 있습니다. 이는 단순히 원본 확률 분포와 예측된 분포 간의 차이를 측정하는 것입니다.\n\\[ D_{KL}(q(x_t| x_0) ∥ p_θ(x_t| x_t−1)) = - ELBO + log(p_θ(x_0| x_t)) \\]\n우리는 \\(D_{KL}\\)을 최소화하기를 원하며 조건부 확률은 상수입니다. 따라서 학습에 사용할 수 있는 유일한 항은 ELBO입니다. ELBO(Evidence Lower Bound)는 재구성 손실을 모델이 매끄럽고 연속적인 분포를 학습하도록 장려하는 정규화 항과 결합합니다.\nELBO를 최대화하면(따라서 \\(D_{KL}\\)을 최소화하면) 모델이 데이터에 잘 맞을 뿐만 아니라 새로운 데이터로 효과적으로 일반화됩니다.\n각 타임스텝에서 원본 확률 분포를 예측하는 대신 \\(p_θ\\) 방정식을 재매개변수화하여 노이즈만 예측할 수 있습니다.\n\\[x_{t-1}  = \\mathcal{N}(x_{t-1} ; \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1 - \\bar{\\alpha_t}}}\\mathcal{E_\\theta}(x_t, t)), \\sqrt{\\beta_t}\\mathcal{E})\\]\n변분 추론(variational inference)에 대한 자세한 내용은 다음에서 확인하세요: https://gregorygundersen.com/blog/2021/04/16/variational-inference/\n우리는 많은 새로운 개념을 살펴보았지만 핵심 내용은 모델이 각 단계에서 노이즈를 예측하도록 학습되고 학습에 사용하는 최종 손실 항은 정방향 프로세스에서 추가된 노이즈와 모델이 예측한 노이즈 간의 평균 제곱 오차(MSE)라는 것입니다.\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run the cell for the question&lt;/b&gt;&lt;/font&gt;\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct! &lt;br&gt;  As discussed in the above section, at each step the model predicts the noise which has to be removed to obtain the original data\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect. &lt;br&gt; As discussed in the above section, at each step the model predicts the noise which has to be removed to obtain the original data\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\nquestion = \"What is the model trained to predict at each time step?\"\ncorrect_answers = [\"Noise Added to the Data\"]\ndecoy_answers = [\"Conditional Probability\", \"Randomized Probability Distribution\", \"Original Data\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the model trained to predict at each time step?\n        \n        \n        Conditional Probability\n        \n        \n        \n        Randomized Probability Distribution\n        \n        \n        \n        Noise Added to the Data\n        \n        \n        \n        Original Data\n        \n    Submit\n    \n    \n    \n\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run the cell for the question&lt;/b&gt;&lt;/font&gt;\ndef create_quiz(question, correct_answers, decoy_answers):\n    # Combine the correct answers with the decoy answers\n    answers = correct_answers + decoy_answers\n    # Shuffle the answers\n    random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\" &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n    &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\"\"\"\n\n    # Add each answer as a checkbox\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n        &lt;input type=\"checkbox\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n        &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\"\"\"\n\n    # Add the submit button and feedback section\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswers() {{\n        var checkboxes = document.getElementsByName('quiz');\n        var correctAnswers = {str(correct_answers)};\n        var feedback = document.getElementById('feedback');\n        var numCorrect = 0;\n\n        // Check each answer\n        for (var i = 0, length = checkboxes.length; i &lt; length; i++) {{\n            var checkbox = checkboxes[i];\n            var label = document.getElementById('label' + i);\n\n            if (correctAnswers.includes(checkbox.value)) {{\n                if (checkbox.checked) {{\n                  if (numCorrect &gt;= 0) {{\n                    numCorrect++;\n                  }}\n                    label.style.color = \"green\";\n                }} else {{\n                    //label.style.color = \"red\";\n                }}\n            }} else if (checkbox.checked) {{\n                numCorrect = -9999;\n                label.style.color = \"red\";\n            }} else {{\n                //label.style.color = \"inherit\";\n            }}\n        }}\n\n        // Provide feedback\n        if (numCorrect === {len(correct_answers)}) {{\n            feedback.innerHTML = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.innerHTML = \"Incorrect.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n    &lt;/script&gt;\"\"\"\n\n    display(HTML(html_code))\nquestion = \"What is the final objective used to guide model training?\"\ncorrect_answers = [\"Minimize the difference between predicted and original distribution\"]\ndecoy_answers = [\"Predict the conditional probability\", \"Maximize the difference between predicted and original distribution\"]\ncreate_quiz(question, correct_answers, decoy_answers)\n\n\n \n    What is the final objective used to guide model training?\n        \n        \n        Maximize the difference between predicted and original distribution\n        \n        \n        \n        Minimize the difference between predicted and original distribution\n        \n        \n        \n        Predict the conditional probability\n        \n    Submit\n    \n    \n    \n\n\n\n\n 섹션 3: DDPM 구현 (모델 아키텍처) \n### \n\n 1. 시간 임베딩 (TIME EMBEDDINGS) \n\n앞서 언급했듯이 모델은 순차적으로 진행하지 않고도 특정 샘플의 모든 타임스텝에서 학습할 수 있습니다. 그러나 모델이 데이터가 어떤 타임스텝에 속하는지 아는 것은 중요합니다. 이를 구현하기 위해 SinusoidalTimeEmbeddings 클래스를 정의합니다. 시간 임베딩 클래스는 타임스텝(숫자)을 데이터와 동일한 차원의 잠재 공간 벡터로 변환합니다. MLP의 각 블록에서 이러한 임베딩이 모델에 추가됩니다. 이는 “Attention is All You Need” 논문(Vaswani et al., 2017)의 자기 어텐션 임베딩과 유사합니다. 임베딩을 통해 신경망은 배치의 모든 이미지에 대해 어떤 특정 타임스텝(노이즈 레벨)에서 작동하는지 “알” 수 있습니다.\n정방향(forward) 메서드는 시간(배치에 있는 여러 노이즈 이미지의 노이즈 레벨)을 나타내는 텐서를 입력으로 받아 모양 (batch_size, dim)의 시간 임베딩을 출력합니다. 시간 임베딩은 사인파 함수를 사용하여 계산되고 연결되어 최종 임베딩을 형성합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# The SinusoidalTimeEmbeddings module takes a tensor of shape (batch_size, 1) i.e. (64,1)\n# as input (i.e. the noise levels of several noisy images in a batch), and turns this\n# into a tensor of shape (batch_size, dim), with dim being the dimensionality of the\n# Time embeddings. This is then added to each residual block, as we will see further.\nclass SinusoidalTimeEmbeddings(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n\n    def forward(self, time):\n        device = time.device\n        # half of embedding dim\n        half_dim = self.dim // 2\n        # scaling factor for sinusoidal Time embeddings\n        embeddings = math.log(10000) / (half_dim - 1)\n        # generate sinusoidal Time embeddings\n        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n        # expand time tensor along second dimesion, multiply with embeddings\n        embeddings = time[:, None] * embeddings[None, :]\n        # concatenate sine and cosing embeddings along last dimension\n        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n        return embeddings\n\n\n### \n\n 2. 신경망 (NEURAL NET) \n\n이제 임베딩이 준비되었으므로 활성화 함수가 뒤따르는 여러 깊게 연결된 레이어로 구성된 간단한 데이터셋을 위한 다층 퍼셉트론(MLP) 모델을 사용합니다.\nDiffuser_MLP는 시간 \\(t\\)의 노이즈 상태 \\(x_t\\)에서 디노이징된 데이터 \\(x_0\\)를 예측합니다. 입력 크기(input_size), 레이어 사이에 적용될 활성화 함수(fun), 레이어 크기 목록(layers)을 받아 입력과 동일한 크기의 출력을 반환합니다.\n정방향 메서드는 입력 텐서 x와 time tensor를 받아 시간 텐서에 사인파 시간 임베딩을 적용하고 입력 텐서와 연결한 다음 MLP 레이어를 통해 공급합니다.\n\n\n\nimage.png\n\n\n\n\nCode\nclass Diffuser_MLP(nn.Module):\n  \"\"\"this model learns to predict the denoised data x_0 from the noised state x_t and the time t\"\"\"\n  def __init__(self, input_size, *, fun=None, layers=None):\n    \"\"\"fun is the activation function; layers is a list of integers, with the integer indicating the width of the embedding dimension as a factor of the input data size\"\"\"\n    super().__init__()\n\n    # dim of time embedding based on size of betas\n    self.time_embedding_dim = math.ceil( math.log2(betas.shape[0]) ) * 2\n    # initialize SinusoidalTimeEmbeddings\n    self.time_embedding_nn = SinusoidalTimeEmbeddings(self.time_embedding_dim)\n    # initialize sequential NN\n    self.net = torch.nn.Sequential()\n\n    # compute initial size of input to NN\n    prev_size = input_size + self.time_embedding_dim\n    # iterate through layers\n    for l in layers:\n      # compute layer size\n      new_size = int(input_size*l)\n      # add linear layer to NN\n      self.net.append( torch.nn.Linear(prev_size, new_size) )\n      # append activation function\n      self.net.append( fun() )\n      # update previous size for next layer\n      prev_size = new_size\n\n    # append linear layer to NN to map final size to input\n    self.net.append( torch.nn.Linear(prev_size, input_size) )\n\n  def forward(self, x, time):\n    # compute time embedding for given time\n    time_embedding = self.time_embedding_nn(time).squeeze(1)\n    # concatenate input data with time embeddings\n    x_t = torch.cat((x, time_embedding), -1)\n    # pass concatenated data through NN\n    output = self.net(x_t)\n    return output\n\n\n모델을 인스턴스화하고, 입력 데이터를 제공하고, 모델의 출력을 계산하고, 데이터 객체의 모양이 올바른지 확인해 봅시다.\n\n\nCode\n# instantiate\ndiff = Diffuser_MLP(input_size, fun=torch.nn.ReLU, layers=(3, 3) )\n\n# example time tensor by expanding single value along batch dimension to match 'example' shape\nexample_time = torch.Tensor([0]).expand(example.shape[0], 1)\n\n# pass exmaple data and exmaple_time through diffuser NN to get output\noutput = diff(example, example_time)\n\n# shape of input data, example_time, and output tensor\nexample.shape, example_time.shape, output.shape\n\n\n(torch.Size([64, 32]), torch.Size([64, 1]), torch.Size([64, 32]))\n\n\n### \n\n 3. 학습 (TRAINING) \n\n학습할 시간입니다! 각 에포크 내에서 데이터 로더를 반복하여 데이터 배치를 처리합니다. 각 배치에 대해 무작위 타임스텝이 선택되고 이전에 정의한 코사인 스케줄러에 따라 노이즈 데이터가 생성됩니다. 모델은 이 노이즈 입력 데이터를 해당 시간 정보와 함께 처리하고 출력을 계산합니다. 그런 다음 모델의 출력과 노이즈 데이터 간에 평균 제곱 오차 손실이 계산됩니다. 그 후 그래디언트는 0이 되고, 역전파가 실행되어 옵티마이저를 사용하여 모델의 파라미터를 업데이트합니다. 학습 과정 전반에 걸쳐 로깅 목적으로 총 손실이 누적되고 단계별 평균 손실이 계산됩니다.\nAdam 옵티마이저는 확산 모델의 파라미터를 최적화하는 데 사용됩니다. ExponentialLR 스케줄러는 학습 중에 학습률을 조절하여 수렴을 용이하게 하는 적응형 조정을 제공하는 데 활용됩니다. 기본 학습 루프에서 모델은 여러 에포크에 걸쳐 학습되며, 각 에포크는 일련의 학습 단계로 구성됩니다. 각 에포크가 완료되면 스케줄러를 사용하여 학습률이 업데이트됩니다.\n\n\nCode\ndef train(model, optimizer, *, n_steps):\n    # Set the model in training mode\n    model.train()\n\n    # Define the loss function as Mean Squared Error\n    loss_fn = nn.MSELoss()\n\n    # Iterate through training steps\n    for epoch in range(n_steps):\n        total_loss = 0\n        total_loss_count = 0\n\n        # Iterate through data batches\n        for batch in data_loader:\n            # Randomly select a time\n            t = random.randrange(0, betas.shape[0])\n\n            # Generate noised data for the selected time step\n            input, noise = noised_data(batch, t)\n\n            # Pass the input data and time to the model to get the output\n            output = model(input, torch.Tensor([t]).expand(input.shape[0], 1))\n\n            # Compute the loss between the output and the noise\n            loss = loss_fn(output, noise)\n\n            # Compute the mean loss\n            loss = loss.mean()\n\n            # Reset gradients\n            optimizer.zero_grad(set_to_none=True)\n\n            # Perform backpropagation\n            loss.backward()\n\n            # Update model parameters\n            optimizer.step()\n\n            # Accumulate total loss\n            total_loss += loss\n            total_loss_count += 1\n\n    # Compute the total loss average over the training batch\n    total_loss = total_loss / total_loss_count\n\n    # Append the total loss to historical_total_loss list\n    with torch.no_grad():\n        historical_total_loss.append(total_loss.item())\n\n    # Return the last historical total loss\n    return historical_total_loss[-1]\n\n# Initialize Adam optimizer for the Diffuser_MLP model parameters with learning rate 1e-3\noptimizer = torch.optim.Adam(diff.parameters(), lr=1e-3)\n\n# Initialize ExponentialLR scheduler for the optimizer with gamma 0.9\nlr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n# Iterate training for 8 epochs\nfor i in range(8):\n    # Perform training for 32 steps\n    total_loss = train(diff, optimizer, n_steps=32)\n\n    # Adjust learning rate using lr_scheduler\n    lr_scheduler.step()\n\n    # Clear previous plot and plot historical total losses\n    pl.clf()\n    plt.yscale('log')\n    plt.ylabel(\"loss\")\n    plt.xlabel(\"epoch\")\n    plt.plot(historical_total_loss[:])\n    display(pl.gcf())\n\n    # Calculate mean loss over the last 4 epochs\n    mean_loss = torch.Tensor(historical_total_loss[-4:]).mean().item()\n\n    # Extract the last 4 losses\n    last_losses = historical_total_loss[-4:]\n\n    # Print the last 4 losses and the mean loss\n    print(f'{last_losses=}\\n{mean_loss=}')\n\n    # Clear output for the next iteration\n    clear_output(wait=True)\n\n# Print the last 4 losses and the mean loss after completing all epochs\nprint(f'{last_losses=}\\n{mean_loss=}')\n\n\nlast_losses=[0.1390904188156128, 0.11419769376516342, 0.12434312701225281, 0.12849435210227966]\nmean_loss=0.12653139233589172\n\n\n\n\n\n\n\n\n\n### \n\n 4. 생성 (GENERATION) \n\n이제 학습된 확산 모델에서 샘플링하여 합성 이미지를 생성할 수 있습니다. 최종 타임스텝에서 시작하여 시간 역순으로 진행하면서 확산 모델에서 반복적으로 샘플링할 것입니다. 각 샘플은 모델의 평균 출력을 기반으로 생성되며 선택적으로 가우스 노이즈로 증강됩니다.\n진행하기 전에 이해해야 할 마지막 개념이 하나 있습니다 - 사후 분산(Posterior Variance)\n샘플링 단계에서 목표는 학습된 모델에서 학습 데이터와 유사한 새 데이터를 생성하는 것입니다. 사후 분산은 데이터 생성 프로세스에 내재된 불확실성을 관리하고 현실적이고 다양한 출력을 달성하기 위해 각 단계에서 추가하거나 제거할 노이즈의 양을 안내하는 데 도움이 됩니다.\n예를 들어 이미지를 생성하는 데 사용되는 확산 모델은 이미지 분포의 평균과 데이터의 분산을 모두 학습합니다. 모델이 이미지 분포의 평균만 학습하고 분산을 무시하면 서로 너무 유사하고 다양성이 부족한 이미지를 생성할 수 있습니다. 사후 분산을 통합함으로써 모델은 미묘한 차이가 있는 다양한 이미지를 생성하여 데이터셋에 존재하는 실제 변동성을 반영할 수 있습니다.\n사후 분산은 다음 공식으로 주어집니다.\n\n여기서 \\(\\bar{\\mu_t}, \\bar{\\beta_t}\\)는 누적 곱에 따라 달라집니다.\n\n\nCode\nposterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\nplt.plot(posterior_variance)\nplt.xlabel(\"timestep\");\nplt.title(\"Posterior Variance\");\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Decorator to temporarily disable gradient computation\n@torch.no_grad()\ndef sample(model, x, t, t_index):\n    # Extract necessary parameters for the current time step\n    betas_t = betas[t]\n    sqrt_one_minus_alphas_cumprod_t = sqrt_one_minus_alphas_cumprod[t]\n    sqrt_recip_alphas_t = sqrt_recip_alphas[t]\n\n    # Compute the mean of the model's output\n    model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t)\n\n    # If it's the first time step, return the model mean\n    if t_index == 0:\n        return model_mean\n    else:\n        # Extract posterior variance for the current time step\n        posterior_variance_t = posterior_variance[t]\n\n        # Generate Gaussian noise\n        noise = torch.randn_like(x)\n\n        # Return the sample by adding noise scaled by the square root of the posterior variance to the model mean\n        return model_mean + torch.sqrt(posterior_variance_t) * noise\n\n# Generate random input tensor 'x'\nx = torch.randn((input_size,)).unsqueeze(0)\n\n# Iterate through time steps in reverse order\nfor t in torch.arange(betas.shape[0]-1, -1, -1):\n    # Clear the previous plot\n    pl.clf()\n    if torch.remainder(t,2) == 0:\n      # Plot the current state of 'x' without gradients\n      plt.plot(x[0].detach())\n      plt.text(0, 1.25, 'time $t = $'+str(t.item()), fontsize = 22)\n      # plt.annotate(f'{t}', xy=(0,1))\n      # Display the plot\n      display(pl.gcf())\n\n      # Pause for a short duration\n      time.sleep(.02)\n\n      # Clear the output for the next iteration\n      clear_output(wait=True)\n\n    # Expand 't' tensor to match the shape (1, 1)\n    T = torch.Tensor([t]).to(torch.long).expand(1, 1)\n\n    # Sample a new 'x' using the model and the current time step\n    x = sample(diff, x, T, t)\n\n\n\n\n\n\n\n\n\n\n\n\n  섹션 4: 요약 및 복습 질문\n확산 모델은 생성 모델링에 강력하고 유연한 접근 방식을 제공하며, 특히 고품질의 다양한 샘플이 필요한 시나리오에서 그렇습니다. 이 워크숍에서는 다음 주요 영역에서 디노이징 확산 확률 모델(DDPM)의 개념과 구현을 살펴보았습니다:\n 1. 확산 모델 이해하기 \n- 정방향 프로세스: 정방향 프로세스는 데이터에 점진적으로 노이즈를 추가하여 여러 단계에 걸쳐 순수한 노이즈로 변환하는 것을 포함합니다. - 역방향 프로세스: 역방향 프로세스는 노이즈 데이터를 단계별로 디노이징하여 원본 데이터를 재구성하거나 새 샘플을 생성하는 것을 목표로 합니다.\n 2. 수학적 기초  - 확산 방정식: 노이즈 스케줄을 포함하여 정방향 및 역방향 확산 프로세스를 지배하는 수학적 방정식이 설명되었습니다. -손실 항: 누적 손실 및 ELBO가 소개되었습니다. - 사후 분산: 샘플링 프로세스 중 다양성과 안정성을 보장하는 데 있어 사후 분산의 역할이 논의되었습니다.\n이제 확산 모델의 논리, 수학 및 응용에 대해 잘 이해했을 것입니다.\n 다음 질문에 답하여 지식을 테스트해 보세요!\n\n확산 모델에서 정방향 프로세스 q의 목적은 무엇입니까? 2.정방향 프로세스에서 분산 스케줄 \\(β_t\\)의 중요성은 무엇입니까?\n선형 노이즈 스케줄과 코사인 노이즈 스케줄의 차이점을 설명하세요.\n확산 모델에서 역방향 프로세스의 목표는 무엇입니까?\n역방향 프로세스에서 조건부 분포 \\(p(x_{t−1}∣x_t)\\)가 중요한 이유는 무엇입니까?\n실제로 실제 조건부 분포 \\(p(x_{t−1}∣x_t)\\)를 직접 사용할 수 없는 이유는 무엇입니까?\n실제로 조건부 분포 \\(p(x_{t−1}∣x_t)\\)를 어떻게 근사합니까?\n\n\n\nCode\n# @markdown &lt;font color='#e59454'&gt; &lt;b&gt; Run cell for all answers!&lt;/b&gt;&lt;/font&gt;\n\n\nprint(\"1. The forward process q gradually adds noise to the data over a number of timesteps until we end up with pure Gaussian noise. \\n The purpose of this process is to transform the original data x_0 into increasingly noisy versions x_t​ until it becomes pure Gaussian noise at the final timestep T\")\nprint(\"2. βt controls the amount of noise added at each timestep. It starts with a small value and gradually increases such that the data becomes more noisy over time. \\nThe final timestep T has the highest noise level, ensuring the data has been transformed into pure Gaussian noise. The schedule is designed to prevent the variance from exploding while ensuring the gradual transformation.\")\nprint(\"3. A linear noise schedule increases the noise linearly over time: the original image information is quickly lost after about half of the total steps.\\n A cosine noise schedule, on the other hand, adds noise more slowly at the beginning and increases it more rapidly towards the end. This helps in preserving the information in the earlier timesteps, making the denoising process more efficient and retaining more of the original data information.\")\nprint(\"4. To generate the original image from its noised version. This involves starting with Gaussian noise and gradually denoising it to produce a sample from the real data distribution.\")\nprint(\"5. p(xt−1∣xt) is important because it describes how to transition from the noised version at time t to a slightly less noised version at time t−1.\\n Knowing this distribution allows us to iteratively denoise an image.\")\nprint(\"6. We can't directly use p(xt−1∣xt) because it is intractable. Calculating this distribution would require knowing the distribution of all possible samples, which is computationally infeasible.\")\nprint(\"7. We approximate p(xt−1∣xt) using a neural network. The neural network learns to predict the conditional probability distribution, denoted as pθ(xt−1∣xt), \\n where θ represents the parameters of the neural network.\")\n\n\n\n\n1. The forward process q gradually adds noise to the data over a number of timesteps until we end up with pure Gaussian noise. \n The purpose of this process is to transform the original data x_0 into increasingly noisy versions x_t​ until it becomes pure Gaussian noise at the final timestep T\n2. βt controls the amount of noise added at each timestep. It starts with a small value and gradually increases such that the data becomes more noisy over time. \nThe final timestep T has the highest noise level, ensuring the data has been transformed into pure Gaussian noise. The schedule is designed to prevent the variance from exploding while ensuring the gradual transformation.\n3. A linear noise schedule increases the noise linearly over time: the original image information is quickly lost after about half of the total steps.\n A cosine noise schedule, on the other hand, adds noise more slowly at the beginning and increases it more rapidly towards the end. This helps in preserving the information in the earlier timesteps, making the denoising process more efficient and retaining more of the original data information.\n4. To generate the original image from its noised version. This involves starting with Gaussian noise and gradually denoising it to produce a sample from the real data distribution.\n5. p(xt−1∣xt) is important because it describes how to transition from the noised version at time t to a slightly less noised version at time t−1.\n Knowing this distribution allows us to iteratively denoise an image.\n6. We can't directly use p(xt−1∣xt) because it is intractable. Calculating this distribution would require knowing the distribution of all possible samples, which is computationally infeasible.\n7. We approximate p(xt−1∣xt) using a neural network. The neural network learns to predict the conditional probability distribution, denoted as pθ(xt−1∣xt), \n where θ represents the parameters of the neural network."
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html",
    "title": "  섹션 1: NumPy에서의 합성곱",
    "section": "",
    "text": "##홈페이지로 돌아가기\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\n CNN을 식별하고 정의하기 \n합성곱 레이어에서 수행되는 합성곱 연산 탐구하기\nCNN이 입력 이미지에서 특징을 학습하고 표현하는 방법 관찰하기\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n 목차 \n섹션 1: Numpy에서의 합성곱\n섹션 2: 합성곱 신경망 학습시키기 * 모델 정의하기 * 데이터셋 생성하기 * 모델 인스턴스화하기 * 학습하기\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d\nfrom scipy import signal as sg\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, transforms\n\n\n\n\n\n\n신경망(NN)은 고도로 비선형적인 함수 근사를 위한 계산 단위 또는 “뉴런”의 계층적 네트워크라는 것을 상기해 보세요. 각 뉴런의 입력은 가중된 업스트림 신호의 집합입니다. 뉴런은 이 가중된 입력 집합을 선형 결합으로 처리하고 선택적으로 활성화 함수를 통해 비선형성을 적용합니다.\n이전 워크숍과 같은 Dense 레이어(완전 연결 층)에서는 현재 레이어의 각 뉴런이 이전 레이어의 모든 뉴런에 연결되며 모든 연결에는 고유한 가중치가 있습니다. 이러한 레이어는 메모리와 계산 측면에서 비용이 많이 듭니다. 반면, 합성곱 레이어(Convolutional layer)에서는 각 뉴런이 이전 레이어의 가까운 몇 개의 로컬 뉴런에만 연결되며 동일한 가중치 세트가 이들을 연결하는 데 사용됩니다. 이 특별한 구조 덕분에 CNN은 이미지의 시각적 패턴을 효율적이고 강력하게 감지할 수 있습니다.\n\n\n\nimage.png\n\n\n합성곱(convolutional)이라는 단어는 이러한 유형의 네트워크에서 발생하는 세분성 감소 프로세스를 나타냅니다. 이미지의 경우 합성곱 신경망 내부를 들여다보면 슬라이딩 윈도우(커널이라고 함)가 이미지의 각 픽셀을 훑고 지나가며 원본보다 흐릿한 버전을 생성하는 것을 볼 수 있습니다. 전문 용어로 오른쪽 이미지는 원본 이미지의 합성곱이라고 말합니다.\n\n\n\nimage.png\n\n\n먼저 1차원 합성곱 연산부터 시작해 보겠습니다. 행렬에 커널을 적용하는 방법은 여러 가지가 있습니다: 패딩 포함(full), 패딩 포함(same), 패딩 없음(valid). 이 예제에서는 패딩 없음(valid)으로 합성곱을 수행합니다.\n이미지 \\(x\\)와 커널 \\(k\\)가 주어졌을 때, 합성곱된 이미지 \\(y\\)의 값은 다음과 같습니다:\n\\[y(t) = \\sum\\limits_{j=0}^{K-1} k[j] \\cdot x[t - j] \\]\n\n\\(t\\)는 \\(y\\)의 위치 인덱스입니다\n\\(j\\)는 커널 \\(k\\)의 위치 인덱스입니다\n\\(K\\)는 커널 배열의 길이입니다\n\n합성곱 연산은 커널을 입력 이미지 위로 슬라이딩하고 각 지점에서 요소별 곱셈과 합계를 수행하여 출력 이미지 \\(y\\)를 계산합니다.\n\n\nCode\nk = [6, 2]\nk_inv = k[::-1] # kernel inverted\n\nx = [1, 2, 5, 4]\n\n\n\n\nCode\nk_inv\n\n\n[2, 6]\n\n\n\n\nCode\ny1 = k_inv[0] * x[0] + k_inv[1] * x[1]\ny2 = k_inv[0] * x[1] + k_inv[1] * x[2]\ny3 = k_inv[0] * x[2] + k_inv[1] * x[3]\ny1, y2, y3\n\n\n(14, 34, 34)\n\n\n아래의 numpy 재구현 명령을 보면, 합성곱 연산은 요소별로 곱하고 합산하기 전에 내부적으로 커널 k를 반전시킵니다. 이 반전을 수행하는 데에는 몇 가지 흥미로운 이유가 있지만, 요약하자면 요소별 곱셈을 수행하기 전에 먼저 정의에 따라 커널을 반전시킨다는 것입니다(그렇지 않으면 반전 없이는 교차 상관(cross-correlation)이라는 연산을 수행하게 됩니다).\n\n\nCode\n# in numpy\ny = np.convolve(k, x, \"valid\")\ny\n\n\narray([14, 34, 34])\n\n\n이제 이미지 픽셀의 5x5 행렬을 고려해 보겠습니다.\n\n각 픽셀을 이진 값(1 = 흰색 픽셀, 0 = 검은색 픽셀)으로 변환할 수 있습니다. 합성곱된 행렬(특징 맵)을 생성하려면 커널이라고도 하는 슬라이딩 윈도우가 필요합니다. 아래 애니메이션에서는 3x3 필터를 사용합니다. 필터의 값은 원본 행렬(입력 이미지)과 요소별로 곱해진 다음 합산됩니다. 이 과정은 필터를 전체 원본 행렬 위로 슬라이딩하여 각 요소에 대해 반복되어 최종적으로 전체 합성곱된 특징 행렬을 생성합니다.\n\n2D 합성곱은 앞서 수행한 1D 연산과 유사하며, 입력 이미지 \\(I\\)의 각 부분과 커널 \\(K\\)의 내적을 취합니다.\n\\[y[i, j] = \\sum_{u=0}^{p-1} \\sum_{v=0}^{q-1} K[u, v] \\cdot I[i - u, j - v]\\]\n\n\\(i\\)와 \\(j\\)는 합성곱된 이미지 \\(y\\)의 행과 열 인덱스입니다\n\\(u\\)와 \\(v\\)는 커널 \\(K\\)의 행과 열 인덱스입니다\n\n아래에서 합성곱 애니메이션을 재현해 보겠습니다.\n\n\nCode\n# 2D image\nI = [[1, 1, 1, 0, 0],\n     [0, 1, 1, 1, 0],\n     [0, 0, 1, 1, 1],\n     [0, 0, 1, 1, 0],\n     [0, 1, 1, 0, 0]]\n\n# 2D kernel\ng = [[1, 0, 1],\n     [0, 1, 0],\n     [1, 0, 1]]\n\nsg.convolve(g, I, \"valid\")\n\n\narray([[4, 3, 4],\n       [2, 4, 3],\n       [2, 3, 4]])\n\n\n\n\nCode\n# let's expand the first two operations for clarity\n# y[0][0]\ny1 = g[0][0] * I[0][0] + g[0][1] * I[0][1] + g[0][2] * I[0][2]  + \\\n     g[1][0] * I[1][0] + g[1][1] * I[1][1] + g[1][2] * I[1][2]  + \\\n     g[2][0] * I[2][0] + g[2][1] * I[2][1] + g[2][2] * I[2][2]\n\n# y[0][1]\ny2 = g[0][0] * I[0][1] + g[0][1] * I[0][2] + g[0][2] * I[0][3]  + \\\n     g[1][0] * I[1][1] + g[1][1] * I[1][2] + g[1][2] * I[1][3]  + \\\n     g[2][0] * I[2][1] + g[2][1] * I[2][2] + g[2][2] * I[2][3]\n\ny1, y2 # the first two values in the convolved feature matrix\n\n\n(4, 3)\n\n\n합성곱 신경망을 학습시키기 전에 다른 이미지, 손으로 쓴 숫자의 흐릿한 버전을 합성곱하는 예시를 살펴보겠습니다.\n\n\nCode\n# Function to perform convolution on an image with a given kernel\ndef convolve_image(image, kernel):\n    # Convolve the image with the kernel\n    convolved_image = convolve2d(image, kernel, mode='same', boundary='wrap')\n    return convolved_image\n\n# Function to display original and convolved images\ndef show_images(original_image, convolved_image):\n    plt.figure(figsize=(10, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_image, cmap='gray')\n    plt.title('Original Image')\n    plt.axis('off')\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(convolved_image, cmap='gray')\n    plt.title('Convolved Image')\n    plt.axis('off')\n\n    plt.show()\n\n# Load the MNIST dataset\ntransform = transforms.Compose([transforms.ToTensor()])\nmnist_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# Select a random MNIST image\nidx = np.random.randint(len(mnist_dataset))\nimage, label = mnist_dataset[idx]\n\n# Convert image to numpy array\nimage_np = image.squeeze().numpy()\n\n# Example kernel\nkernel = np.array([[0, -1, 0],\n                   [-1, 5, -1],\n                   [0, -1, 0]])\n\n# Perform convolution\nconvolved_image = convolve_image(image_np, kernel)\n\n# Crop the convolved image to match original dimensions\nconvolved_image_cropped = convolved_image[:image_np.shape[0], :image_np.shape[1]]\n\n# Show original and convolved images\nshow_images(image_np, convolved_image_cropped)\n\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|██████████| 9912422/9912422 [00:00&lt;00:00, 101737923.78it/s]\n\n\nExtracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|██████████| 28881/28881 [00:00&lt;00:00, 99536313.74it/s]\n\n\nExtracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n\n100%|██████████| 1648877/1648877 [00:00&lt;00:00, 54088292.91it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████| 4542/4542 [00:00&lt;00:00, 16681723.96it/s]\n\n\nExtracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    # random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n\nquestion = \"What is the purpose of the convolutional layer in a CNN?\"\ncorrect_answer = \"To extract features from the input images by applying filters that detect edges, textures, and shapes.\"\ndecoy_answers = [\n    \"To randomly initialize the weights of the model.\",\n    \"To combine different layers of the CNN into one.\"\n]\nexplanation = \"The convolutional layer applies filters to input images, helping the model detect low-level features like edges and textures, and higher-level features in deeper layers.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To extract features from the input images by applying filters that detect edges, textures, and shapes.\n        \n        \n        \n          \n          To randomly initialize the weights of the model.\n        \n        \n        \n          \n          To combine different layers of the CNN into one.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The convolutional layer applies filters to input images, helping the model detect low-level features like edges and textures, and higher-level features in deeper layers.\n      \n    \n    \n    \n\n\n\n\n\n\n\n\n\nCode\n# dense neural network\nclass DenseModel(nn.Module):\n    def __init__(self):\n        super(DenseModel, self).__init__()\n\n        # Fully connected layers\n        self.d1 = nn.Linear(28 * 28, 128)\n        self.d2 = nn.Linear(128, 128)  # Added another fully connected layer\n        self.d3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Flatten input\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.d1(x)\n        x = F.relu(x)\n        x = self.d2(x)  # Added another fully connected layer\n        x = F.relu(x)\n        x = self.d3(x)\n\n        # Softmax activation\n        out = F.softmax(x, dim=1)\n        return out\n\n# convolutional neural network\nclass ConvModel(nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n\n        # 28x28x1 =&gt; 26x26x32\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.d1 = nn.Linear(26 * 26 * 32, 128)\n        self.d2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # 32x1x28x28 =&gt; 32x32x26x26\n        x = self.conv1(x)\n        x = F.relu(x)\n\n        # flatten =&gt; 32 x (32*26*26)\n        x = x.flatten(start_dim = 1)\n\n        # 32 x (32*26*26) =&gt; 32x128\n        x = self.d1(x)\n        x = F.relu(x)\n\n        # logits =&gt; 32x10\n        logits = self.d2(x)\n        out = F.softmax(logits, dim=1)\n        return out\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What advantage does a CNN with multiple convolutional layers have?\"\ncorrect_answer = \"It allows the network to capture progressively complex features at each layer.\"\ndecoy_answers = [\n    \"It helps downsample the input, reducing the number of computations required.\",\n    \"It makes the model learn features in smaller image segments.\",\n    \"It ensures each layer has identical filters for uniform feature extraction.\"\n]\nexplanation = \"Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It allows the network to capture progressively complex features at each layer.\n        \n        \n        \n          \n          It helps downsample the input, reducing the number of computations required.\n        \n        \n        \n          \n          It makes the model learn features in smaller image segments.\n        \n        \n        \n          \n          It ensures each layer has identical filters for uniform feature extraction.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What effect does a higher learning rate have on the training process?\"\ncorrect_answer = \"It increases the step size of each update, potentially speeding up convergence but risking instability.\"\ndecoy_answers = [\n    \"It decreases the step size, improving stability and precision.\",\n    \"It has no impact on training speed or convergence.\",\n    \"It reduces model accuracy by adding noise to gradients.\"\n]\nexplanation = \"A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It increases the step size of each update, potentially speeding up convergence but risking instability.\n        \n        \n        \n          \n          It decreases the step size, improving stability and precision.\n        \n        \n        \n          \n          It has no impact on training speed or convergence.\n        \n        \n        \n          \n          It reduces model accuracy by adding noise to gradients.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the purpose of the final fully connected layer in a CNN?\"\ncorrect_answer = \"It combines features from previous layers to make final predictions.\"\ndecoy_answers = [\n    \"It normalizes feature maps for the final output.\",\n    \"It helps detect edges within the feature maps.\",\n    \"It increases the spatial resolution of the output.\"\n]\nexplanation = \"The fully connected layer aggregates features extracted by previous layers to make classification predictions.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It combines features from previous layers to make final predictions.\n        \n        \n        \n          \n          It normalizes feature maps for the final output.\n        \n        \n        \n          \n          It helps detect edges within the feature maps.\n        \n        \n        \n          \n          It increases the spatial resolution of the output.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The fully connected layer aggregates features extracted by previous layers to make classification predictions.\n      \n    \n    \n    \n\n\n\n\n\nPyTorch에서 데이터셋을 준비하기 위해 DataLoader를 사용할 것입니다. PyTorch DataLoader는 학습 또는 테스트 과정 중에 데이터셋을 효율적으로 로드하고 반복하는 데 도움이 되는 유틸리티입니다. 여기서 데이터 로드를 위한 배치 크기를 정의하는데, 이는 학습 또는 테스트의 각 반복 중에 함께 처리되는 샘플 수를 결정합니다. 또한 transforms.Compose를 사용하여 변환 파이프라인을 정의합니다. 이 경우 PIL 이미지 또는 numpy 배열을 PyTorch 텐서로 변환하는 transforms.ToTensor()라는 하나의 변환만 있습니다.\n그런 다음 학습 데이터셋(trainset)을 위한 DataLoader를 생성합니다. 배치별로 로드할 샘플 수를 결정하는 배치 크기(batch_size=BATCH_SIZE)를 지정합니다. shuffle=True로 설정하면 매 에포크마다 데이터를 섞어 학습 중 샘플의 순서를 무작위화하여 모델이 데이터 순서에 기반한 잘못된 패턴을 학습하는 것을 방지합니다. num_workers=2는 데이터 로드에 사용할 서브프로세스 수를 나타냅니다. 이는 데이터 로드 작업을 병렬화하여 데이터 로드 프로세스 속도를 높입니다.\n여기서는 기계 학습 및 컴퓨터 비전 분야에서 널리 사용되는 데이터셋인 MNIST 데이터셋을 사용할 것입니다. 이 데이터셋은 손으로 쓴 숫자(0-9)의 28x28 픽셀 회색조 이미지 모음과 해당 숫자를 나타내는 레이블로 구성되어 있습니다. 각 이미지에는 포함된 숫자가 레이블로 지정되어 있습니다.\n\n\nCode\n%%capture\nBATCH_SIZE = 32\n\n## transformations\ntransform = transforms.Compose(\n    [transforms.ToTensor()])\n\n## download and load training dataset\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\n## download and load testing dataset\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n                                         shuffle=False, num_workers=2)\n\n\n\n데이터셋의 데이터 포인트를 시각화할 수 있습니다.\n\n\nCode\n## functions to show an image\ndef imshow(img):\n    #img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n## get some random training images\ndataiter = iter(trainloader)\n\n## show images\nfor i in range(1):  # Assuming you want to show 4 images\n    images, labels = next(dataiter)\n    imshow(torchvision.utils.make_grid(images))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the dimensions\nfor images, labels in trainloader:\n    print(\"Image batch dimensions:\", images.shape)\n    print(\"Image label dimensions:\", labels.shape)\n    break\n\n\nImage batch dimensions: torch.Size([32, 1, 28, 28])\nImage label dimensions: torch.Size([32])\n\n\n\n\n\n\n\nCode\n## test the model with 1 batch\nmodel = ConvModel()\nfor images, labels in trainloader:\n    print(\"batch size:\", images.shape)\n    out = model(images)\n    print(out.shape)\n    break\n\n\nbatch size: torch.Size([32, 1, 28, 28])\ntorch.Size([32, 10])\n\n\n\n\n\n\n\nCode\nlearning_rate = 0.001\nnum_epochs = 5\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvModel()\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n\nCode\n## compute accuracy\ndef get_accuracy(logit, target, batch_size):\n    ''' Obtain accuracy for training round '''\n    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n    accuracy = 100.0 * corrects/batch_size\n    return accuracy.item()\n\n\n\n\nCode\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n\n\nEpoch: 0 | Loss: 1.5820 | Train Accuracy: 88.22\nEpoch: 1 | Loss: 1.4914 | Train Accuracy: 97.30\nEpoch: 2 | Loss: 1.4812 | Train Accuracy: 98.25\nEpoch: 3 | Loss: 1.4766 | Train Accuracy: 98.66\nEpoch: 4 | Loss: 1.4742 | Train Accuracy: 98.88\n\n\n\n\nCode\ntest_acc = 0.0\nfor i, (images, labels) in enumerate(testloader, 0):\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n\nprint('Test Accuracy: %.2f'%( test_acc/i))\n\n\nTest Accuracy: 98.37\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the backpropagation process in CNNs, what is being updated?\"\ncorrect_answer = \"The weights of the filters are updated to minimize the loss.\"\ndecoy_answers = [\n    \"The pixel values of the input images are adjusted to fit the model.\",\n    \"The number of filters in each layer is modified to improve accuracy.\",\n    \"The dimensions of the feature maps are changed.\"\n]\nexplanation = \"During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          The weights of the filters are updated to minimize the loss.\n        \n        \n        \n          \n          The pixel values of the input images are adjusted to fit the model.\n        \n        \n        \n          \n          The number of filters in each layer is modified to improve accuracy.\n        \n        \n        \n          \n          The dimensions of the feature maps are changed.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the training loop, why is it important to zero out gradients using `optimizer.zero_grad()`?\"\ncorrect_answer = \"To prevent gradients from accumulating across batches, ensuring correct updates to the model.\"\ndecoy_answers = [\n    \"To reset the weights of the model after each epoch.\",\n    \"To clear the model parameters so they can be re-initialized.\",\n    \"To reduce the learning rate with each batch.\"\n]\nexplanation = \"Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To prevent gradients from accumulating across batches, ensuring correct updates to the model.\n        \n        \n        \n          \n          To reset the weights of the model after each epoch.\n        \n        \n        \n          \n          To clear the model parameters so they can be re-initialized.\n        \n        \n        \n          \n          To reduce the learning rate with each batch.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\n      \n    \n    \n    \n\n\n데이터셋에서 이미지를 선택하여 학습된 모델의 합성곱 레이어에 통과시켜 봅시다. 입력 이미지에 반응하여 이 합성곱 레이어에서 생성된 특징 맵을 추출하고 시각화할 수 있습니다. 이 시각화는 CNN이 가장자리(edges)나 질감(textures)과 같은 어떤 종류의 특징을 감지하도록 학습하는지 이해하는 데 도움이 됩니다.\n이 커널이 포착하는 정보의 유형은 학습 과정 중에 학습된 가중치에 따라 달라집니다. 그러나 3x3 커널 크기의 합성곱 연산은 일반적으로 입력 이미지의 로컬 공간 패턴이나 특징을 포착합니다. 이러한 패턴에는 이미지에 존재하는 가장자리, 모서리, 질감 또는 기타 시각적 속성이 포함될 수 있습니다. 32개의 출력 채널이 있으므로 출력 특징 맵의 각 채널은 이러한 로컬 패턴의 다양한 측면이나 변형을 포착하여 이미지 콘텐츠에 대한 더 풍부한 표현을 제공합니다.\n\n\nCode\n# Access the first image from the training set\nimage, _ = trainset[0]\n\n# Plot the image\nplt.imshow(image.squeeze(), cmap='gray')\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Choose an image from the dataset\nimage, _ = trainset[0]  # Choose any image from the dataset\n\n# Convert the image to a tensor and move it to the appropriate device\nimage = image.unsqueeze(0).to(device)\n\n# Get the feature maps from a specific convolutional layer\nconv_layer = model.conv1  # Choose the convolutional layer you want to visualize\nactivation = conv_layer(image)\n\n# Plot the feature maps\nplt.figure(figsize=(10, 10))\nfor i in range(activation.size(1)):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(activation.squeeze().detach().cpu().numpy()[i], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n이미지 크레딧 * 그림 2: 이미지 합성곱 https://gregorygundersen.com/blog/2017/02/24/cnns/ * 그림 3: 5x5 픽셀 이미지 https://ibm.box.com/shared/static/0s5v7doe2p5xuzifs47bxmmuwrn3kra2.bmp * 그림 4: 이미지를 훑는 커널 애니메이션 https://ibm.box.com/shared/static/fvutcm8jwa5j2o7xv2zzqyz2yu3zwhz4.gif\nColab에 wandb(weights and biases) 모듈을 추가하여 학습을 실시간으로 효율적으로 추적하세요!\nwandb 웹사이트에 계정을 만들어 모든 프로젝트를 기록 및 액세스하고, 프로젝트 보고서를 생성하고, 하이퍼파라미터 검색을 실행하는 등의 작업을 수행하세요.\nWandb는 아키텍처 성능을 추적하는 데 널리 사용되는 로깅 모듈이며 사용법을 배울 가치가 있는 도구입니다.\n여기에서 계정(학생 JHU 계정 사용 가능)을 만드세요: https://wandb.auth0.com/login?state=hKFo2SBFTXR0eVRGQ3F2V1ZPQ3hfZ29seTVSdzhzcVkxWEVzN6FupWxvZ2luo3RpZNkgdjVEMjdCdVc4ZzY1UHBPLVRhakt3NnN5NVZ2b25YU0qjY2lk2SBWU001N1VDd1Q5d2JHU3hLdEVER1FISUtBQkhwcHpJdw&client=VSM57UCwT9wbGSxKtEDGQHIKABHppzIw&protocol=oauth2&nonce=aFV4UGtDY0gtfmphLUxtbw%3D%3D&redirect_uri=https%3A%2F%2Fapi.wandb.ai%2Foidc%2Fcallback&response_mode=form_post&response_type=id_token&scope=openid%20profile%20email&signup=true\nColab에서 wandb를 사용하여 모델 추적을 시작하려면 아래의 간단한 단계를 따르세요. 여기서는 최소한의 도구로 학습 데이터만 추적하겠지만, 이 심층 튜토리얼을 살펴보세요: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=TxUlHUj52t-d\n\n\nCode\nimport random\n\n\n\n\nCode\n!pip install wandb -qqq\nimport wandb\nwandb.login()\n\n#initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"pytorch-intro\",\n        config={\n            \"epochs\": 10,\n            \"batch_size\": 128,\n            \"lr\": 1e-3,\n            \"dropout\": random.uniform(0.01, 0.80),\n            }) #feel free to add other metadata like what optimizers you used etc.\n\n\n#Put the following line in both your training and validation loops so you can track how performance changes in both scenario\n#You can change variable names such as frm 'train/loss' to 'test/loss', 'accuracy', etc. and the corresponding variables will be plotted in wandb.\n#Make sure your information is put in a dictionary, or else you'll have an error!\nwandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n#After your training loop, run\nwandb.finish()\n\n\n\n\nCode\n#Putting it all together:\n\n#🐝  import the module in & login to your account\n!pip install wandb -qqq\nimport wandb\nwandb.login() #Go to bottom of cell and followw instructions. It will ask for an API from a given link that's connected to your account. Make sure you're logged into wandb before running this cell!\n\n#🐝 initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"CNN_Classifier\",\n        config={\n            \"epochs\": num_epochs,\n            \"lr\":learning_rate,\n            })\n\n#Setup training loop:\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        #🐝 insert training logging step here:\n        wandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n    #🐝 If we did a validation loop, we'd insert the logging step here:\n\n#🐝 finish wandb run\nwandb.finish()"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#모델-정의하기",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#모델-정의하기",
    "title": "  섹션 1: NumPy에서의 합성곱",
    "section": "",
    "text": "Code\n# dense neural network\nclass DenseModel(nn.Module):\n    def __init__(self):\n        super(DenseModel, self).__init__()\n\n        # Fully connected layers\n        self.d1 = nn.Linear(28 * 28, 128)\n        self.d2 = nn.Linear(128, 128)  # Added another fully connected layer\n        self.d3 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # Flatten input\n        x = x.view(x.size(0), -1)\n\n        # Fully connected layers\n        x = self.d1(x)\n        x = F.relu(x)\n        x = self.d2(x)  # Added another fully connected layer\n        x = F.relu(x)\n        x = self.d3(x)\n\n        # Softmax activation\n        out = F.softmax(x, dim=1)\n        return out\n\n# convolutional neural network\nclass ConvModel(nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n\n        # 28x28x1 =&gt; 26x26x32\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n        self.d1 = nn.Linear(26 * 26 * 32, 128)\n        self.d2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        # 32x1x28x28 =&gt; 32x32x26x26\n        x = self.conv1(x)\n        x = F.relu(x)\n\n        # flatten =&gt; 32 x (32*26*26)\n        x = x.flatten(start_dim = 1)\n\n        # 32 x (32*26*26) =&gt; 32x128\n        x = self.d1(x)\n        x = F.relu(x)\n\n        # logits =&gt; 32x10\n        logits = self.d2(x)\n        out = F.softmax(logits, dim=1)\n        return out\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What advantage does a CNN with multiple convolutional layers have?\"\ncorrect_answer = \"It allows the network to capture progressively complex features at each layer.\"\ndecoy_answers = [\n    \"It helps downsample the input, reducing the number of computations required.\",\n    \"It makes the model learn features in smaller image segments.\",\n    \"It ensures each layer has identical filters for uniform feature extraction.\"\n]\nexplanation = \"Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It allows the network to capture progressively complex features at each layer.\n        \n        \n        \n          \n          It helps downsample the input, reducing the number of computations required.\n        \n        \n        \n          \n          It makes the model learn features in smaller image segments.\n        \n        \n        \n          \n          It ensures each layer has identical filters for uniform feature extraction.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Multiple convolutional layers enable the network to build upon learned features, progressing from edges to complex patterns.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What effect does a higher learning rate have on the training process?\"\ncorrect_answer = \"It increases the step size of each update, potentially speeding up convergence but risking instability.\"\ndecoy_answers = [\n    \"It decreases the step size, improving stability and precision.\",\n    \"It has no impact on training speed or convergence.\",\n    \"It reduces model accuracy by adding noise to gradients.\"\n]\nexplanation = \"A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It increases the step size of each update, potentially speeding up convergence but risking instability.\n        \n        \n        \n          \n          It decreases the step size, improving stability and precision.\n        \n        \n        \n          \n          It has no impact on training speed or convergence.\n        \n        \n        \n          \n          It reduces model accuracy by adding noise to gradients.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        A higher learning rate speeds up training by taking larger steps, but can overshoot optimal points, causing instability.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the purpose of the final fully connected layer in a CNN?\"\ncorrect_answer = \"It combines features from previous layers to make final predictions.\"\ndecoy_answers = [\n    \"It normalizes feature maps for the final output.\",\n    \"It helps detect edges within the feature maps.\",\n    \"It increases the spatial resolution of the output.\"\n]\nexplanation = \"The fully connected layer aggregates features extracted by previous layers to make classification predictions.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          It combines features from previous layers to make final predictions.\n        \n        \n        \n          \n          It normalizes feature maps for the final output.\n        \n        \n        \n          \n          It helps detect edges within the feature maps.\n        \n        \n        \n          \n          It increases the spatial resolution of the output.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        The fully connected layer aggregates features extracted by previous layers to make classification predictions."
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#데이터셋-생성하기",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#데이터셋-생성하기",
    "title": "  섹션 1: NumPy에서의 합성곱",
    "section": "",
    "text": "PyTorch에서 데이터셋을 준비하기 위해 DataLoader를 사용할 것입니다. PyTorch DataLoader는 학습 또는 테스트 과정 중에 데이터셋을 효율적으로 로드하고 반복하는 데 도움이 되는 유틸리티입니다. 여기서 데이터 로드를 위한 배치 크기를 정의하는데, 이는 학습 또는 테스트의 각 반복 중에 함께 처리되는 샘플 수를 결정합니다. 또한 transforms.Compose를 사용하여 변환 파이프라인을 정의합니다. 이 경우 PIL 이미지 또는 numpy 배열을 PyTorch 텐서로 변환하는 transforms.ToTensor()라는 하나의 변환만 있습니다.\n그런 다음 학습 데이터셋(trainset)을 위한 DataLoader를 생성합니다. 배치별로 로드할 샘플 수를 결정하는 배치 크기(batch_size=BATCH_SIZE)를 지정합니다. shuffle=True로 설정하면 매 에포크마다 데이터를 섞어 학습 중 샘플의 순서를 무작위화하여 모델이 데이터 순서에 기반한 잘못된 패턴을 학습하는 것을 방지합니다. num_workers=2는 데이터 로드에 사용할 서브프로세스 수를 나타냅니다. 이는 데이터 로드 작업을 병렬화하여 데이터 로드 프로세스 속도를 높입니다.\n여기서는 기계 학습 및 컴퓨터 비전 분야에서 널리 사용되는 데이터셋인 MNIST 데이터셋을 사용할 것입니다. 이 데이터셋은 손으로 쓴 숫자(0-9)의 28x28 픽셀 회색조 이미지 모음과 해당 숫자를 나타내는 레이블로 구성되어 있습니다. 각 이미지에는 포함된 숫자가 레이블로 지정되어 있습니다.\n\n\nCode\n%%capture\nBATCH_SIZE = 32\n\n## transformations\ntransform = transforms.Compose(\n    [transforms.ToTensor()])\n\n## download and load training dataset\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=2)\n\n## download and load testing dataset\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n                                         shuffle=False, num_workers=2)\n\n\n\n데이터셋의 데이터 포인트를 시각화할 수 있습니다.\n\n\nCode\n## functions to show an image\ndef imshow(img):\n    #img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n## get some random training images\ndataiter = iter(trainloader)\n\n## show images\nfor i in range(1):  # Assuming you want to show 4 images\n    images, labels = next(dataiter)\n    imshow(torchvision.utils.make_grid(images))\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# check the dimensions\nfor images, labels in trainloader:\n    print(\"Image batch dimensions:\", images.shape)\n    print(\"Image label dimensions:\", labels.shape)\n    break\n\n\nImage batch dimensions: torch.Size([32, 1, 28, 28])\nImage label dimensions: torch.Size([32])"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#모델-인스턴스화하기",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#모델-인스턴스화하기",
    "title": "  섹션 1: NumPy에서의 합성곱",
    "section": "",
    "text": "Code\n## test the model with 1 batch\nmodel = ConvModel()\nfor images, labels in trainloader:\n    print(\"batch size:\", images.shape)\n    out = model(images)\n    print(out.shape)\n    break\n\n\nbatch size: torch.Size([32, 1, 28, 28])\ntorch.Size([32, 10])"
  },
  {
    "objectID": "notebooks/WS03_ConvolutionalNeuralNetworks.html#학습하기",
    "href": "notebooks/WS03_ConvolutionalNeuralNetworks.html#학습하기",
    "title": "  섹션 1: NumPy에서의 합성곱",
    "section": "",
    "text": "Code\nlearning_rate = 0.001\nnum_epochs = 5\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = ConvModel()\nmodel = model.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n\n\n\nCode\n## compute accuracy\ndef get_accuracy(logit, target, batch_size):\n    ''' Obtain accuracy for training round '''\n    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n    accuracy = 100.0 * corrects/batch_size\n    return accuracy.item()\n\n\n\n\nCode\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n\n\nEpoch: 0 | Loss: 1.5820 | Train Accuracy: 88.22\nEpoch: 1 | Loss: 1.4914 | Train Accuracy: 97.30\nEpoch: 2 | Loss: 1.4812 | Train Accuracy: 98.25\nEpoch: 3 | Loss: 1.4766 | Train Accuracy: 98.66\nEpoch: 4 | Loss: 1.4742 | Train Accuracy: 98.88\n\n\n\n\nCode\ntest_acc = 0.0\nfor i, (images, labels) in enumerate(testloader, 0):\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n\nprint('Test Accuracy: %.2f'%( test_acc/i))\n\n\nTest Accuracy: 98.37\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the backpropagation process in CNNs, what is being updated?\"\ncorrect_answer = \"The weights of the filters are updated to minimize the loss.\"\ndecoy_answers = [\n    \"The pixel values of the input images are adjusted to fit the model.\",\n    \"The number of filters in each layer is modified to improve accuracy.\",\n    \"The dimensions of the feature maps are changed.\"\n]\nexplanation = \"During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          The weights of the filters are updated to minimize the loss.\n        \n        \n        \n          \n          The pixel values of the input images are adjusted to fit the model.\n        \n        \n        \n          \n          The number of filters in each layer is modified to improve accuracy.\n        \n        \n        \n          \n          The dimensions of the feature maps are changed.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        During backpropagation, the gradients of the loss function with respect to the filter weights are computed, and these weights are updated to minimize the loss.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"During the training loop, why is it important to zero out gradients using `optimizer.zero_grad()`?\"\ncorrect_answer = \"To prevent gradients from accumulating across batches, ensuring correct updates to the model.\"\ndecoy_answers = [\n    \"To reset the weights of the model after each epoch.\",\n    \"To clear the model parameters so they can be re-initialized.\",\n    \"To reduce the learning rate with each batch.\"\n]\nexplanation = \"Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      \n    \n        \n          \n          To prevent gradients from accumulating across batches, ensuring correct updates to the model.\n        \n        \n        \n          \n          To reset the weights of the model after each epoch.\n        \n        \n        \n          \n          To clear the model parameters so they can be re-initialized.\n        \n        \n        \n          \n          To reduce the learning rate with each batch.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        \n        Zeroing out gradients prevents unwanted accumulation, ensuring that only gradients from the current batch are used for updates.\n      \n    \n    \n    \n\n\n데이터셋에서 이미지를 선택하여 학습된 모델의 합성곱 레이어에 통과시켜 봅시다. 입력 이미지에 반응하여 이 합성곱 레이어에서 생성된 특징 맵을 추출하고 시각화할 수 있습니다. 이 시각화는 CNN이 가장자리(edges)나 질감(textures)과 같은 어떤 종류의 특징을 감지하도록 학습하는지 이해하는 데 도움이 됩니다.\n이 커널이 포착하는 정보의 유형은 학습 과정 중에 학습된 가중치에 따라 달라집니다. 그러나 3x3 커널 크기의 합성곱 연산은 일반적으로 입력 이미지의 로컬 공간 패턴이나 특징을 포착합니다. 이러한 패턴에는 이미지에 존재하는 가장자리, 모서리, 질감 또는 기타 시각적 속성이 포함될 수 있습니다. 32개의 출력 채널이 있으므로 출력 특징 맵의 각 채널은 이러한 로컬 패턴의 다양한 측면이나 변형을 포착하여 이미지 콘텐츠에 대한 더 풍부한 표현을 제공합니다.\n\n\nCode\n# Access the first image from the training set\nimage, _ = trainset[0]\n\n# Plot the image\nplt.imshow(image.squeeze(), cmap='gray')\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Choose an image from the dataset\nimage, _ = trainset[0]  # Choose any image from the dataset\n\n# Convert the image to a tensor and move it to the appropriate device\nimage = image.unsqueeze(0).to(device)\n\n# Get the feature maps from a specific convolutional layer\nconv_layer = model.conv1  # Choose the convolutional layer you want to visualize\nactivation = conv_layer(image)\n\n# Plot the feature maps\nplt.figure(figsize=(10, 10))\nfor i in range(activation.size(1)):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(activation.squeeze().detach().cpu().numpy()[i], cmap='gray')\n    plt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n이미지 크레딧 * 그림 2: 이미지 합성곱 https://gregorygundersen.com/blog/2017/02/24/cnns/ * 그림 3: 5x5 픽셀 이미지 https://ibm.box.com/shared/static/0s5v7doe2p5xuzifs47bxmmuwrn3kra2.bmp * 그림 4: 이미지를 훑는 커널 애니메이션 https://ibm.box.com/shared/static/fvutcm8jwa5j2o7xv2zzqyz2yu3zwhz4.gif\nColab에 wandb(weights and biases) 모듈을 추가하여 학습을 실시간으로 효율적으로 추적하세요!\nwandb 웹사이트에 계정을 만들어 모든 프로젝트를 기록 및 액세스하고, 프로젝트 보고서를 생성하고, 하이퍼파라미터 검색을 실행하는 등의 작업을 수행하세요.\nWandb는 아키텍처 성능을 추적하는 데 널리 사용되는 로깅 모듈이며 사용법을 배울 가치가 있는 도구입니다.\n여기에서 계정(학생 JHU 계정 사용 가능)을 만드세요: https://wandb.auth0.com/login?state=hKFo2SBFTXR0eVRGQ3F2V1ZPQ3hfZ29seTVSdzhzcVkxWEVzN6FupWxvZ2luo3RpZNkgdjVEMjdCdVc4ZzY1UHBPLVRhakt3NnN5NVZ2b25YU0qjY2lk2SBWU001N1VDd1Q5d2JHU3hLdEVER1FISUtBQkhwcHpJdw&client=VSM57UCwT9wbGSxKtEDGQHIKABHppzIw&protocol=oauth2&nonce=aFV4UGtDY0gtfmphLUxtbw%3D%3D&redirect_uri=https%3A%2F%2Fapi.wandb.ai%2Foidc%2Fcallback&response_mode=form_post&response_type=id_token&scope=openid%20profile%20email&signup=true\nColab에서 wandb를 사용하여 모델 추적을 시작하려면 아래의 간단한 단계를 따르세요. 여기서는 최소한의 도구로 학습 데이터만 추적하겠지만, 이 심층 튜토리얼을 살펴보세요: https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=TxUlHUj52t-d\n\n\nCode\nimport random\n\n\n\n\nCode\n!pip install wandb -qqq\nimport wandb\nwandb.login()\n\n#initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"pytorch-intro\",\n        config={\n            \"epochs\": 10,\n            \"batch_size\": 128,\n            \"lr\": 1e-3,\n            \"dropout\": random.uniform(0.01, 0.80),\n            }) #feel free to add other metadata like what optimizers you used etc.\n\n\n#Put the following line in both your training and validation loops so you can track how performance changes in both scenario\n#You can change variable names such as frm 'train/loss' to 'test/loss', 'accuracy', etc. and the corresponding variables will be plotted in wandb.\n#Make sure your information is put in a dictionary, or else you'll have an error!\nwandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n#After your training loop, run\nwandb.finish()\n\n\n\n\nCode\n#Putting it all together:\n\n#🐝  import the module in & login to your account\n!pip install wandb -qqq\nimport wandb\nwandb.login() #Go to bottom of cell and followw instructions. It will ask for an API from a given link that's connected to your account. Make sure you're logged into wandb before running this cell!\n\n#🐝 initialize a wandb instance with relevant data:\nwandb.init(\n        project=\"CNN_Classifier\",\n        config={\n            \"epochs\": num_epochs,\n            \"lr\":learning_rate,\n            })\n\n#Setup training loop:\nfor epoch in range(num_epochs):\n    train_running_loss = 0.0\n    train_acc = 0.0\n\n    model = model.train()\n\n    ## training step\n    for i, (images, labels) in enumerate(trainloader):\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        ## forward + backprop + loss\n        logits = model(images)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        #🐝 insert training logging step here:\n        wandb.log({\"epoch\": epoch, \"train/loss\": loss})\n\n        ## update model params\n        optimizer.step()\n\n        train_running_loss += loss.detach().item()\n        train_acc += get_accuracy(logits, labels, BATCH_SIZE)\n\n    model.eval()\n    print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \\\n          %(epoch, train_running_loss / i, train_acc/i))\n    #🐝 If we did a validation loop, we'd insert the logging step here:\n\n#🐝 finish wandb run\nwandb.finish()"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html",
    "title": "  섹션 1: 모델 정의",
    "section": "",
    "text": "##홈페이지로 돌아가기\n이 튜토리얼은 sentdex의 Harrison Kinsley의 자료를 각색하고 이미지를 차용했습니다.\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\nPyTorch로 신경망 모델 정의 및 인스턴스화\n다중 클래스 분류 작업을 위한 학습 루프 정의 및 실행\n신경망의 학습된 분류 환경 시각화\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n 목차 \n섹션 1: 모델 정의\n\n신경망 클래스 정의\n나선형 다중 클래스 분류 데이터셋 생성\n모델, 손실 함수, 옵티마이저 인스턴스화\n\n섹션 2: 학습\n\n손실, 옵티마이저, 정확도 계산을 포함한 학습 루프를 올바르게 배치\n\n섹션 3: 성능\n\n학습된 분류 영역을 메쉬 표현으로 시각화\n\n\n\n\n\nCode\n!pip install nnfs\n\n\nCollecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.26.4)\nDownloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nnfs\nfrom nnfs.datasets import spiral_data\n\nnnfs.init()\n\n\n\n\nCode\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\n\n\n\n\n이전 노트북에서는 모델의 구성 요소를 정의하기 위해 7개의 클래스가 필요했지만, 아래는 PyTorch를 사용할 때 필요한 전부입니다!\n\n\nCode\n# custom NN inheriting from nn.Module\nclass MyModel(nn.Module):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    # first dense (fully connected) layer with 2 input features, 64 output neurons\n    self.dense1 = nn.Linear(2, 64)\n    # define activation function (ReLU) for first hidden layer\n    self.relu1 = nn.ReLU()\n    # # second desne layer with 64 input neurons, 3 output neurons\n    self.dense2 = nn.Linear(64, 3)\n\n  # forward pass\n  # computes output of model given input data x\n  def forward(self, x):\n    # pass input x through first dense layer\n    x = self.dense1(x)\n    # apply ReLU activation function to output of first dense layer\n    x = self.relu1(x)\n    # pass output of ReLU activation function through second dense layer\n    x = self.dense2(x)\n    # return output of second dense layer\n    return x\n\n\n\n\n우리는 입력 \\(X\\)(이전 워크숍과 같은 두 개의 설명적 특성인 x와 y 좌표)가 주어졌을 때 점의 색상(y \\(ϵ\\) \\(\\{\\)초록, 빨강, 또는 파랑\\(\\}\\) )을 예측하도록 신경망을 학습시킬 것입니다.\n\n\nCode\n# generate data for a classification task\nX, y = spiral_data(samples=100, classes=3)\n\n# convert dataset to pytorch tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\n# visualize dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\n\nReLU 활성화 함수를 가진 두 개의 Dense(완전 연결) 레이어로 구성된 피드포워드 신경망을 사용할 것입니다. 첫 번째 레이어는 200개의 입력 특성을 받아 64개의 뉴런을 출력하고, 두 번째 레이어는 64개의 뉴런을 받아 3개의 뉴런을 출력합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# Define the model\nmodel = MyModel()\n\n# Loss and optimizer\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-7)\n\n\n\n\n\n\n\n\nCode\n# Train in loop\nfor epoch in range(10001):\n    # Forward pass\n    outputs = model(X)\n\n    # Calculate the loss\n    loss = loss_function(outputs, y)\n\n    # Zero gradients, backward pass, and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Calculate accuracy\n    _, predicted = torch.max(outputs, 1)\n    accuracy = (predicted == y).float().mean()\n\n    # Print epoch, accuracy, loss, learning rate every 100 epochs\n    if epoch % 100 == 0:\n        print(f'Epoch: {epoch}, Accuracy: {accuracy.item():.3f}, Loss: {loss.item():.3f}, Learning Rate: {optimizer.param_groups[0][\"lr\"]}')\n\n\nEpoch: 0, Accuracy: 0.347, Loss: 1.095, Learning Rate: 0.05\nEpoch: 100, Accuracy: 0.787, Loss: 0.525, Learning Rate: 0.05\nEpoch: 200, Accuracy: 0.857, Loss: 0.360, Learning Rate: 0.05\nEpoch: 300, Accuracy: 0.887, Loss: 0.280, Learning Rate: 0.05\nEpoch: 400, Accuracy: 0.900, Loss: 0.238, Learning Rate: 0.05\nEpoch: 500, Accuracy: 0.917, Loss: 0.212, Learning Rate: 0.05\nEpoch: 600, Accuracy: 0.917, Loss: 0.198, Learning Rate: 0.05\nEpoch: 700, Accuracy: 0.927, Loss: 0.188, Learning Rate: 0.05\nEpoch: 800, Accuracy: 0.927, Loss: 0.182, Learning Rate: 0.05\nEpoch: 900, Accuracy: 0.923, Loss: 0.174, Learning Rate: 0.05\nEpoch: 1000, Accuracy: 0.890, Loss: 0.257, Learning Rate: 0.05\nEpoch: 1100, Accuracy: 0.930, Loss: 0.166, Learning Rate: 0.05\nEpoch: 1200, Accuracy: 0.930, Loss: 0.155, Learning Rate: 0.05\nEpoch: 1300, Accuracy: 0.933, Loss: 0.147, Learning Rate: 0.05\nEpoch: 1400, Accuracy: 0.937, Loss: 0.141, Learning Rate: 0.05\nEpoch: 1500, Accuracy: 0.940, Loss: 0.137, Learning Rate: 0.05\nEpoch: 1600, Accuracy: 0.937, Loss: 0.134, Learning Rate: 0.05\nEpoch: 1700, Accuracy: 0.940, Loss: 0.131, Learning Rate: 0.05\nEpoch: 1800, Accuracy: 0.947, Loss: 0.129, Learning Rate: 0.05\nEpoch: 1900, Accuracy: 0.940, Loss: 0.128, Learning Rate: 0.05\nEpoch: 2000, Accuracy: 0.947, Loss: 0.125, Learning Rate: 0.05\nEpoch: 2100, Accuracy: 0.943, Loss: 0.122, Learning Rate: 0.05\nEpoch: 2200, Accuracy: 0.943, Loss: 0.123, Learning Rate: 0.05\nEpoch: 2300, Accuracy: 0.943, Loss: 0.121, Learning Rate: 0.05\nEpoch: 2400, Accuracy: 0.947, Loss: 0.118, Learning Rate: 0.05\nEpoch: 2500, Accuracy: 0.950, Loss: 0.117, Learning Rate: 0.05\nEpoch: 2600, Accuracy: 0.947, Loss: 0.112, Learning Rate: 0.05\nEpoch: 2700, Accuracy: 0.943, Loss: 0.109, Learning Rate: 0.05\nEpoch: 2800, Accuracy: 0.957, Loss: 0.107, Learning Rate: 0.05\nEpoch: 2900, Accuracy: 0.950, Loss: 0.124, Learning Rate: 0.05\nEpoch: 3000, Accuracy: 0.950, Loss: 0.106, Learning Rate: 0.05\nEpoch: 3100, Accuracy: 0.947, Loss: 0.106, Learning Rate: 0.05\nEpoch: 3200, Accuracy: 0.950, Loss: 0.105, Learning Rate: 0.05\nEpoch: 3300, Accuracy: 0.950, Loss: 0.104, Learning Rate: 0.05\nEpoch: 3400, Accuracy: 0.950, Loss: 0.103, Learning Rate: 0.05\nEpoch: 3500, Accuracy: 0.950, Loss: 0.103, Learning Rate: 0.05\nEpoch: 3600, Accuracy: 0.950, Loss: 0.102, Learning Rate: 0.05\nEpoch: 3700, Accuracy: 0.950, Loss: 0.102, Learning Rate: 0.05\nEpoch: 3800, Accuracy: 0.950, Loss: 0.101, Learning Rate: 0.05\nEpoch: 3900, Accuracy: 0.950, Loss: 0.101, Learning Rate: 0.05\nEpoch: 4000, Accuracy: 0.950, Loss: 0.100, Learning Rate: 0.05\nEpoch: 4100, Accuracy: 0.950, Loss: 0.100, Learning Rate: 0.05\nEpoch: 4200, Accuracy: 0.950, Loss: 0.099, Learning Rate: 0.05\nEpoch: 4300, Accuracy: 0.950, Loss: 0.098, Learning Rate: 0.05\nEpoch: 4400, Accuracy: 0.950, Loss: 0.098, Learning Rate: 0.05\nEpoch: 4500, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4600, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4700, Accuracy: 0.950, Loss: 0.096, Learning Rate: 0.05\nEpoch: 4800, Accuracy: 0.950, Loss: 0.097, Learning Rate: 0.05\nEpoch: 4900, Accuracy: 0.960, Loss: 0.094, Learning Rate: 0.05\nEpoch: 5000, Accuracy: 0.960, Loss: 0.094, Learning Rate: 0.05\nEpoch: 5100, Accuracy: 0.957, Loss: 0.091, Learning Rate: 0.05\nEpoch: 5200, Accuracy: 0.970, Loss: 0.098, Learning Rate: 0.05\nEpoch: 5300, Accuracy: 0.953, Loss: 0.090, Learning Rate: 0.05\nEpoch: 5400, Accuracy: 0.957, Loss: 0.090, Learning Rate: 0.05\nEpoch: 5500, Accuracy: 0.960, Loss: 0.089, Learning Rate: 0.05\nEpoch: 5600, Accuracy: 0.963, Loss: 0.112, Learning Rate: 0.05\nEpoch: 5700, Accuracy: 0.963, Loss: 0.093, Learning Rate: 0.05\nEpoch: 5800, Accuracy: 0.963, Loss: 0.092, Learning Rate: 0.05\nEpoch: 5900, Accuracy: 0.963, Loss: 0.090, Learning Rate: 0.05\nEpoch: 6000, Accuracy: 0.963, Loss: 0.089, Learning Rate: 0.05\nEpoch: 6100, Accuracy: 0.963, Loss: 0.088, Learning Rate: 0.05\nEpoch: 6200, Accuracy: 0.963, Loss: 0.088, Learning Rate: 0.05\nEpoch: 6300, Accuracy: 0.963, Loss: 0.087, Learning Rate: 0.05\nEpoch: 6400, Accuracy: 0.963, Loss: 0.087, Learning Rate: 0.05\nEpoch: 6500, Accuracy: 0.963, Loss: 0.086, Learning Rate: 0.05\nEpoch: 6600, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6700, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6800, Accuracy: 0.963, Loss: 0.085, Learning Rate: 0.05\nEpoch: 6900, Accuracy: 0.963, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7000, Accuracy: 0.963, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7100, Accuracy: 0.967, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7200, Accuracy: 0.963, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7300, Accuracy: 0.967, Loss: 0.083, Learning Rate: 0.05\nEpoch: 7400, Accuracy: 0.967, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7500, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7600, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7700, Accuracy: 0.967, Loss: 0.084, Learning Rate: 0.05\nEpoch: 7800, Accuracy: 0.967, Loss: 0.082, Learning Rate: 0.05\nEpoch: 7900, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 8000, Accuracy: 0.970, Loss: 0.082, Learning Rate: 0.05\nEpoch: 8100, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8200, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8300, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8400, Accuracy: 0.970, Loss: 0.081, Learning Rate: 0.05\nEpoch: 8500, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8600, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8700, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8800, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 8900, Accuracy: 0.970, Loss: 0.080, Learning Rate: 0.05\nEpoch: 9000, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9100, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9200, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9300, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9400, Accuracy: 0.970, Loss: 0.079, Learning Rate: 0.05\nEpoch: 9500, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9600, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9700, Accuracy: 0.967, Loss: 0.081, Learning Rate: 0.05\nEpoch: 9800, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\nEpoch: 9900, Accuracy: 0.970, Loss: 0.077, Learning Rate: 0.05\nEpoch: 10000, Accuracy: 0.970, Loss: 0.078, Learning Rate: 0.05\n\n\n\n\n\n\n\nCode\n# create meshgrid of points covering the feature space\nh = 0.02\n# determine min and max values for x,y axes\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n# create meshgrid of points with spacing h\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# convert meshgrid to torch tensor\nmeshgrid_points = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n\n# pass meshgrid points through model\nwith torch.no_grad():\n  # forward pass through first dense\n  z1 = model.dense1(meshgrid_points)\n  # apply relu activation\n  a1 = torch.relu(z1)\n  # forward pass through second dense\n  z2 = model.dense2(a1)\n  # compute softmax probabilities for each class\n  exp_scores = torch.exp(z2 - torch.max(z2, axis=1, keepdim=True).values)\n  probs = exp_scores / torch.sum(exp_scores, axis=1, keepdim=True)\n\n# predictions\n# determine predicted class for each point in meshgrid\n_, predictions = torch.max(probs, axis=1)\n# reshape predictions to match shape of meshgrid\nZ = predictions.numpy().reshape(xx.shape)\n\n# plot decision boundary based on predictions\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# plot original data on top of decision boundary\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n# plot limits set to match extent of meshgrid\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n질문: 신경망의 은닉층 활성화 함수를 ReLU에서 sigmoid로 변경하면 어떻게 되나요? 이 변경 사항을 시도해보고 성능 변화에 대해 언급해 보세요.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint('\\n ReLU outperforms sigmoid. \\n ReLU mitigates the vanishing gradient problem by allowing gradients to flow for positive inputs. \\n Sigmoid will lead to poorer convergence due to the smaller effective gradient.')\n\n\n질문: 손실 함수를 CrossEntropyLoss에서 MeanSquaredError로 변경하면 분류 모델의 학습에 어떤 영향을 미치나요? 이것을 시도해보고 무슨 일이 일어나는지 설명해 보세요.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using MSE for classification results in worse performance. \\n This is because MSE doesn't explicitly encourage the model to push the probability of the correct class to 1 while minimizing the others.\")\n\n\n질문: 학습률 0.01의 SGD 옵티마이저를 사용하는 것과 학습률 0.001의 Adam 옵티마이저를 사용하는 것은 어떻게 다른가요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Both optimizers perform similarly, but Adam with a learning rate of 0.001 will convergence faster.\")\n\n\n질문: 신경망 모델에 128개의 뉴런을 가진 은닉층을 하나 더 추가하면 어떻게 되나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Adding another hidden layer with 128 neurons to the neural network will increase the model's capacity to learn more complex patterns in the data. \\n Adding the extra layer will improve the performance slightly, but the original model already achieves high accuracy (~0.970).\")\n\n\n질문: 0.001의 가중치 감쇠(weight decay)를 사용하면 신경망 모델의 학습에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using a weight decay of 0.001 (up from the original 5e-7) will increase regularization, penalizing large weights more heavily. \\n Training accuracy may decrease slightly, but will improve generalization.\")"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#데이터셋-생성",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#데이터셋-생성",
    "title": "  섹션 1: 모델 정의",
    "section": "",
    "text": "우리는 입력 \\(X\\)(이전 워크숍과 같은 두 개의 설명적 특성인 x와 y 좌표)가 주어졌을 때 점의 색상(y \\(ϵ\\) \\(\\{\\)초록, 빨강, 또는 파랑\\(\\}\\) )을 예측하도록 신경망을 학습시킬 것입니다.\n\n\nCode\n# generate data for a classification task\nX, y = spiral_data(samples=100, classes=3)\n\n# convert dataset to pytorch tensors\nX = torch.tensor(X, dtype=torch.float32)\ny = torch.tensor(y, dtype=torch.long)\n\n# visualize dataset\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#모델-인스턴스화",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#모델-인스턴스화",
    "title": "  섹션 1: 모델 정의",
    "section": "",
    "text": "ReLU 활성화 함수를 가진 두 개의 Dense(완전 연결) 레이어로 구성된 피드포워드 신경망을 사용할 것입니다. 첫 번째 레이어는 200개의 입력 특성을 받아 64개의 뉴런을 출력하고, 두 번째 레이어는 64개의 뉴런을 받아 3개의 뉴런을 출력합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# Define the model\nmodel = MyModel()\n\n# Loss and optimizer\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.05, weight_decay=5e-7)"
  },
  {
    "objectID": "notebooks/WS02_NeuralNetworksWithPyTorch.html#질문",
    "href": "notebooks/WS02_NeuralNetworksWithPyTorch.html#질문",
    "title": "  섹션 1: 모델 정의",
    "section": "",
    "text": "질문: 신경망의 은닉층 활성화 함수를 ReLU에서 sigmoid로 변경하면 어떻게 되나요? 이 변경 사항을 시도해보고 성능 변화에 대해 언급해 보세요.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint('\\n ReLU outperforms sigmoid. \\n ReLU mitigates the vanishing gradient problem by allowing gradients to flow for positive inputs. \\n Sigmoid will lead to poorer convergence due to the smaller effective gradient.')\n\n\n질문: 손실 함수를 CrossEntropyLoss에서 MeanSquaredError로 변경하면 분류 모델의 학습에 어떤 영향을 미치나요? 이것을 시도해보고 무슨 일이 일어나는지 설명해 보세요.\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using MSE for classification results in worse performance. \\n This is because MSE doesn't explicitly encourage the model to push the probability of the correct class to 1 while minimizing the others.\")\n\n\n질문: 학습률 0.01의 SGD 옵티마이저를 사용하는 것과 학습률 0.001의 Adam 옵티마이저를 사용하는 것은 어떻게 다른가요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Both optimizers perform similarly, but Adam with a learning rate of 0.001 will convergence faster.\")\n\n\n질문: 신경망 모델에 128개의 뉴런을 가진 은닉층을 하나 더 추가하면 어떻게 되나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Adding another hidden layer with 128 neurons to the neural network will increase the model's capacity to learn more complex patterns in the data. \\n Adding the extra layer will improve the performance slightly, but the original model already achieves high accuracy (~0.970).\")\n\n\n질문: 0.001의 가중치 감쇠(weight decay)를 사용하면 신경망 모델의 학습에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Using a weight decay of 0.001 (up from the original 5e-7) will increase regularization, penalizing large weights more heavily. \\n Training accuracy may decrease slightly, but will improve generalization.\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning for Proteins (DL4Proteins) 워크숍",
    "section": "",
    "text": "Deep Learning for Proteins (DL4Proteins) 워크숍\n\n\n\nCell Motherboard Wallpaper\n\n\nDL4Proteins에 오신 것을 환영합니다!\n\n\n\nOverview of Topics\n\n\nDL4Proteins 노트북 시리즈의 목표는 단백질 설계 및 예측을 위한 딥러닝을 대중화하여 과학계의 혁신적인 순간에 도달하는 것입니다. 2024년 노벨 화학상이 David Baker, Demis Hassabis, John Jumper에게 컴퓨터를 이용한 단백질 설계 및 구조 예측의 획기적인 공로로 수여됨에 따라, 본 자료는 이러한 혁명을 이끈 바로 그 도구와 방법론에 대한 실습 중심의 입문서를 제공합니다. DL4Proteins는 기초 머신러닝 원리와 AlphaFold, RFDiffusion, ProteinMPNN과 같은 최첨단 접근 방식을 결합하여 연구자, 교육자, 학생들이 단백질 공학의 미래에 기여할 수 있는 지식을 갖추도록 돕습니다. 이 오픈 소스 노트북들은 최신 연구와 강의실 학습 간의 간극을 메우며 합성 생물학 및 치료제 분야의 차세대 혁신가를 양성하는 데 기여합니다.\n관련 프리프린트(preprint)는 이 노트북 시리즈에 대한 상세한 교육적 프레임워크를 제시하며, 각 노트북의 동기, 학습 성과 및 기저에 깔린 딥러닝 원리들을 설명합니다.\n\n프리프린트:\nDL4Proteins Jupyter Notebooks Teach how to use Artificial Intelligence for Biomolecular Structure Prediction and Design\n아래의 Jupyter 노트북들은 현재 단백질 설계 분야에서 활용되는 핵심적인 머신러닝 개념과 모델에 대한 소개를 제공합니다. 노트북은 Google Colaboratory에서 실행할 수 있습니다.\n그림과 질문이 올바르게 표시되도록 Colab 노트북을 라이트 모드(light mode)로 설정해 주세요.\n\n\n목차\n\n\n챕터 1: NumPy를 활용한 신경망\n\n\n챕터 2: PyTorch를 활용한 신경망\n\n\n챕터 3: 합성곱 신경망 (CNN)\n\n\n챕터 4: 셰익스피어와 단백질을 위한 언어 모델\n\n\n챕터 5: 언어 모델 임베딩과 다운스트림 태스크를 위한 전이 학습\n\n\n챕터 6: AlphaFold 소개\n\n\n챕터 7: 단백질을 위한 그래프 신경망 (GNN)\n\n\n챕터 8: 디노이징 확산 확률 모델 (DDPM)\n\n\n챕터 9: 종합 - RFDiffusion에서 ProteinMPNN, AlphaFold까지\n\n\n챕터 10: RFDiffusion 소개 - All Atom\n문제가 발생하면 Issues 탭에 남겨주세요. 이 리포지토리는 지속적으로 피드백을 반영하며 업데이트되고 있습니다!\n저자: Michael F. Chungyoun, Sreevarsha Puvada, Gabriel Au, Courtney Thomas, Britnie J. Carpentier, Jeffrey J. Gray\n감사의 말: Sergey Lyskov, Sergey Ovchinnikov, Johns Hopkins 2023년 540.614/414 단백질 구조 예측 수업 수강생들, 그리고 Johns Hopkins Center for Teaching Excellence and Innovation - Instructional Enhancement Grant.\n인용 및 추가 리소스: 이 리포지토리의 각 노트북은 주요 온라인 도구, 교육 자료, 출판물, 오픈 소스 리포지토리 등 다양한 최첨단 리소스에서 영감과 방법론을 얻었습니다. 주요 리소스에는 Harrison Kinsley, Andrej Karpathy, Petar Veličković의 유튜브 시리즈가 포함됩니다. 이러한 자료들은 각 노트북 내에서 인용되었으며, 사용자가 더 깊은 통찰력을 얻기 위해 이러한 기본 저작물들을 탐구할 것을 권장합니다."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "DL4Proteins-notebooks",
    "section": "",
    "text": "MIT License\nCopyright (c) 2025 Gray Lab\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "##홈페이지로 돌아가기\n이 튜토리얼은 sentdex의 Harrison Kinsley의 자료를 각색하고 이미지를 차용했습니다.\n목표: 이 워크숍을 마치면 다음 개념을 이해할 수 있습니다:\n\n 뉴런 (기계 학습의 맥락에서) \n순전파(Forward pass) / 역전파(backward pass)\nReLU 활성화 함수\n Softmax 활성화 손실 및 범주형 교차 엔트로피(categorical cross entropy)\nAdam 옵티마이저\n가중치(Weights)와 편향(biases) 및 각각의 업데이트\n학습(Training)과 에포크(epochs)\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n 목차 \n섹션 1: 소개 및 첫 번째 뉴런 코딩\n\n뉴런이란 무엇인가?\n뉴런의 입력과 출력 계산하기\n\n섹션 2: 뉴런 레이어 코딩 * 리스트의 리스트를 사용하여 출력 계산하기 * 내적(Dot Product)의 개념 * 신경망에서 배치(batch) 사용하기\n섹션 3: 은닉층 활성화 함수 * 계단 함수(Step Function) * 시그모이드 함수(Sigmoid Function) * Rectified Linear Unit (ReLU)\n섹션 4: 출력층 활성화 함수 * Softmax * 오버플로우 방지\n섹션 5: 손실 계산 및 구현 * 범주형 교차 엔트로피(Categorical Cross Entropy) * 출력 배치의 손실 계산 * 에러: Log(0) * 정확도 공식\n섹션 6: 역전파(Backpropogation) * 편미분과 그래디언트 * 연쇄 법칙(Chain Rule)\n섹션 7: 옵티마이저 * 확률적 경사 하강법(Stochastic Gradient Descent)\n섹션 8: 학습률과 모멘텀 * 학습률 감쇠(Learning Rate Decay) * 모멘텀(Momentum)\n섹션 9: 처음부터 완전한 신경망 구축하기\n\n\n\n\nCode\n# package for creating our dataset\n!pip install nnfs\n\n\nCollecting nnfs\n  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\nInstalling collected packages: nnfs\nSuccessfully installed nnfs-0.5.1\n\n\n\n\nCode\nimport numpy as np\nimport nnfs\nfrom nnfs.datasets import spiral_data\nimport matplotlib.pyplot as plt\nimport math\n\nnnfs.init()\n\n\n\n\n\n\n왜 이것들을 신경망(neural networks)이라고 부를까요?\n시각적으로 네트워크처럼 보이기 때문입니다. 파란색 원은 뉴런이고 주황색 선으로 연결되어 있습니다. 이 경우 입력층, 각각 4개의 뉴런이 있는 두 개의 은닉층, 그리고 출력층이 있습니다.\n데이터는 왼쪽에서 시작하여 앞으로 전달됩니다(이 이미지에서는 입력으로 들어오는 데이터가 2개뿐입니다). 결국 우리가 원하는 것을 반환하는 출력층으로 전달됩니다. 출력의 예로는 이미지를 고양이 또는 개로 분류하는 것이 있습니다. 출력 뉴런 중 하나는 고양이 예측과 관련되고 다른 하나는 개 예측과 관련되며, 네트워크의 예측은 어떤 뉴런이 더 강하게 발화했는지에 따라 결정됩니다.\n학습 중에 가중치(weights)와 편향(biases)을 조정하여 신경망의 예측 정확도를 최적화할 수 있습니다. 이에 대해서는 나중에 더 자세히 논의하겠습니다…\n\n\n\nimage.png\n\n\n위와 같은 완전 연결(fully connected) 신경망의 뉴런을 코딩하는 것부터 시작해 보겠습니다. 완전 연결 신경망에서는 모든 뉴런이 이전의 모든 단일 뉴런과 고유하게 연결되어 있습니다.\n이전 레이어에서 3개의 뉴런이 우리가 구축하고 있는 뉴런으로 입력된다고 가정해 봅시다.\n아래의 단일 뉴런을 고려해보세요:\n\n\n\nimage.png\n\n\n이전 레이어(3개의 뉴런이 있음)로부터의 출력을 계산해 봅시다.\n\n\nCode\n\n# this will be the input to our current neuron\ninputs = np.array([1, 2, 3])\n\n# every unique input will have a unique weight associated with it\n# since we have three inputs, we have 3 weights\nweights = np.array([0.2, 0.8, -0.5])\n\n# every unique neuron has a unique bias\nbias = 2\n\n# output from our neuron is the input*weight + bias\noutput = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\nprint(output)\n\n\n2.3\n\n\n이제 출력층 뉴런 중 하나(4번째 레이어, 맨 위 뉴런)를 모델링해 봅시다.\n\n\n\nimage.png\n\n\n무엇이 바뀔까요? 고유한 가중치가 하나 더 생기지만, 다른 편향이 있나요? 아닙니다! 우리는 여전히 하나의 관련 편향을 가진 단일 뉴런만 모델링하고 있습니다.\n  이전 셀과 동일한 방법론을 사용하여 위 뉴런의 출력을 계산할 수 있나요?\n\n\nCode\n# &lt;b&gt; &lt;font color='#569098'&gt; 섹션 2: 뉴런 레이어 코딩\n\n\n\n\nCode\n각각 3개의 입력을 가진 3개의 뉴런을 모델링하는 것은 어떨까요?\n\n\n\n\n\n각각 3개의 입력을 가진 3개의 뉴런을 모델링하는 것은 어떨까요?\n\n\n\nimage.png\n\n\n3개의 뉴런이 있으므로 3개의 고유한 가중치 세트가 있게 됩니다. 각 가중치 세트에는 4개의 값(입력이 4개이므로)이 있습니다. 또한 3개의 고유한 편향이 필요합니다.\n\n\nCode\n{{IMG_0}}\n\n\n[4.8, 1.21, 2.385]\n\n\noutput이 다른 것이 되기를 원한다면 어떻게 해야 할까요? 뉴런의 input은 이미 이전 레이어에서 제공되거나(또는 입력 레이어에서 옴) 변경할 수 없습니다. weights와 bias는 어떨까요? 고정되어 있나요?\nweights와 biases는 신경망의 조절 노브와 같아서 다른 모든 뉴런 간의 통신을 변경하기 위해 조절할 수 있다는 것이 조금 더 명확해져야 합니다.\n가중치에 대한 lists의 list를 사용하여 위 코드를 단순화해 봅시다(여전히 3개의 뉴런). 이것은 레이어에 대해 입력*가중치 + 편향을 수행하는 더 깔끔한 방법입니다.\n\n\nCode\n&lt;b&gt;&lt;font color='#e59454'&gt; `.shape` numpy 메서드를 사용하여 아래 각각의 모양(shape)을 출력하세요&lt;/font&gt;&lt;/b&gt;\n\n\n[4.8, 1.21, 2.385]\n\n\n조정/미세조정을 위해 왜 weights와 biases 둘 다 필요할까요? 이론적으로는 둘 다 필요하지 않을 수도 있습니다. 하지만 둘 다 있으면 서로 다른 유형의 수정을 제공하므로 가능한 출력을 다양화하는 데 도움이 됩니다. 이것은 그래프 y = mx + b를 개념화하는 것과 정확히 같습니다. 기울기(가중치)를 변경하면 그래프의 가파른 정도가 바뀌고, b(편향)를 변경하면 플롯의 위치가 바뀝니다.\n\n\n\nimage.png\n\n\n우리는 다양한 모양과 크기의 배열을 다룰 것입니다. 아래 세 가지를 고려해보세요:\n\n\nCode\n&lt;font color='#e59454'&gt; &lt;b&gt; 이전 레이어의 3개 뉴런에 연결된 단일 뉴런에 대한 `output`을 계산하는 코드를 작성하세요 &lt;/b&gt;\n\n\n .shape numpy 메서드를 사용하여 아래 각각의 모양(shape)을 출력하세요\n\n\nCode\n## **&lt;font color='#569098'&gt; 배치(Batches)에 대한 코멘트&lt;/font&gt;**\n\n배치를 사용하면 병렬로 계산할 수 있습니다. 배치가 클수록 더 많은 병렬 연산을 실행할 수 있습니다. 이것이 바로 우리가 CPU의 4-8코어 대신 계산을 실행할 수 있는 수백-수천 개의 코어가 있는 GPU에서 신경망 학습을 수행하는 경향이 있는 이유이기도 합니다. 배치는 *일반화(generalization)*에도 도움이 됩니다. 한 번에 하나씩 또는 모든 포인트를 한 번에 노출시키는 대신 한 번에 여러 데이터 포인트를 네트워크에 노출시키는 것은 신경망이 일반화하는 데 도움이 되는 것으로 널리 이해되고 있습니다. 일반적으로 우리는 32, 64, 아마도 128의 배치 크기로 작업할 것입니다.\n\n\n\n\nCode\n# &lt;b&gt; &lt;font color='#569098'&gt; 섹션 3: 은닉층 활성화 함수\n\n\n 배열은 상동적(homologous)이어야 합니다: 각 차원에서 동일한 크기를 가져야 합니다.\n## 내적(Dot product) 소개\n(이미 확실히 이해하고 있다면 건너뛰어도 좋습니다)\ninputs(벡터)에 weights(벡터의 행렬)를 어떻게 곱할까요? 내적입니다! a와 b 사이의 내적을 수행할 수 있나요?\n상기해보세요:\n\\(a \\cdot b= \\sum \\limits_ {i=1}^n a_i b_i\\)\n  아래 코드 상자에 내적 코드를 작성하세요 \n\n\nCode\n 따라서 계단 함수의 경우 뉴런의 출력은 말 그대로 0 또는 1입니다.\n\n\n  이전 레이어의 3개 뉴런에 연결된 단일 뉴런에 대한 output을 계산하는 코드를 작성하세요 \n\n\nCode\n이것이 이론적으로는 괜찮지만, 더 세분화된 것을 사용하면 신경망 학습이 더 잘 이루어질 것이라는 점이 금방 분명해집니다. 예를 들어 계단 함수에서 출력이 0인 경우, 입력 값이 출력 1에 도달하는 데 얼마나 가까웠는지 어떻게 알 수 있을까요? 이것을 알면 모델이 스스로를 더 잘 수정하는 데 도움이 되며, *`loss`*를 계산하고 가중치와 편향을 *`optimizing`*할 때 작용합니다.\n\n그러한 함수 중 하나가 &lt;b&gt;&lt;font color='#e59454'&gt;시그모이드 함수&lt;/font&gt;&lt;/b&gt;입니다. 출력이 0(또는 1)에 얼마나 가까운지(또는 멀리 있는지) 쉽게 알 수 있습니다. 그러나 시그모이드 함수조차도 완벽하지 않지만, *기울기 소실(vanishing gradients)*이라는 단점은 우리가 기울기(gradient)에 도달할 때까지는 그다지 이해되지 않을 것입니다.\n\n\n4.8\n\n\n  아래 코드 상자에 뉴런의 dotlayer에 대한 출력을 계산하는 코드를 작성하세요 \n\n\nCode\n이로써 &lt;b&gt;&lt;font color='#e59454'&gt;Rectified Linear Unit (ReLU) 활성화 함수&lt;/font&gt;&lt;/b&gt;에 도달하게 됩니다.\n\nx가 0보다 크면 출력은 x이고, 그렇지 않으면 출력은 0입니다. ReLU를 사용하는 주된 이유 중 하나는 빠르다는 것입니다: x가 0보다 작으면 0 출력, x가 0보다 크면 x 출력.\n\n왜 선형 활성화 함수(y=x와 같은)를 사용하지 않는지 의문을 가질 수 있지만, 비선형 데이터를 선형 함수로 모델링하는 것은 불가능합니다 :(\n그래서 우리는 비선형 활성화 함수가 필요합니다.\n\n\n[4.79999995 1.21000004 2.38499999]\n\n\n\n\n배치를 사용하면 병렬로 계산할 수 있습니다. 배치가 클수록 더 많은 병렬 연산을 실행할 수 있습니다. 이것이 바로 우리가 CPU의 4-8코어 대신 계산을 실행할 수 있는 수백-수천 개의 코어가 있는 GPU에서 신경망 학습을 수행하는 경향이 있는 이유이기도 합니다. 배치는 일반화(generalization)에도 도움이 됩니다. 한 번에 하나씩 또는 모든 포인트를 한 번에 노출시키는 대신 한 번에 여러 데이터 포인트를 네트워크에 노출시키는 것은 신경망이 일반화하는 데 도움이 되는 것으로 널리 이해되고 있습니다. 일반적으로 우리는 32, 64, 아마도 128의 배치 크기로 작업할 것입니다.\n\n\n\n\n활성화 함수는 은닉층(입력 또는 출력 레이어 이외의 레이어)에서 여러 가지 이유로 사용됩니다. 이들은 각 뉴런의 출력을 정규화하여 값이 네트워크를 통과하면서 너무 커지거나 작아지는 것을 방지합니다.\n각 뉴런은 활성화 함수를 갖게 되며, 이는 inputs * weights + bias를 계산한 후에 적용됩니다.\n따라서 output = Activation Function (inputs * weights + bias)\n여기서는 3가지 활성화 함수에 대해 이야기하겠습니다. 1. 계단 함수(Step function) 2. 시그모이드 함수(Sigmoid function) 3. Rectified linear (ReLU) 함수.\n  계단 함수 의 경우 입력이 0보다 크면 출력은 1이고, 그렇지 않으면 출력은 0입니다.\n\n\n\nimage.png\n\n\n따라서 계단 함수의 경우 뉴런의 출력은 말 그대로 0 또는 1입니다.\n\n\n\nimage.png\n\n\n이것이 이론적으로는 괜찮지만, 더 세분화된 것을 사용하면 신경망 학습이 더 잘 이루어질 것이라는 점이 금방 분명해집니다. 예를 들어 계단 함수에서 출력이 0인 경우, 입력 값이 출력 1에 도달하는 데 얼마나 가까웠는지 어떻게 알 수 있을까요? 이것을 알면 모델이 스스로를 더 잘 수정하는 데 도움이 되며, loss를 계산하고 가중치와 편향을 optimizing할 때 작용합니다.\n그러한 함수 중 하나가 시그모이드 함수입니다. 출력이 0(또는 1)에 얼마나 가까운지(또는 멀리 있는지) 쉽게 알 수 있습니다. 그러나 시그모이드 함수조차도 완벽하지 않지만, 기울기 소실(vanishing gradients)이라는 단점은 우리가 기울기(gradient)에 도달할 때까지는 그다지 이해되지 않을 것입니다.\n\n\n\nimage.png\n\n\n이로써 Rectified Linear Unit (ReLU) 활성화 함수에 도달하게 됩니다.\nx가 0보다 크면 출력은 x이고, 그렇지 않으면 출력은 0입니다. ReLU를 사용하는 주된 이유 중 하나는 빠르다는 것입니다: x가 0보다 작으면 0 출력, x가 0보다 크면 x 출력.\n왜 선형 활성화 함수(y=x와 같은)를 사용하지 않는지 의문을 가질 수 있지만, 비선형 데이터를 선형 함수로 모델링하는 것은 불가능합니다 :( 그래서 우리는 비선형 활성화 함수가 필요합니다.\n\n\n\nimage.png\n\n\n지금까지 배운 신경망 출력을 조정하기 위한 다양한 매개변수를 상기해 봅시다..\n가중치와 편향을 조정하면 뉴런의 출력에 어떤 영향을 미치는지 다시 살펴보면… 가중치를 변경하면 활성화의 강도(기울기)가 변경되고, 편향으로 활성화 지점을 상쇄할 수 있습니다. 또한 가중치를 음수로 하여 뉴런을 비활성화하는 데 필요한 x 값을 확인할 수도 있습니다.\n\n\nCode\n우리는 결국 문제에 부딪힐 것입니다... &lt;font color ='#e59454'&gt;&lt;b&gt; log(0)\n\n\nMatch the following:\n1. What controls the offset of the output\n2. What controls the strength/slope of the output?\n3. What do we use to normalize the output?\nA. Weights\nB. Bias\nC. Activation Function\nMatch for 1 (enter A, B, or C): B\nMatch for 2 (enter A, B, or C): A\nMatch for 3 (enter A, B, or C): C\nCorrect match for 1. What controls the offset of the output\nCorrect match for 2. What controls the strength/slope of the output?\nCorrect match for 3. What do we use to normalize the output?\n\n\n  inputs 배열의 각 값을 받아 각 값에 대한 ReLU function의 output을 계산하는 알고리즘을 아래 코딩 셀에 작성하세요\n\n\nCode\n예측 값을 클리핑하면 이 무한대 문제에 빠지지 않도록 보장합니다.\n\n\n\n\nCode\n그리고 정확도는 어떻게 계산할까요?\n\n\n\n\n\n왜 또 다른 활성화 함수가 필요할까요? 우리는 상대적인 “정확성”을 정량화하는 데 도움이 되는 함수를 원합니다. 아래 두 개의 출력 레이어에서 어느 것이 상대적으로 더 정확할까요? 배열의 첫 번째 노드가 정답 예측이라고 가정할 때, layer_outputs1이 다른 배열 값보다 상대적으로 크기 때문에 아마도 더 정확할 것입니다.\n\n\nCode\n{{IMG_0}}\n\n\nSoftmax 함수는 기계 학습에서 다중 클래스 분류 작업에 일반적으로 사용됩니다. 임의의 실수 값 점수(종종 로짓이라고 함) 벡터를 입력으로 받아 여러 클래스에 대한 확률 분포로 정규화합니다.\n\\(K\\)개의 요소를 가진 벡터 \\(z\\)에 대한 Softmax 함수는 다음과 같이 정의됩니다:\n\\[ \\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\]\n여기서: - $ (z)_i $는 결과 확률 분포의 \\(i\\)번째 요소입니다. - \\(e\\)는 자연 로그의 밑(오일러 수)입니다. - \\(z_i\\)는 입력 벡터 \\(z\\)의 \\(i\\)번째 요소입니다. - 분모는 모든 클래스에 대한 지수화된 점수의 합입니다.\nSoftmax 함수는 출력 확률의 합이 1이 되도록 보장하므로 모델이 클래스 확률을 출력해야 하는 분류 작업에 적합합니다. layer_outputs1과 layer_outputs2의 Softmax를 계산할 수 있나요? 각 레이어의 Softmax를 계산한 후, Softmax된 배열의 합을 출력하여 합이 1이 되는지 확인하세요\n\n\nCode\n{{IMG_0}}\n\n\n\n\nCode\n**편미분(partial derivative)**은 단일 입력이 함수의 출력에 얼마나 많은 영향을 미치는지 측정합니다.\n\n모든 매개변수의 편미분을 결합하면 결과 방정식 세트를 &lt;font color='#e59454'&gt;**그래디언트(gradient)**&lt;/font&gt;라고 합니다.\n\n\n단일 레이어 출력 벡터에서 벗어나, 실제로는 출력 배치를 갖게 될 것입니다.\n\n\nCode\n아래 이미지와 유사하게 순전파는 함수의 체인입니다. 입력 데이터를 첫 번째 레이어로 전달하여 레이어의 가중치와 편향을 갖고 결과는 ReLU 활성화 함수를 통해 흐릅니다. 이 계산을 다른 레이어에서 반복하여 출력층과 Softmax 활성화까지 계속합니다.\n\n\n[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n\n\n오버플로우 방지란 무엇인가요?\n지수를 다룰 때 프로세서가 계산할 수 없는 정말 큰 값에 도달하여 장애물이 되기 쉽습니다.\n\n\nCode\n손실을 개선하기 위해 각 가중치와 편향이 손실에 어떤 영향을 미치는지 배워야 합니다. 함수 체인에 대해 이를 수행하려면 *연쇄 법칙(chain rule)*을 사용합니다. 이는 함수 체인의 미분은 이 체인에 있는 모든 함수의 미분의 곱이라는 것입니다.\n\n매개변수(가중치 등)가 손실에 어떤 영향을 미치는지 계산하려면:\n\n1. 가장 바깥쪽 함수의 다음 함수에 대한 미분을 찾는 것으로 시작합니다.\n2. 여기에 그 앞의 함수에 대한 다음 함수의 미분을 곱합니다.\n3. 매개변수(가중치 또는 편향)에 도달할 때까지 체인의 각 함수의 미분을 계속 곱합니다.\n\n연쇄 법칙은 단일 입력이 최종 출력(이 경우 손실)에 어떤 영향을 미치는지 파악하는 데 필수적입니다.\n\n\nRuntimeWarning: overflow encountered in exp\n  np.exp(10000)\n\n\ninf\n\n\n우리는 모든 출력을 가져와 레이어의 가장 큰 값을 레이어의 모든 값에서 뺌으로써 신경망에서 이를 방지할 수 있습니다. 이제 가장 큰 값은 0이고 다른 모든 값은 0보다 작습니다.\n\n\n\n모델을 어떻게 최적화할 수 있을까요? 예측된 레이블과 실제 레이블의 정확도에 대해 최적화해야 할까요? 이것은 좋은 전략이지만, 출력이 단일 숫자가 아닌 확률 분포이기 때문에 많은 유용한 정보를 버리게 될 것입니다.\n범주형 교차 엔트로피(Categorical cross-entropy)는 신경망 분류 모델, 특히 다중 클래스 분류 작업에서 일반적으로 사용되는 손실 함수입니다. 이는 레이블의 실제 확률 분포와 모델이 출력한 예측 확률 분포 간의 비유사성을 측정합니다. 이 손실은 실제 확률과 예측 확률 간의 큰 오류에 불이익을 주어 모델이 올바른 클래스 레이블에 대해 더 높은 확률을 출력하도록 장려합니다.\n\\[ L(y, \\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij}) \\]\n여기서: - $ L(y, ) $는 범주형 교차 엔트로피 손실입니다. - $ N $은 데이터셋의 샘플 수입니다. - $ K $는 클래스 수입니다. - $ y $는 모양이 $ (N, K) $인 실제 원-핫 인코딩 레이블 행렬입니다. - $ $는 모델이 출력한 예측 확률 분포 행렬로 모양은 $ (N, K) $입니다. - $ y_{ij} $는 샘플 \\(i\\)가 클래스 $ j $에 속할 실제 확률입니다. - \\(\\hat{y}_{ij}\\)는 샘플 \\(i\\)가 클래스 \\(j\\)에 속할 예측 확률입니다.\n\n\nCode\n첫 번째 단계는 연쇄 법칙을 사용하여 각 매개변수와 입력에 대한 미분과 편미분을 계산하여 그래디언트를 역전파하는 것입니다. 우리가 직면한 중첩 함수는 아래와 같이 쓸 수 있습니다.\n\n\n0.35667494393873245\n\n\n\n\nCode\n{{IMG_0}}\n\n\n0.35667494393873245\n\n\n\n\nCode\n가중치와 편향에 대한 미분은 그것들의 영향에 대해 알려주며 가중치와 편향을 업데이트하는 데 사용됩니다. 입력에 대한 미분은 체인의 이전 함수로 전달하여 더 많은 레이어를 연결하는 데 사용됩니다.\n\n손실 함수의 미분을 계산하고 모든 연속 레이어의 모든 활성화 함수와 뉴런의 미분과 함께 연쇄 법칙을 적용해야 합니다. 가중치와 편향에 대한 미분은 매개변수를 업데이트하는 데 사용됩니다. 레이어 입력에 대한 미분은 다른 레이어로 연결하는 데 사용됩니다(이것이 체인의 이전 레이어로 역전파하는 이유입니다).\n\n\n0.35667494393873245 0.6931471805599453\n\n\n 세 이미지 배치에 대한 손실을 계산해 봅시다\n\n\nCode\nReLU() 미분을 취하고, 합산 연산의 미분을 취하고, 둘 다 곱하는 등... 거꾸로 작업하는 것이 연쇄 법칙을 사용한 역전파입니다. 결과 출력 함수의 그래디언트는 이후 레이어의 후속 함수 그래디언트와 현재 함수의 그래디언트를 곱하여 NN을 통해 다시 전달됩니다.\n\n이것은 입력, 가중치 및 편향에 대한 활성화된 뉴런의 편미분의 전체 세트입니다. 이 역전파 과정의 시각화를 보려면 [이 비디오](https://www.youtube.com/watch?v=_9qHQA30hys&t=2s)를 확인하세요.\n\n\n[0.7 0.5 0.9]\n\n\n\n\nCode\n# &lt;b&gt; &lt;font color='#569098'&gt; 섹션 7: 옵티마이저\n\n\n[0.35667494 0.69314718 0.10536052]\n0.38506088005216804\n\n\n우리는 결국 문제에 부딪힐 것입니다…  log(0)\n\n\nCode\n# &lt;b&gt; &lt;font color='#569098'&gt; 섹션 8: 학습률과 모멘텀\n\n\n[       inf 0.69314718 0.10536052]\n\n\nRuntimeWarning: divide by zero encountered in log\n  print(-np.log(softmax_outputs[[0, 1, 2], class_targets]))\n\n\n\n\nCode\n모델이 가능한 한 정확하기를 원하지만, 옵티마이저의 목표는 정확도를 직접 향상시키는 것이 아니라 손실을 최소화하는 것입니다. 손실은 개별 샘플 손실의 평균이므로 일부 손실은 상당히 감소할 수 있지만 다른 손실은 약간만 증가할 수 있으며, 이는 잘못된 예측으로 이어질 수 있습니다. 이는 전체 손실을 낮출 수 있지만 잘못 예측된 샘플의 수를 증가시켜 정확도를 낮출 수 있습니다.\n\n학습 초기에는 더 큰 업데이트가 솔루션 공간의 다른 영역을 탐색하는 데 도움이 되지만, 학습이 진행됨에 따라 더 작고 더 정교한 업데이트를 원합니다. 이를 달성하기 위해 **&lt;font color='#e59454'&gt;학습률 감쇠(learning rate decay)&lt;/font&gt;**를 구현합니다. 초기 학습률을 단계 수로 나누어 학습률이 처음에는 빠르게 떨어지고 나중에는 느려지도록 합니다. 이를 통해 모델은 최적의 솔루션에 가까워질수록 미세 조정을 할 수 있습니다.\n\n지난 섹션에서는 모델을 최적화하는 방법으로 **&lt;font color='#e59454'&gt;SGD&lt;/font&gt;**를 소개했습니다. 효과적일 수 있지만, 잠재적으로 모델이 손실 함수에 대한 전역 최소값을 찾는 데 도움이 될 수 있는 추가 논리 없이 그래디언트를 따르는 기본 방법입니다.\n\n**&lt;font color='#e59454'&gt;모멘텀(Momentum)&lt;/font&gt;**은 여러 업데이트에 걸쳐 그래디언트의 롤링 평균을 생성하고 이 평균을 현재 그래디언트와 결합하여 이 문제를 해결합니다. 이는 모델이 지역 최소값을 지나 손실을 더욱 줄이는 데 도움이 됩니다.\n\n\ninf\n\n\nRuntimeWarning: divide by zero encountered in log\n  print(np.mean(-np.log(softmax_outputs[[0, 1, 2], class_targets])))\n\n\n한 가지 옵션은 값을 아주 작은 양만큼 클리핑(clip)하는 것입니다.\n\n\nCode\n## &lt;b&gt;&lt;font color='#569098'&gt;모델 정의하기&lt;/b&gt;\n\n아래는 모델을 만드는 데 사용할 일련의 클래스입니다. 모든 클래스를 이해할 필요는 없지만 주석을 훑어보고 각각의 일반적인 목적을 확인하세요.\n\n\n16.11809565095832\n\n\n예측 값을 클리핑하면 이 무한대 문제에 빠지지 않도록 보장합니다.\n\n\nCode\n## &lt;b&gt;&lt;font color='#569098'&gt;모델 인스턴스화하기&lt;/b&gt;\n\nReLU 활성화 함수를 가진 두 개의 Dense(완전 연결) 레이어로 구성된 피드포워드 신경망을 사용할 것입니다. 첫 번째 레이어는 200개의 입력 특성을 받아 64개의 뉴런을 출력하고, 두 번째 레이어는 64개의 뉴런을 받아 3개의 뉴런을 출력합니다.\n\n\n그리고 정확도는 어떻게 계산할까요?\n\n\nCode\n## &lt;b&gt;&lt;font color='#569098'&gt;학습&lt;/b&gt;\n\n\n[2 1 1]\n0.6666666666666666\n\n\n\n\n\n가중치와 편향을 무작위로 검색하는 것은 비효율적입니다. 가중치와 편향을 조정하는 방법을 알기 위해서는 먼저 손실에 미치는 영향을 이해해야 합니다. 모든 입력, 가중치, 편향이 뉴런 출력과 손실 함수의 끝에 미치는 영향을 알기 위해서는, 뉴런과 전체 모델의 순전파 동안 수행되는 각 연산의 미분(derivative)을 계산해야 합니다.\n\n\n\nimage.png\n\n\n미분은 덧셈/곱셈에만 국한되지 않습니다. max()와 같이 순전파에 사용되는 다른 함수에 대해서도 미분을 유도해야 합니다.\n\n\n\nimage.png\n\n\n편미분(partial derivative)은 단일 입력이 함수의 출력에 얼마나 많은 영향을 미치는지 측정합니다.\n모든 매개변수의 편미분을 결합하면 결과 방정식 세트를 그래디언트(gradient)라고 합니다.\n\n\n\nimage.png\n\n\n아래 이미지와 유사하게 순전파는 함수의 체인입니다. 입력 데이터를 첫 번째 레이어로 전달하여 레이어의 가중치와 편향을 갖고 결과는 ReLU 활성화 함수를 통해 흐릅니다. 이 계산을 다른 레이어에서 반복하여 출력층과 Softmax 활성화까지 계속합니다.\n\n\n\nimage.png\n\n\n손실을 개선하기 위해 각 가중치와 편향이 손실에 어떤 영향을 미치는지 배워야 합니다. 함수 체인에 대해 이를 수행하려면 연쇄 법칙(chain rule)을 사용합니다. 이는 함수 체인의 미분은 이 체인에 있는 모든 함수의 미분의 곱이라는 것입니다.\n매개변수(가중치 등)가 손실에 어떤 영향을 미치는지 계산하려면:\n\n가장 바깥쪽 함수의 다음 함수에 대한 미분을 찾는 것으로 시작합니다.\n여기에 그 앞의 함수에 대한 다음 함수의 미분을 곱합니다.\n매개변수(가중치 또는 편향)에 도달할 때까지 체인의 각 함수의 미분을 계속 곱합니다.\n\n연쇄 법칙은 단일 입력이 최종 출력(이 경우 손실)에 어떤 영향을 미치는지 파악하는 데 필수적입니다.\n\n\n\nimage.png\n\n\n단일 뉴런에 대해 ReLU 함수를 역전파하고 이 단일 뉴런에 대한 출력을 최소화하려는 것처럼 행동해 봅시다. 우리는 미분과 편미분이 포함된 연쇄 법칙을 활용하여 각 변수가 ReLU 활성화 출력에 미치는 영향을 계산할 것입니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# neuron with 3 inputs\nx = np.array([1.0, -2.0, 3.0]) # inputs\nw = np.array([-3.0, -1.0, 2.0]) # weights\nb = 1.0 # bias\n\n## forward pass\n# input*weights\nxw0 = x[0] * w[0]\nxw1 = x[1] * w[1]\nxw2 = x[2] * w[2]\n\n# add bias\nz = xw0 + xw1 + xw2 + b\n\n# ReLU activation function\ny = max(z, 0)\nprint(y)\n\n\n6.0\n\n\n첫 번째 단계는 연쇄 법칙을 사용하여 각 매개변수와 입력에 대한 미분과 편미분을 계산하여 그래디언트를 역전파하는 것입니다. 우리가 직면한 중첩 함수는 아래와 같이 쓸 수 있습니다.\n\n\n\nimage.png\n\n\n가중치와 편향에 대한 미분은 그것들의 영향에 대해 알려주며 가중치와 편향을 업데이트하는 데 사용됩니다. 입력에 대한 미분은 체인의 이전 함수로 전달하여 더 많은 레이어를 연결하는 데 사용됩니다.\n손실 함수의 미분을 계산하고 모든 연속 레이어의 모든 활성화 함수와 뉴런의 미분과 함께 연쇄 법칙을 적용해야 합니다. 가중치와 편향에 대한 미분은 매개변수를 업데이트하는 데 사용됩니다. 레이어 입력에 대한 미분은 다른 레이어로 연결하는 데 사용됩니다(이것이 체인의 이전 레이어로 역전파하는 이유입니다).\n역전파(Backward pass)\n다음 레이어로부터의 미분:\n\\(\\frac{d y}{d y} = 1.0\\)\nReLU의 미분 및 연쇄 법칙:\n\\((1.0) * \\frac{d}{d z}ReLU(z) = 1(z&gt;0)=1(6&gt;0)= 1.0 * 1.0 = 1.0\\)\n합계의 편미분 및 연쇄 법칙:\n\\(1.0 * \\frac{\\delta}{\\delta (x_0 w_0)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta (x_1 w_1)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta (x_2 w_2)} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n\\(1.0 * \\frac{\\delta}{\\delta b} (x_0w_0 + x_1w_1 + x_2w_2 + b) = (1.0) * (1.0) = 1.0\\)\n곱셈의 편미분 및 연쇄 법칙:\n\\((1.0) * \\frac{\\delta}{\\delta x_0}(x_0w_0) = (1.0) * w_0 = -3.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_0}(x_0w_0) = (1.0) * x_0 = 1.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta x_1}(x_0w_0) = (1.0) * w_1 = -1.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_1}(x_0w_0) = (1.0) * x_1 = -2.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta x_2}(x_0w_0) = (1.0) * w_2 = 2.0\\)\n\\((1.0) * \\frac{\\delta}{\\delta w_2}(x_0w_0) = (1.0) * x_2 = 3.0\\)\n\n\nCode\n## backward pass\n# derivative from the next layer\ndvalue = 1.0 # ∂y / ∂y = 1\nprint(dvalue)\n\n# derivative of ReLU and chain rule\n# (∂y / ∂y) * (∂ ReLU(z) / ∂\ndrelu_dz = dvalue * (1.0 if z &gt; 0 else 0.)\nprint(drelu_dz)\n\n# partial derivatives of summation, the chain rule\ndsum_dxw0 = 1\ndsum_dxw1 = 1\ndsum_dxw2 = 1\ndsum_db = 1\ndrelu_dxw0 = drelu_dz * dsum_dxw0\ndrelu_dxw1 = drelu_dz * dsum_dxw1\ndrelu_dxw2 = drelu_dz * dsum_dxw2\ndrelu_db = drelu_dz * dsum_db\nprint(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n\n# Partial derivatives of the multiplication and chain rule\ndmul_dx0 = w[0]\ndmul_dx1 = w[1]\ndmul_dx2 = w[2]\ndmul_dw0 = x[0]\ndmul_dw1 = x[1]\ndmul_dw2 = x[2]\ndrelu_dx0 = drelu_dxw0 * dmul_dx0\ndrelu_dw0 = drelu_dxw0 * dmul_dw0\ndrelu_dx1 = drelu_dxw1 * dmul_dx1\ndrelu_dw1 = drelu_dxw1 * dmul_dw1\ndrelu_dx2 = drelu_dxw2 * dmul_dx2\ndrelu_dw2 = drelu_dxw2 * dmul_dw2\nprint(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n\n\n1.0\n1.0\n1.0 1.0 1.0 1.0\n-3.0 1.0 -1.0 -2.0 2.0 3.0\n\n\nReLU() 미분을 취하고, 합산 연산의 미분을 취하고, 둘 다 곱하는 등… 거꾸로 작업하는 것이 연쇄 법칙을 사용한 역전파입니다. 결과 출력 함수의 그래디언트는 이후 레이어의 후속 함수 그래디언트와 현재 함수의 그래디언트를 곱하여 NN을 통해 다시 전달됩니다.\n이것은 입력, 가중치 및 편향에 대한 활성화된 뉴런의 편미분의 전체 세트입니다. 이 역전파 과정의 시각화를 보려면 이 비디오를 확인하세요.\n\n\n\n우리는 계산된 그래디언트를 사용하여 가중치와 편향을 조정하여 궁극적으로 손실 측정값을 줄이고자 합니다. 이런 방식으로 뉴런의 활성화 함수(ReLU) 출력을 성공적으로 줄일 수 있습니다. 이전 접근 방식에서는 이 조정을 달성하기 위해 각 가중치와 편향에서 그래디언트의 일부를 뺐습니다. 수백만, 수십억, 심지어 더 많은 차원이 있는 경우, 확률적 경사 하강법(SGD)은 전역 최소값을 찾는 가장 잘 알려진 방법입니다.\n\n\n\n모델이 가능한 한 정확하기를 원하지만, 옵티마이저의 목표는 정확도를 직접 향상시키는 것이 아니라 손실을 최소화하는 것입니다. 손실은 개별 샘플 손실의 평균이므로 일부 손실은 상당히 감소할 수 있지만 다른 손실은 약간만 증가할 수 있으며, 이는 잘못된 예측으로 이어질 수 있습니다. 이는 전체 손실을 낮출 수 있지만 잘못 예측된 샘플의 수를 증가시켜 정확도를 낮출 수 있습니다.\n학습 초기에는 더 큰 업데이트가 솔루션 공간의 다른 영역을 탐색하는 데 도움이 되지만, 학습이 진행됨에 따라 더 작고 더 정교한 업데이트를 원합니다. 이를 달성하기 위해 학습률 감쇠(learning rate decay)를 구현합니다. 초기 학습률을 단계 수로 나누어 학습률이 처음에는 빠르게 떨어지고 나중에는 느려지도록 합니다. 이를 통해 모델은 최적의 솔루션에 가까워질수록 미세 조정을 할 수 있습니다.\n지난 섹션에서는 모델을 최적화하는 방법으로 SGD를 소개했습니다. 효과적일 수 있지만, 잠재적으로 모델이 손실 함수에 대한 전역 최소값을 찾는 데 도움이 될 수 있는 추가 논리 없이 그래디언트를 따르는 기본 방법입니다.\n모멘텀(Momentum)은 여러 업데이트에 걸쳐 그래디언트의 롤링 평균을 생성하고 이 평균을 현재 그래디언트와 결합하여 이 문제를 해결합니다. 이는 모델이 지역 최소값을 지나 손실을 더욱 줄이는 데 도움이 됩니다.\n\n\n\n배운 개념을 간단한 신경망에 구현해 봅시다. 이 워크숍의 의도는 기계 학습의 모든 측면을 다루는 것이 아니라 신경망 이면의 몇 가지 핵심 개념을 소개하는 것이었습니다(확실히 이해하려면 여러 워크숍과 여러 아키텍처 및 데이터셋을 다루는 경험이 필요합니다). 첫 번째 패스가 위협적일 수 있지만, 모델을 인스턴스화하고 학습시키는 것을 관찰하려면 아래의 각 셀을 실행해 보세요. 그렇게 한 후, 코드를 다시 한 번 살펴보세요.\n\n\n아래는 모델을 만드는 데 사용할 일련의 클래스입니다. 모든 클래스를 이해할 필요는 없지만 주석을 훑어보고 각각의 일반적인 목적을 확인하세요.\n\n\nCode\n# Dense layer\nclass Layer_Dense:\n  \"\"\"\n  Dense layer of a neural network\n  Facilitates:\n  - Forward propogation of data throught layer\n  - Backward propogation of gradients during training\n  \"\"\"\n\n  # Layer initialization\n  def __init__(self, n_inputs, n_neurons):\n    # Initialize weights and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1, n_neurons))\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs, weights and biases\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Gradients on parameters\n    self.dweights = np.dot(self.inputs.T, dvalues)\n    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n    # Gradient on values\n    self.dinputs = np.dot(dvalues, self.weights.T)\n\n\n# ReLU activation\nclass Activation_ReLU:\n  \"\"\"\n  Rectified linear unit activation function\n  Applied to input of neural network layer\n  Introduces non-linearity into the network\n  \"\"\"\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs\n    self.output = np.maximum(0, inputs)\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Since we need to modify original variable,\n    # let’s make a copy of values first\n    self.dinputs = dvalues.copy()\n    # Zero gradient where input values were negative\n    self.dinputs[self.inputs &lt;= 0] = 0\n\n\n# Softmax classifier - combined Softmax activation\n# and cross-entropy loss for faster backward step\nclass Activation_Softmax_Loss_CategoricalCrossentropy():\n  \"\"\"\n  Combination of softmax activation function and categorical cross entropy loss function\n  Commonly used in classification tasks\n  We minimize loss by adjustng model parameters to improve performance\n  \"\"\"\n\n  # create activation and loss function objectives\n  def __init__(self):\n    self.activation = Activation_Softmax()\n    self.loss = Loss_CategoricalCrossentropy()\n\n  # forward pass\n  def forward(self, inputs, y_true):\n    # output layer's activation function\n    self.activation.forward(inputs)\n    # set the output\n    self.output = self.activation.output\n    # calculate and return loss value\n    return self.loss.calculate(self.output, y_true)\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n\n    # if labels one-hot encoded, turn into discrete values\n    if len(y_true.shape) == 2:\n      y_true = np.argmax(y_true, axis=1)\n\n    # copy so we can safely modify\n    self.dinputs = dvalues.copy()\n    # Calculate gradient\n    self.dinputs[range(samples), y_true] -= 1\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n# Adam optimizer\nclass Optimizer_Adam:\n\n  \"\"\"\n  Adam optimization algorithm to optimize parameters of neural network\n  Initalize with learning rate, decay, epsilon, momentum\n  Pre-update params: Adjust learning rate based on decay\n  Update params: Update params using momentum and cache corrections\n  Post-update params: Track number of optimization steps performed\n  \"\"\"\n\n  # Initialize optimizer - set settings\n  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n    self.learning_rate = learning_rate\n    self.current_learning_rate = learning_rate\n    self.decay = decay\n    self.iterations = 0\n    self.epsilon = epsilon\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n\n  # Call once before any parameter updates\n  def pre_update_params(self):\n    if self.decay:\n      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n\n  # Update parameters\n  def update_params(self, layer):\n\n    # If layer does not contain cache arrays, create them filled with zeros\n    if not hasattr(layer, 'weight_cache'):\n      layer.weight_momentums = np.zeros_like(layer.weights)\n      layer.weight_cache = np.zeros_like(layer.weights)\n      layer.bias_momentums = np.zeros_like(layer.biases)\n      layer.bias_cache = np.zeros_like(layer.biases)\n\n    # Update momentum with current gradients\n    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n\n    # Get corrected momentum\n    # self.iteration is 0 at first pass\n    # and we need to start with 1 here\n    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n\n    # update cache with squared current gradients\n    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n\n    # get corrected cache\n    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n\n    # Vanilla SGD parameter update + normalization with square root cache\n    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n\n  # call once after any parameter updates\n  def post_update_params(self):\n    self.iterations += 1\n\n\n# Softmax activation\nclass Activation_Softmax:\n\n  \"\"\"\n  Softmax activation function for multi-class classification\n  Compute probabilities for each class\n  \"\"\"\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Get unnormalized probabilities\n    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n\n    # Normalize them for each sample\n    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\n    self.output = probabilities\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Create uninitialized array\n    self.dinputs = np.empty_like(dvalues)\n\n    # Enumerate outputs and gradients\n    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n      # Flatten output array\n      single_output = single_output.reshape(-1, 1)\n\n      # Calculate Jacobian matrix of the output\n      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n\n      # Calculate sample-wise gradient and add it to the array of sample gradients\n      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n\n\n# Common loss class\nclass Loss:\n\n  # calculates data and regularization losses, given model output and ground truth values\n  def calculate(self, output, y):\n\n    # calculate sample losses\n    sample_losses = self.forward(output, y)\n\n    # calculate mean losses\n    data_loss = np.mean(sample_losses)\n\n    # return loss\n    return data_loss\n\n\n# cross entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n  \"\"\"\n  Computes categorical cross entropy\n  Quantifies discrepency between predicted and true class probabilities\n  \"\"\"\n\n  # forward pass\n  def forward(self, y_pred, y_true):\n\n    # number samples in batch\n    samples = len(y_pred)\n\n    # clip data to prevent division by 0\n    # clip both sides to not drag mean towards any value\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n    # probabilities for target values (only if categorical labels)\n    if len(y_true.shape) == 1:\n      correct_confidences = y_pred_clipped[ range(samples), y_true ]\n\n    # mask values (only for one-hot encoded labels)\n    elif len(y_true.shape) == 2:\n      correct_confidences = np.sum( y_pred_clipped * y_true, axis=1 )\n\n    # losses\n    negative_log_likelihoods = -np.log(correct_confidences)\n    return negative_log_likelihoods\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n    # Number of labels in every sample\n    # We’ll use the first sample to count them\n    labels = len(dvalues[0])\n\n    if len(y_true.shape) == 1:\n      y_true = np.eye(labels)[y_true]\n\n    # calculate gradient\n    self.dinputs = -y_true / dvalues\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n\n\n\n우리는 입력 \\(X\\)(이전 워크숍과 같은 두 개의 설명적 특성인 x와 y 좌표)가 주어졌을 때 점의 색상(y \\(ϵ\\) \\(\\{\\)초록, 빨강, 또는 파랑\\(\\}\\) )을 예측하도록 신경망을 학습시킬 것입니다.\n\n\nCode\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# an example feature set (x,y) and it's label (color)\nX[5], y[5]\n\n\n\n\n\nReLU 활성화 함수를 가진 두 개의 Dense(완전 연결) 레이어로 구성된 피드포워드 신경망을 사용할 것입니다. 첫 번째 레이어는 200개의 입력 특성을 받아 64개의 뉴런을 출력하고, 두 번째 레이어는 64개의 뉴런을 받아 3개의 뉴런을 출력합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# Create Dense layer with 2 input features and 64 output values\ndense1 = Layer_Dense(2, 64)\n\n# Create ReLU activation (to be used with Dense layer):\nactivation1 = Activation_ReLU()\n\n# Create second Dense layer with 64 input features (as we take output\n# of previous layer here) and 3 output values (output values)\ndense2 = Layer_Dense(64, 3)\n\n# Create Softmax classifier’s combined loss and activation\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n\n# Create optimizer\noptimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n\n\n\n\n\n\n\nCode\n# Train in loop\nfor epoch in range(10001):\n  # Perform a forward pass of our training data through this layer\n  dense1.forward(X)\n\n  # Perform a forward pass through activation function\n  # takes the output of first dense layer here\n  activation1.forward(dense1.output)\n\n  # Perform a forward pass through second Dense layer\n  # takes outputs of activation function of first layer as inputs\n  dense2.forward(activation1.output)\n\n  # Perform a forward pass through the activation/loss function\n  # takes the output of second dense layer here and returns loss\n  loss = loss_activation.forward(dense2.output, y)\n\n  # Calculate accuracy from output of activation2 and targets\n  # calculate values along first axis\n  predictions = np.argmax(loss_activation.output, axis=1)\n\n  if len(y.shape) == 2:\n    y = np.argmax(y, axis=1)\n\n  accuracy = np.mean(predictions==y)\n\n  if not epoch % 100:\n    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n\n  # backward pass\n  loss_activation.backward(loss_activation.output, y)\n  dense2.backward(loss_activation.dinputs)\n  activation1.backward(dense2.dinputs)\n  dense1.backward(activation1.dinputs)\n\n  # update weights and biases\n  optimizer.pre_update_params()\n  optimizer.update_params(dense1)\n  optimizer.update_params(dense2)\n  optimizer.post_update_params()\n\n\n\nepoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.670, loss: 0.705, lr: 0.04999752512250644\nepoch: 200, acc: 0.797, loss: 0.522, lr: 0.04999502549496326\nepoch: 300, acc: 0.847, loss: 0.430, lr: 0.049992526117345455\nepoch: 400, acc: 0.887, loss: 0.344, lr: 0.04999002698961558\nepoch: 500, acc: 0.910, loss: 0.303, lr: 0.049987528111736124\nepoch: 600, acc: 0.907, loss: 0.276, lr: 0.049985029483669646\nepoch: 700, acc: 0.917, loss: 0.252, lr: 0.049982531105378675\nepoch: 800, acc: 0.920, loss: 0.245, lr: 0.04998003297682575\nepoch: 900, acc: 0.930, loss: 0.228, lr: 0.049977535097973466\nepoch: 1000, acc: 0.940, loss: 0.217, lr: 0.049975037468784345\nepoch: 1100, acc: 0.937, loss: 0.205, lr: 0.049972540089220974\nepoch: 1200, acc: 0.947, loss: 0.192, lr: 0.04997004295924593\nepoch: 1300, acc: 0.947, loss: 0.184, lr: 0.04996754607882181\nepoch: 1400, acc: 0.943, loss: 0.183, lr: 0.049965049447911185\nepoch: 1500, acc: 0.943, loss: 0.189, lr: 0.04996255306647668\nepoch: 1600, acc: 0.943, loss: 0.165, lr: 0.049960056934480884\nepoch: 1700, acc: 0.943, loss: 0.161, lr: 0.04995756105188642\nepoch: 1800, acc: 0.943, loss: 0.158, lr: 0.049955065418655915\nepoch: 1900, acc: 0.943, loss: 0.155, lr: 0.04995257003475201\nepoch: 2000, acc: 0.947, loss: 0.151, lr: 0.04995007490013731\nepoch: 2100, acc: 0.943, loss: 0.148, lr: 0.0499475800147745\nepoch: 2200, acc: 0.953, loss: 0.145, lr: 0.0499450853786262\nepoch: 2300, acc: 0.957, loss: 0.142, lr: 0.0499425909916551\nepoch: 2400, acc: 0.957, loss: 0.138, lr: 0.04994009685382384\nepoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\nepoch: 2600, acc: 0.960, loss: 0.132, lr: 0.049935109325431604\nepoch: 2700, acc: 0.957, loss: 0.130, lr: 0.049932615934796004\nepoch: 2800, acc: 0.937, loss: 0.159, lr: 0.04993012279315098\nepoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\nepoch: 3000, acc: 0.960, loss: 0.123, lr: 0.049925137256683606\nepoch: 3100, acc: 0.960, loss: 0.121, lr: 0.04992264486178666\nepoch: 3200, acc: 0.960, loss: 0.119, lr: 0.04992015271573119\nepoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\nepoch: 3400, acc: 0.960, loss: 0.115, lr: 0.049915169169995596\nepoch: 3500, acc: 0.950, loss: 0.131, lr: 0.049912677770240964\nepoch: 3600, acc: 0.957, loss: 0.118, lr: 0.049910186619178794\nepoch: 3700, acc: 0.960, loss: 0.113, lr: 0.04990769571677183\nepoch: 3800, acc: 0.960, loss: 0.111, lr: 0.04990520506298287\nepoch: 3900, acc: 0.960, loss: 0.110, lr: 0.04990271465777467\nepoch: 4000, acc: 0.960, loss: 0.109, lr: 0.049900224501110035\nepoch: 4100, acc: 0.960, loss: 0.107, lr: 0.04989773459295174\nepoch: 4200, acc: 0.960, loss: 0.106, lr: 0.04989524493326262\nepoch: 4300, acc: 0.960, loss: 0.104, lr: 0.04989275552200545\nepoch: 4400, acc: 0.960, loss: 0.103, lr: 0.04989026635914307\nepoch: 4500, acc: 0.960, loss: 0.102, lr: 0.04988777744463829\nepoch: 4600, acc: 0.960, loss: 0.101, lr: 0.049885288778453954\nepoch: 4700, acc: 0.960, loss: 0.100, lr: 0.049882800360552884\nepoch: 4800, acc: 0.960, loss: 0.099, lr: 0.04988031219089794\nepoch: 4900, acc: 0.960, loss: 0.098, lr: 0.049877824269451976\nepoch: 5000, acc: 0.960, loss: 0.098, lr: 0.04987533659617785\nepoch: 5100, acc: 0.967, loss: 0.107, lr: 0.04987284917103844\nepoch: 5200, acc: 0.960, loss: 0.097, lr: 0.04987036199399661\nepoch: 5300, acc: 0.960, loss: 0.096, lr: 0.04986787506501525\nepoch: 5400, acc: 0.960, loss: 0.095, lr: 0.04986538838405724\nepoch: 5500, acc: 0.960, loss: 0.094, lr: 0.049862901951085496\nepoch: 5600, acc: 0.960, loss: 0.094, lr: 0.049860415766062906\nepoch: 5700, acc: 0.960, loss: 0.093, lr: 0.0498579298289524\nepoch: 5800, acc: 0.960, loss: 0.092, lr: 0.04985544413971689\nepoch: 5900, acc: 0.960, loss: 0.092, lr: 0.049852958698319315\nepoch: 6000, acc: 0.960, loss: 0.091, lr: 0.04985047350472258\nepoch: 6100, acc: 0.960, loss: 0.090, lr: 0.04984798855888967\nepoch: 6200, acc: 0.960, loss: 0.089, lr: 0.049845503860783506\nepoch: 6300, acc: 0.960, loss: 0.089, lr: 0.049843019410367055\nepoch: 6400, acc: 0.960, loss: 0.088, lr: 0.04984053520760327\nepoch: 6500, acc: 0.953, loss: 0.126, lr: 0.049838051252455155\nepoch: 6600, acc: 0.960, loss: 0.090, lr: 0.049835567544885655\nepoch: 6700, acc: 0.960, loss: 0.088, lr: 0.04983308408485778\nepoch: 6800, acc: 0.960, loss: 0.088, lr: 0.0498306008723345\nepoch: 6900, acc: 0.960, loss: 0.087, lr: 0.04982811790727884\nepoch: 7000, acc: 0.960, loss: 0.086, lr: 0.04982563518965381\nepoch: 7100, acc: 0.960, loss: 0.086, lr: 0.049823152719422406\nepoch: 7200, acc: 0.960, loss: 0.085, lr: 0.049820670496547675\nepoch: 7300, acc: 0.960, loss: 0.085, lr: 0.04981818852099264\nepoch: 7400, acc: 0.960, loss: 0.084, lr: 0.049815706792720335\nepoch: 7500, acc: 0.960, loss: 0.083, lr: 0.0498132253116938\nepoch: 7600, acc: 0.960, loss: 0.083, lr: 0.04981074407787611\nepoch: 7700, acc: 0.960, loss: 0.082, lr: 0.049808263091230306\nepoch: 7800, acc: 0.960, loss: 0.082, lr: 0.04980578235171948\nepoch: 7900, acc: 0.963, loss: 0.081, lr: 0.04980330185930667\nepoch: 8000, acc: 0.963, loss: 0.081, lr: 0.04980082161395499\nepoch: 8100, acc: 0.963, loss: 0.080, lr: 0.04979834161562752\nepoch: 8200, acc: 0.963, loss: 0.084, lr: 0.04979586186428736\nepoch: 8300, acc: 0.963, loss: 0.080, lr: 0.04979338235989761\nepoch: 8400, acc: 0.963, loss: 0.079, lr: 0.04979090310242139\nepoch: 8500, acc: 0.963, loss: 0.079, lr: 0.049788424091821805\nepoch: 8600, acc: 0.963, loss: 0.078, lr: 0.049785945328062006\nepoch: 8700, acc: 0.963, loss: 0.078, lr: 0.0497834668111051\nepoch: 8800, acc: 0.963, loss: 0.077, lr: 0.049780988540914256\nepoch: 8900, acc: 0.963, loss: 0.077, lr: 0.0497785105174526\nepoch: 9000, acc: 0.963, loss: 0.076, lr: 0.04977603274068329\nepoch: 9100, acc: 0.963, loss: 0.076, lr: 0.04977355521056952\nepoch: 9200, acc: 0.963, loss: 0.075, lr: 0.049771077927074414\nepoch: 9300, acc: 0.963, loss: 0.075, lr: 0.0497686008901612\nepoch: 9400, acc: 0.707, loss: 1.982, lr: 0.04976612409979302\nepoch: 9500, acc: 0.963, loss: 0.079, lr: 0.0497636475559331\nepoch: 9600, acc: 0.963, loss: 0.076, lr: 0.049761171258544616\nepoch: 9700, acc: 0.963, loss: 0.076, lr: 0.0497586952075908\nepoch: 9800, acc: 0.963, loss: 0.075, lr: 0.04975621940303483\nepoch: 9900, acc: 0.963, loss: 0.074, lr: 0.049753743844839965\nepoch: 10000, acc: 0.967, loss: 0.074, lr: 0.04975126853296942\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Assuming you have access to the weights and biases of your trained model\n# For the weights and biases of dense1 and dense2 layers\nW1, b1 = dense1.weights, dense1.biases\nW2, b2 = dense2.weights, dense2.biases\n\n# Create a meshgrid of points covering the feature space\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Flatten the meshgrid points and apply the first dense layer and ReLU activation\npoints = np.c_[xx.ravel(), yy.ravel()]\nz1 = np.dot(points, W1) + b1\na1 = np.maximum(0, z1)\n\n# Apply the second dense layer\nz2 = np.dot(a1, W2) + b2\n\n# Apply softmax activation to get probabilities\nexp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n# Predictions\npredictions = np.argmax(probs, axis=1)\nZ = predictions.reshape(xx.shape)\n\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\nNumpy에서 처음부터 완전한 신경망을 학습시켰습니다(휴!). 이것은 꼼꼼한 작업이었지만, PyTorch와 같은 딥러닝 패키지에서 내부적으로 수행되는 작업의 양을 이해하는 데 도움이 될 것입니다. 이것이 신경망 아키텍처 및 학습의 해부학적 구조의 모든 측면에 대한 광범위한 개요는 아니었지만, (희망컨대) 주요 입문 개념 중 일부를 조금 더 잘 이해하게 되었기를 바랍니다. 준비가 되면 WS4_IntroToNNs_PyTorch.ipynb에서 PyTorch로 동일한 아키텍처를 재구현한 것도 확인해 보세요."
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#배치batches에-대한-코멘트",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#배치batches에-대한-코멘트",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "배치를 사용하면 병렬로 계산할 수 있습니다. 배치가 클수록 더 많은 병렬 연산을 실행할 수 있습니다. 이것이 바로 우리가 CPU의 4-8코어 대신 계산을 실행할 수 있는 수백-수천 개의 코어가 있는 GPU에서 신경망 학습을 수행하는 경향이 있는 이유이기도 합니다. 배치는 일반화(generalization)에도 도움이 됩니다. 한 번에 하나씩 또는 모든 포인트를 한 번에 노출시키는 대신 한 번에 여러 데이터 포인트를 네트워크에 노출시키는 것은 신경망이 일반화하는 데 도움이 되는 것으로 널리 이해되고 있습니다. 일반적으로 우리는 32, 64, 아마도 128의 배치 크기로 작업할 것입니다."
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#모델-정의하기",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#모델-정의하기",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "아래는 모델을 만드는 데 사용할 일련의 클래스입니다. 모든 클래스를 이해할 필요는 없지만 주석을 훑어보고 각각의 일반적인 목적을 확인하세요.\n\n\nCode\n# Dense layer\nclass Layer_Dense:\n  \"\"\"\n  Dense layer of a neural network\n  Facilitates:\n  - Forward propogation of data throught layer\n  - Backward propogation of gradients during training\n  \"\"\"\n\n  # Layer initialization\n  def __init__(self, n_inputs, n_neurons):\n    # Initialize weights and biases\n    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n    self.biases = np.zeros((1, n_neurons))\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs, weights and biases\n    self.output = np.dot(inputs, self.weights) + self.biases\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Gradients on parameters\n    self.dweights = np.dot(self.inputs.T, dvalues)\n    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n    # Gradient on values\n    self.dinputs = np.dot(dvalues, self.weights.T)\n\n\n# ReLU activation\nclass Activation_ReLU:\n  \"\"\"\n  Rectified linear unit activation function\n  Applied to input of neural network layer\n  Introduces non-linearity into the network\n  \"\"\"\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Calculate output values from inputs\n    self.output = np.maximum(0, inputs)\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Since we need to modify original variable,\n    # let’s make a copy of values first\n    self.dinputs = dvalues.copy()\n    # Zero gradient where input values were negative\n    self.dinputs[self.inputs &lt;= 0] = 0\n\n\n# Softmax classifier - combined Softmax activation\n# and cross-entropy loss for faster backward step\nclass Activation_Softmax_Loss_CategoricalCrossentropy():\n  \"\"\"\n  Combination of softmax activation function and categorical cross entropy loss function\n  Commonly used in classification tasks\n  We minimize loss by adjustng model parameters to improve performance\n  \"\"\"\n\n  # create activation and loss function objectives\n  def __init__(self):\n    self.activation = Activation_Softmax()\n    self.loss = Loss_CategoricalCrossentropy()\n\n  # forward pass\n  def forward(self, inputs, y_true):\n    # output layer's activation function\n    self.activation.forward(inputs)\n    # set the output\n    self.output = self.activation.output\n    # calculate and return loss value\n    return self.loss.calculate(self.output, y_true)\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n\n    # if labels one-hot encoded, turn into discrete values\n    if len(y_true.shape) == 2:\n      y_true = np.argmax(y_true, axis=1)\n\n    # copy so we can safely modify\n    self.dinputs = dvalues.copy()\n    # Calculate gradient\n    self.dinputs[range(samples), y_true] -= 1\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples\n\n\n# Adam optimizer\nclass Optimizer_Adam:\n\n  \"\"\"\n  Adam optimization algorithm to optimize parameters of neural network\n  Initalize with learning rate, decay, epsilon, momentum\n  Pre-update params: Adjust learning rate based on decay\n  Update params: Update params using momentum and cache corrections\n  Post-update params: Track number of optimization steps performed\n  \"\"\"\n\n  # Initialize optimizer - set settings\n  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n    self.learning_rate = learning_rate\n    self.current_learning_rate = learning_rate\n    self.decay = decay\n    self.iterations = 0\n    self.epsilon = epsilon\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n\n  # Call once before any parameter updates\n  def pre_update_params(self):\n    if self.decay:\n      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n\n  # Update parameters\n  def update_params(self, layer):\n\n    # If layer does not contain cache arrays, create them filled with zeros\n    if not hasattr(layer, 'weight_cache'):\n      layer.weight_momentums = np.zeros_like(layer.weights)\n      layer.weight_cache = np.zeros_like(layer.weights)\n      layer.bias_momentums = np.zeros_like(layer.biases)\n      layer.bias_cache = np.zeros_like(layer.biases)\n\n    # Update momentum with current gradients\n    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n\n    # Get corrected momentum\n    # self.iteration is 0 at first pass\n    # and we need to start with 1 here\n    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n\n    # update cache with squared current gradients\n    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n\n    # get corrected cache\n    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n\n    # Vanilla SGD parameter update + normalization with square root cache\n    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n\n  # call once after any parameter updates\n  def post_update_params(self):\n    self.iterations += 1\n\n\n# Softmax activation\nclass Activation_Softmax:\n\n  \"\"\"\n  Softmax activation function for multi-class classification\n  Compute probabilities for each class\n  \"\"\"\n\n  # Forward pass\n  def forward(self, inputs):\n    # Remember input values\n    self.inputs = inputs\n    # Get unnormalized probabilities\n    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n\n    # Normalize them for each sample\n    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n\n    self.output = probabilities\n\n  # Backward pass\n  def backward(self, dvalues):\n    # Create uninitialized array\n    self.dinputs = np.empty_like(dvalues)\n\n    # Enumerate outputs and gradients\n    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n      # Flatten output array\n      single_output = single_output.reshape(-1, 1)\n\n      # Calculate Jacobian matrix of the output\n      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n\n      # Calculate sample-wise gradient and add it to the array of sample gradients\n      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n\n\n# Common loss class\nclass Loss:\n\n  # calculates data and regularization losses, given model output and ground truth values\n  def calculate(self, output, y):\n\n    # calculate sample losses\n    sample_losses = self.forward(output, y)\n\n    # calculate mean losses\n    data_loss = np.mean(sample_losses)\n\n    # return loss\n    return data_loss\n\n\n# cross entropy loss\nclass Loss_CategoricalCrossentropy(Loss):\n  \"\"\"\n  Computes categorical cross entropy\n  Quantifies discrepency between predicted and true class probabilities\n  \"\"\"\n\n  # forward pass\n  def forward(self, y_pred, y_true):\n\n    # number samples in batch\n    samples = len(y_pred)\n\n    # clip data to prevent division by 0\n    # clip both sides to not drag mean towards any value\n    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n\n    # probabilities for target values (only if categorical labels)\n    if len(y_true.shape) == 1:\n      correct_confidences = y_pred_clipped[ range(samples), y_true ]\n\n    # mask values (only for one-hot encoded labels)\n    elif len(y_true.shape) == 2:\n      correct_confidences = np.sum( y_pred_clipped * y_true, axis=1 )\n\n    # losses\n    negative_log_likelihoods = -np.log(correct_confidences)\n    return negative_log_likelihoods\n\n  # backward pass\n  def backward(self, dvalues, y_true):\n\n    # number of samples\n    samples = len(dvalues)\n    # Number of labels in every sample\n    # We’ll use the first sample to count them\n    labels = len(dvalues[0])\n\n    if len(y_true.shape) == 1:\n      y_true = np.eye(labels)[y_true]\n\n    # calculate gradient\n    self.dinputs = -y_true / dvalues\n    # Normalize gradient\n    self.dinputs = self.dinputs / samples"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#데이터셋-생성하기",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#데이터셋-생성하기",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "우리는 입력 \\(X\\)(이전 워크숍과 같은 두 개의 설명적 특성인 x와 y 좌표)가 주어졌을 때 점의 색상(y \\(ϵ\\) \\(\\{\\)초록, 빨강, 또는 파랑\\(\\}\\) )을 예측하도록 신경망을 학습시킬 것입니다.\n\n\nCode\n# Create dataset\nX, y = spiral_data(samples=100, classes=3)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# an example feature set (x,y) and it's label (color)\nX[5], y[5]"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#모델-인스턴스화하기",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#모델-인스턴스화하기",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "ReLU 활성화 함수를 가진 두 개의 Dense(완전 연결) 레이어로 구성된 피드포워드 신경망을 사용할 것입니다. 첫 번째 레이어는 200개의 입력 특성을 받아 64개의 뉴런을 출력하고, 두 번째 레이어는 64개의 뉴런을 받아 3개의 뉴런을 출력합니다.\n\n\n\nimage.png\n\n\n\n\nCode\n# Create Dense layer with 2 input features and 64 output values\ndense1 = Layer_Dense(2, 64)\n\n# Create ReLU activation (to be used with Dense layer):\nactivation1 = Activation_ReLU()\n\n# Create second Dense layer with 64 input features (as we take output\n# of previous layer here) and 3 output values (output values)\ndense2 = Layer_Dense(64, 3)\n\n# Create Softmax classifier’s combined loss and activation\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n\n# Create optimizer\noptimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#학습",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#학습",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "Code\n# Train in loop\nfor epoch in range(10001):\n  # Perform a forward pass of our training data through this layer\n  dense1.forward(X)\n\n  # Perform a forward pass through activation function\n  # takes the output of first dense layer here\n  activation1.forward(dense1.output)\n\n  # Perform a forward pass through second Dense layer\n  # takes outputs of activation function of first layer as inputs\n  dense2.forward(activation1.output)\n\n  # Perform a forward pass through the activation/loss function\n  # takes the output of second dense layer here and returns loss\n  loss = loss_activation.forward(dense2.output, y)\n\n  # Calculate accuracy from output of activation2 and targets\n  # calculate values along first axis\n  predictions = np.argmax(loss_activation.output, axis=1)\n\n  if len(y.shape) == 2:\n    y = np.argmax(y, axis=1)\n\n  accuracy = np.mean(predictions==y)\n\n  if not epoch % 100:\n    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n\n  # backward pass\n  loss_activation.backward(loss_activation.output, y)\n  dense2.backward(loss_activation.dinputs)\n  activation1.backward(dense2.dinputs)\n  dense1.backward(activation1.dinputs)\n\n  # update weights and biases\n  optimizer.pre_update_params()\n  optimizer.update_params(dense1)\n  optimizer.update_params(dense2)\n  optimizer.post_update_params()\n\n\n\nepoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\nepoch: 100, acc: 0.670, loss: 0.705, lr: 0.04999752512250644\nepoch: 200, acc: 0.797, loss: 0.522, lr: 0.04999502549496326\nepoch: 300, acc: 0.847, loss: 0.430, lr: 0.049992526117345455\nepoch: 400, acc: 0.887, loss: 0.344, lr: 0.04999002698961558\nepoch: 500, acc: 0.910, loss: 0.303, lr: 0.049987528111736124\nepoch: 600, acc: 0.907, loss: 0.276, lr: 0.049985029483669646\nepoch: 700, acc: 0.917, loss: 0.252, lr: 0.049982531105378675\nepoch: 800, acc: 0.920, loss: 0.245, lr: 0.04998003297682575\nepoch: 900, acc: 0.930, loss: 0.228, lr: 0.049977535097973466\nepoch: 1000, acc: 0.940, loss: 0.217, lr: 0.049975037468784345\nepoch: 1100, acc: 0.937, loss: 0.205, lr: 0.049972540089220974\nepoch: 1200, acc: 0.947, loss: 0.192, lr: 0.04997004295924593\nepoch: 1300, acc: 0.947, loss: 0.184, lr: 0.04996754607882181\nepoch: 1400, acc: 0.943, loss: 0.183, lr: 0.049965049447911185\nepoch: 1500, acc: 0.943, loss: 0.189, lr: 0.04996255306647668\nepoch: 1600, acc: 0.943, loss: 0.165, lr: 0.049960056934480884\nepoch: 1700, acc: 0.943, loss: 0.161, lr: 0.04995756105188642\nepoch: 1800, acc: 0.943, loss: 0.158, lr: 0.049955065418655915\nepoch: 1900, acc: 0.943, loss: 0.155, lr: 0.04995257003475201\nepoch: 2000, acc: 0.947, loss: 0.151, lr: 0.04995007490013731\nepoch: 2100, acc: 0.943, loss: 0.148, lr: 0.0499475800147745\nepoch: 2200, acc: 0.953, loss: 0.145, lr: 0.0499450853786262\nepoch: 2300, acc: 0.957, loss: 0.142, lr: 0.0499425909916551\nepoch: 2400, acc: 0.957, loss: 0.138, lr: 0.04994009685382384\nepoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\nepoch: 2600, acc: 0.960, loss: 0.132, lr: 0.049935109325431604\nepoch: 2700, acc: 0.957, loss: 0.130, lr: 0.049932615934796004\nepoch: 2800, acc: 0.937, loss: 0.159, lr: 0.04993012279315098\nepoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\nepoch: 3000, acc: 0.960, loss: 0.123, lr: 0.049925137256683606\nepoch: 3100, acc: 0.960, loss: 0.121, lr: 0.04992264486178666\nepoch: 3200, acc: 0.960, loss: 0.119, lr: 0.04992015271573119\nepoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\nepoch: 3400, acc: 0.960, loss: 0.115, lr: 0.049915169169995596\nepoch: 3500, acc: 0.950, loss: 0.131, lr: 0.049912677770240964\nepoch: 3600, acc: 0.957, loss: 0.118, lr: 0.049910186619178794\nepoch: 3700, acc: 0.960, loss: 0.113, lr: 0.04990769571677183\nepoch: 3800, acc: 0.960, loss: 0.111, lr: 0.04990520506298287\nepoch: 3900, acc: 0.960, loss: 0.110, lr: 0.04990271465777467\nepoch: 4000, acc: 0.960, loss: 0.109, lr: 0.049900224501110035\nepoch: 4100, acc: 0.960, loss: 0.107, lr: 0.04989773459295174\nepoch: 4200, acc: 0.960, loss: 0.106, lr: 0.04989524493326262\nepoch: 4300, acc: 0.960, loss: 0.104, lr: 0.04989275552200545\nepoch: 4400, acc: 0.960, loss: 0.103, lr: 0.04989026635914307\nepoch: 4500, acc: 0.960, loss: 0.102, lr: 0.04988777744463829\nepoch: 4600, acc: 0.960, loss: 0.101, lr: 0.049885288778453954\nepoch: 4700, acc: 0.960, loss: 0.100, lr: 0.049882800360552884\nepoch: 4800, acc: 0.960, loss: 0.099, lr: 0.04988031219089794\nepoch: 4900, acc: 0.960, loss: 0.098, lr: 0.049877824269451976\nepoch: 5000, acc: 0.960, loss: 0.098, lr: 0.04987533659617785\nepoch: 5100, acc: 0.967, loss: 0.107, lr: 0.04987284917103844\nepoch: 5200, acc: 0.960, loss: 0.097, lr: 0.04987036199399661\nepoch: 5300, acc: 0.960, loss: 0.096, lr: 0.04986787506501525\nepoch: 5400, acc: 0.960, loss: 0.095, lr: 0.04986538838405724\nepoch: 5500, acc: 0.960, loss: 0.094, lr: 0.049862901951085496\nepoch: 5600, acc: 0.960, loss: 0.094, lr: 0.049860415766062906\nepoch: 5700, acc: 0.960, loss: 0.093, lr: 0.0498579298289524\nepoch: 5800, acc: 0.960, loss: 0.092, lr: 0.04985544413971689\nepoch: 5900, acc: 0.960, loss: 0.092, lr: 0.049852958698319315\nepoch: 6000, acc: 0.960, loss: 0.091, lr: 0.04985047350472258\nepoch: 6100, acc: 0.960, loss: 0.090, lr: 0.04984798855888967\nepoch: 6200, acc: 0.960, loss: 0.089, lr: 0.049845503860783506\nepoch: 6300, acc: 0.960, loss: 0.089, lr: 0.049843019410367055\nepoch: 6400, acc: 0.960, loss: 0.088, lr: 0.04984053520760327\nepoch: 6500, acc: 0.953, loss: 0.126, lr: 0.049838051252455155\nepoch: 6600, acc: 0.960, loss: 0.090, lr: 0.049835567544885655\nepoch: 6700, acc: 0.960, loss: 0.088, lr: 0.04983308408485778\nepoch: 6800, acc: 0.960, loss: 0.088, lr: 0.0498306008723345\nepoch: 6900, acc: 0.960, loss: 0.087, lr: 0.04982811790727884\nepoch: 7000, acc: 0.960, loss: 0.086, lr: 0.04982563518965381\nepoch: 7100, acc: 0.960, loss: 0.086, lr: 0.049823152719422406\nepoch: 7200, acc: 0.960, loss: 0.085, lr: 0.049820670496547675\nepoch: 7300, acc: 0.960, loss: 0.085, lr: 0.04981818852099264\nepoch: 7400, acc: 0.960, loss: 0.084, lr: 0.049815706792720335\nepoch: 7500, acc: 0.960, loss: 0.083, lr: 0.0498132253116938\nepoch: 7600, acc: 0.960, loss: 0.083, lr: 0.04981074407787611\nepoch: 7700, acc: 0.960, loss: 0.082, lr: 0.049808263091230306\nepoch: 7800, acc: 0.960, loss: 0.082, lr: 0.04980578235171948\nepoch: 7900, acc: 0.963, loss: 0.081, lr: 0.04980330185930667\nepoch: 8000, acc: 0.963, loss: 0.081, lr: 0.04980082161395499\nepoch: 8100, acc: 0.963, loss: 0.080, lr: 0.04979834161562752\nepoch: 8200, acc: 0.963, loss: 0.084, lr: 0.04979586186428736\nepoch: 8300, acc: 0.963, loss: 0.080, lr: 0.04979338235989761\nepoch: 8400, acc: 0.963, loss: 0.079, lr: 0.04979090310242139\nepoch: 8500, acc: 0.963, loss: 0.079, lr: 0.049788424091821805\nepoch: 8600, acc: 0.963, loss: 0.078, lr: 0.049785945328062006\nepoch: 8700, acc: 0.963, loss: 0.078, lr: 0.0497834668111051\nepoch: 8800, acc: 0.963, loss: 0.077, lr: 0.049780988540914256\nepoch: 8900, acc: 0.963, loss: 0.077, lr: 0.0497785105174526\nepoch: 9000, acc: 0.963, loss: 0.076, lr: 0.04977603274068329\nepoch: 9100, acc: 0.963, loss: 0.076, lr: 0.04977355521056952\nepoch: 9200, acc: 0.963, loss: 0.075, lr: 0.049771077927074414\nepoch: 9300, acc: 0.963, loss: 0.075, lr: 0.0497686008901612\nepoch: 9400, acc: 0.707, loss: 1.982, lr: 0.04976612409979302\nepoch: 9500, acc: 0.963, loss: 0.079, lr: 0.0497636475559331\nepoch: 9600, acc: 0.963, loss: 0.076, lr: 0.049761171258544616\nepoch: 9700, acc: 0.963, loss: 0.076, lr: 0.0497586952075908\nepoch: 9800, acc: 0.963, loss: 0.075, lr: 0.04975621940303483\nepoch: 9900, acc: 0.963, loss: 0.074, lr: 0.049753743844839965\nepoch: 10000, acc: 0.967, loss: 0.074, lr: 0.04975126853296942"
  },
  {
    "objectID": "notebooks/WS01_NeuralNetworksWithNumpy.html#예측-시각화하기",
    "href": "notebooks/WS01_NeuralNetworksWithNumpy.html#예측-시각화하기",
    "title": "  섹션 1: 소개 및 첫 번째 뉴런 코딩",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\n\n# Assuming you have access to the weights and biases of your trained model\n# For the weights and biases of dense1 and dense2 layers\nW1, b1 = dense1.weights, dense1.biases\nW2, b2 = dense2.weights, dense2.biases\n\n# Create a meshgrid of points covering the feature space\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Flatten the meshgrid points and apply the first dense layer and ReLU activation\npoints = np.c_[xx.ravel(), yy.ravel()]\nz1 = np.dot(points, W1) + b1\na1 = np.maximum(0, z1)\n\n# Apply the second dense layer\nz2 = np.dot(a1, W2) + b2\n\n# Apply softmax activation to get probabilities\nexp_scores = np.exp(z2 - np.max(z2, axis=1, keepdims=True))\nprobs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\n# Predictions\npredictions = np.argmax(probs, axis=1)\nZ = predictions.reshape(xx.shape)\n\n# Plot decision boundary\nplt.contourf(xx, yy, Z, cmap='brg', alpha=0.8)\n\n# Plot data points\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg')\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()\n\n\n\n\n\n\n\n\n\nNumpy에서 처음부터 완전한 신경망을 학습시켰습니다(휴!). 이것은 꼼꼼한 작업이었지만, PyTorch와 같은 딥러닝 패키지에서 내부적으로 수행되는 작업의 양을 이해하는 데 도움이 될 것입니다. 이것이 신경망 아키텍처 및 학습의 해부학적 구조의 모든 측면에 대한 광범위한 개요는 아니었지만, (희망컨대) 주요 입문 개념 중 일부를 조금 더 잘 이해하게 되었기를 바랍니다. 준비가 되면 WS4_IntroToNNs_PyTorch.ipynb에서 PyTorch로 동일한 아키텍처를 재구현한 것도 확인해 보세요."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html",
    "href": "notebooks/WS07_GNNsForProteins.html",
    "title": "섹션 1: 그래프 이론",
    "section": "",
    "text": "##홈페이지로 돌아가기\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\nGNN에 이상적인 데이터 유형과 단백질 구조가 이러한 유형의 신경망에 적합한 이유를 식별합니다.\n그래프 데이터의 맥락에서 노드와 엣지를 정의합니다. 단백질은 어떻게 그래프로 표현될 수 있습니까?\n단백질 예측을 위한 노드 수준, 엣지 수준 및 그래프 수준 작업의 예를 듭니다.\n방향성 엣지와 무방향성 엣지의 차이를 설명합니다.\nGNN의 어떤 작업에 불변성(invariance) 또는 등변성(equivariance)이 필요한지 분류하고 이유를 설명합니다.\n메시지 전달(message passing)의 단계를 명명하고 설명합니다.\nGNN 내의 다양한 레이어와 다른 모델과의 차이점을 설명합니다.\n단백질에 대한 노드 수준, 엣지 수준 또는 그래프 수준 질문에 답하기 위해 GNN을 적용합니다.\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n목차:\n\n\n\n그래프 이론: 그래프는 어떻게 정의됩니까?\n그래프 이론: 그래프 예측 작업\n그래프 이론: 인접 행렬로의 표현\n그래프 이론: 방향성 및 무방향성 그래프\n그래프 이론: 특징 임베딩으로 노드와 엣지 추가 정의\n\n\n\n\n\nGNN 이론: 메시지 전달\nGNN 이론: 순열 불변성 vs. 순열 등변성\n\n\n\n\n\nGCN 구현: 입력 데이터 로드 및 준비\nGCN 구현: 메시지 전달 함수 정의\nGCN 구현: GraphConv 레이어 정의\nGCN 구현: 모델 정의\nGCN 구현: 학습 및 평가\n\n\n\n\n\n\n\n\n\nCode\n# Install modules\n!pip install graphein\n!pip install pytorch3d\n!pip install torch-geometric\n!pip install torch\n!pip install biovec\n!pip install biographs\n\n\n\nRequirement already satisfied: graphein in /usr/local/lib/python3.10/dist-packages (1.7.7)\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from graphein) (2.2.2)\n\nRequirement already satisfied: biopandas&gt;=0.5.1 in /usr/local/lib/python3.10/dist-packages (from graphein) (0.5.1)\n\nRequirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from graphein) (1.84)\n\nRequirement already satisfied: bioservices&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.11.2)\n\nRequirement already satisfied: cpdb-protein==0.2.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (0.2.0)\n\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from graphein) (3.0.11)\n\nRequirement already satisfied: deepdiff in /usr/local/lib/python3.10/dist-packages (from graphein) (8.0.1)\n\nRequirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (from graphein) (0.7.3)\n\nRequirement already satisfied: looseversion in /usr/local/lib/python3.10/dist-packages (from graphein) (1.1.2)\n\nRequirement already satisfied: matplotlib&gt;=3.4.3 in /usr/local/lib/python3.10/dist-packages (from graphein) (3.8.0)\n\nRequirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from graphein) (1.0.0)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from graphein) (3.4.2)\n\nRequirement already satisfied: numpy&lt;2 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.26.4)\n\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from graphein) (5.24.1)\n\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from graphein) (2.10.3)\n\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from graphein) (13.9.4)\n\nRequirement already satisfied: rich-click in /usr/local/lib/python3.10/dist-packages (from graphein) (1.8.5)\n\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from graphein) (0.13.2)\n\nRequirement already satisfied: pyyaml!=5.4.0,!=5.4.1,!=6.0.0 in /usr/local/lib/python3.10/dist-packages (from graphein) (6.0.2)\n\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from graphein) (1.5.2)\n\nRequirement already satisfied: scipy&gt;=1.8 in /usr/local/lib/python3.10/dist-packages (from graphein) (1.13.1)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from graphein) (4.66.6)\n\nRequirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from graphein) (3.2)\n\nRequirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from graphein) (2024.10.0)\n\nRequirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (from graphein) (0.2.36)\n\nRequirement already satisfied: mmtf-python==1.1.3 in /usr/local/lib/python3.10/dist-packages (from biopandas&gt;=0.5.1-&gt;graphein) (1.1.3)\n\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from biopandas&gt;=0.5.1-&gt;graphein) (75.1.0)\n\nRequirement already satisfied: msgpack&gt;=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mmtf-python==1.1.3-&gt;biopandas&gt;=0.5.1-&gt;graphein) (1.1.0)\n\nRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.4.4)\n\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (4.12.3)\n\nRequirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (6.9.0)\n\nRequirement already satisfied: easydev&gt;=0.12.0 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.13.2)\n\nRequirement already satisfied: grequests in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (2.32.3)\n\nRequirement already satisfied: requests-cache in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.2.1)\n\nRequirement already satisfied: suds-community&gt;=0.7 in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.2.0)\n\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (5.3.0)\n\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (1.17.0)\n\nRequirement already satisfied: xmltodict in /usr/local/lib/python3.10/dist-packages (from bioservices&gt;=1.10.0-&gt;graphein) (0.14.2)\n\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (1.3.1)\n\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (0.12.1)\n\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (4.55.1)\n\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (1.4.7)\n\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (24.2)\n\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (11.0.0)\n\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (3.2.0)\n\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib&gt;=3.4.3-&gt;graphein) (2.8.2)\n\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;graphein) (2024.2)\n\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;graphein) (2024.2)\n\nRequirement already satisfied: orderly-set==5.2.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff-&gt;graphein) (5.2.2)\n\nRequirement already satisfied: tenacity&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly-&gt;graphein) (9.0.0)\n\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (2.27.1)\n\nRequirement already satisfied: typing-extensions&gt;=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic-&gt;graphein) (4.12.2)\n\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;graphein) (3.0.0)\n\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;graphein) (2.18.0)\n\nRequirement already satisfied: click&gt;=7 in /usr/local/lib/python3.10/dist-packages (from rich-click-&gt;graphein) (8.1.7)\n\nRequirement already satisfied: joblib&gt;=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;graphein) (1.4.2)\n\nRequirement already satisfied: threadpoolctl&gt;=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-&gt;graphein) (3.5.0)\n\nRequirement already satisfied: colorama&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (0.4.6)\n\nRequirement already satisfied: line-profiler&lt;5.0.0,&gt;=4.1.2 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.2.0)\n\nRequirement already satisfied: pexpect&lt;5.0.0,&gt;=4.9.0 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.9.0)\n\nRequirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.2.0 in /usr/local/lib/python3.10/dist-packages (from easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (4.3.6)\n\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;graphein) (0.1.2)\n\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib&gt;=3.4.3-&gt;graphein) (1.16.0)\n\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2.6)\n\nRequirement already satisfied: gevent in /usr/local/lib/python3.10/dist-packages (from grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.11.1)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.4.0)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (2024.8.30)\n\nRequirement already satisfied: attrs&gt;=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.2.0)\n\nRequirement already satisfied: cattrs&gt;=22.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (24.1.2)\n\nRequirement already satisfied: url-normalize&gt;=1.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (1.4.3)\n\nRequirement already satisfied: exceptiongroup&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs&gt;=22.2-&gt;requests-cache-&gt;bioservices&gt;=1.10.0-&gt;graphein) (1.2.2)\n\nRequirement already satisfied: ptyprocess&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect&lt;5.0.0,&gt;=4.9.0-&gt;easydev&gt;=0.12.0-&gt;bioservices&gt;=1.10.0-&gt;graphein) (0.7.0)\n\nRequirement already satisfied: zope.event in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (5.0)\n\nRequirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (7.2)\n\nRequirement already satisfied: greenlet&gt;=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gevent-&gt;grequests-&gt;bioservices&gt;=1.10.0-&gt;graphein) (3.1.1)\n\nERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\n\nERROR: No matching distribution found for pytorch3d\n\nRequirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n\nRequirement already satisfied: psutil&gt;=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (2.4.4)\n\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.3.1)\n\nRequirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (4.0.3)\n\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (24.2.0)\n\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.5.0)\n\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (6.1.0)\n\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (0.2.1)\n\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;torch-geometric) (1.18.3)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch-geometric) (3.0.2)\n\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (3.4.0)\n\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (3.10)\n\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (2.2.3)\n\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;torch-geometric) (2024.8.30)\n\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict&lt;7.0,&gt;=4.5-&gt;aiohttp-&gt;torch-geometric) (4.12.2)\n\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n\nRequirement already satisfied: typing-extensions&gt;=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-&gt;torch) (1.3.0)\n\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch) (3.0.2)\n\nRequirement already satisfied: biovec in /usr/local/lib/python3.10/dist-packages (0.2.7)\n\nRequirement already satisfied: gensim==3.4.0 in /usr/local/lib/python3.10/dist-packages (from biovec) (3.4.0)\n\nRequirement already satisfied: tqdm&gt;=2.0.0 in /usr/local/lib/python3.10/dist-packages (from biovec) (4.66.6)\n\nRequirement already satisfied: pyfasta==0.5.2 in /usr/local/lib/python3.10/dist-packages (from biovec) (0.5.2)\n\nRequirement already satisfied: numpy&gt;=1.11.3 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.26.4)\n\nRequirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.13.1)\n\nRequirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (1.16.0)\n\nRequirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.10/dist-packages (from gensim==3.4.0-&gt;biovec) (7.0.5)\n\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open&gt;=1.2.1-&gt;gensim==3.4.0-&gt;biovec) (1.17.0)\n\nRequirement already satisfied: biographs in /usr/local/lib/python3.10/dist-packages (0.1)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from biographs) (3.4.2)\n\nRequirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (from biographs) (1.84)\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython-&gt;biographs) (1.26.4)\n\n\n\n\n\n\nCode\n# Get libraries\nimport os\nimport re\nimport graphein\nimport graphein.protein as gp\nfrom graphein.protein.config import ProteinGraphConfig\nfrom graphein.protein.graphs import construct_graph\nfrom graphein.protein.visualisation import plotly_protein_structure_graph\n\nimport pandas as pd\n# from Bio import PDB\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport networkx as nx\nfrom networkx import to_networkx_graph\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.utils import from_networkx\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool\nimport biographs as bg\nimport Bio.PDB\nfrom biopandas.pdb import PandasPdb\n# from plotly.subplots import make_subplots\n\n\n\n\nCode\n# Clone the repository with sparse checkout and depth limit\n!git clone -n --depth=1 --filter=tree:0 \\\n  https://github.com/courtel/GNN_dl_workshop.git repo_partial\n\n\n%cd repo_partial # Get into cloned repo\n!git sparse-checkout set --no-cone images # Sparse checkout to fetch only specified folder\n!git checkout\n\n%cd ..\n!mv repo_partial/images ./images\n!rm -rf repo_partial\n\n\nCloning into 'repo_partial'...\nremote: Enumerating objects: 1, done.\nremote: Counting objects: 100% (1/1), done.\nremote: Total 1 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\nReceiving objects: 100% (1/1), done.\n[Errno 2] No such file or directory: 'repo_partial # Get into cloned repo'\n/\nfatal: not a git repository (or any of the parent directories): .git\nfatal: not a git repository (or any of the parent directories): .git\n/\nmv: cannot stat 'repo_partial/images': No such file or directory\n\n\n\n\n\n\n그래프는 우리 주변 세계에 매우 만연해 있습니다. 그래프는 객체 간의 복잡한 관계를 포착할 수 있기 때문에 데이터를 표현하는 강력한 방법입니다. 소셜 네트워크, 교통 네트워크, 통신 네트워크, 지식 그래프(검색 엔진 등), 심지어 문장 속의 단어까지도 그래프로 표현할 수 있습니다. 그래프는 로컬(노드 수준) 및 글로벌(그래프 수준) 정보를 모두 표현할 수 있어 모델링 대상의 전체 복잡성을 포착합니다.\n대부분의 신경망은 유클리드 데이터에서 작동합니다. 예를 들어, 이미지는 노드와 엣지의 격자 모양의 정렬된 구조에 따라 기능이 달라지는 CNN에서 이상적으로 처리되는 선형 그래프입니다. GNN은 소셜 네트워크와 같이 더 복잡하고 예측하기 어려운 비선형 데이터에 사용할 수 있습니다.\n\n\nCode\n# @title Hover over the nodes, edges, and graph border to see embeddings for each.\n\n\nfrom IPython.display import HTML, display\n\n# Load D3.js\ndisplay(HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v6.min.js\"&gt;&lt;/script&gt;\n\"\"\"))\n\nhtml_code = \"\"\"\n&lt;div id=\"container\"&gt;\n  &lt;div&gt;\n    &lt;/div&gt;\n    &lt;div style=\"display: flex; align-items: flex-start;\"&gt;\n      &lt;div&gt;\n        &lt;h2&gt;Grid-like Image Graph&lt;/h2&gt;\n        &lt;div id=\"grid-image\"&gt;&lt;/div&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-left: 20px;\"&gt;\n        &lt;h2&gt;Social Network Graph&lt;/h2&gt;\n        &lt;div id=\"social-network-graph\"&gt;&lt;/div&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;\n&lt;/div&gt;\n\n&lt;style&gt;\n  h2 {\n    text-align: center;\n  }\n\n  .node {\n    stroke: #bbb;\n    stroke-width: 1px;\n  }\n\n  .node.hovered {\n    stroke-width: 3px;\n    stroke: #1f77b4;\n  }\n\n  .edge {\n    stroke: #bbb;\n    stroke-width: 1px;\n  }\n\n  .edge.hovered {\n    stroke-width: 3px;\n    stroke: #ff7f0e;\n  }\n\n  .overlay {\n    fill: rgba(0, 0, 0, 0);\n    cursor: pointer;\n  }\n\n  .graph {\n    stroke: #bbb;\n    stroke-width: 4px;\n    fill: none;\n  }\n\n  .graph.hovered {\n    stroke: #2ca02c;\n    stroke-width: 10px;\n  }\n\n  .tooltip {\n    position: absolute;\n    background-color: #fff;\n    border: 1px solid #bbb;\n    padding: 5px;\n    border-radius: 3px;\n    pointer-events: none;\n    font-size: 12px;\n  }\n&lt;/style&gt;\n\n&lt;div class=\"tooltip\" id=\"tooltip\" style=\"display: none;\"&gt;&lt;/div&gt;\n\n&lt;script&gt;\n  function makeGridImage() {\n    const gridSize = 5;  // Change this to adjust the grid size (e.g., 5x5 grid)\n    const nodes = [];\n    const links = [];\n\n    for (let i = 0; i &lt; gridSize; i++) {\n      for (let j = 0; j &lt; gridSize; j++) {\n        const id = i * gridSize + j;\n        nodes.push({\n          id: id,\n          x: j / (gridSize - 1),\n          y: i / (gridSize - 1),\n          element: `P(${i},${j})`,\n          features: `Pixel (${i},${j})`\n        });\n\n        // Connect the node to its right neighbor\n        if (j &lt; gridSize - 1) {\n          links.push({ source: id, target: id + 1 });\n        }\n\n        // Connect the node to its bottom neighbor\n        if (i &lt; gridSize - 1) {\n          links.push({ source: id, target: id + gridSize });\n        }\n      }\n    }\n\n    const graph = { features: 'Grid Image Representation: ...' };\n\n    return [nodes, links, graph];\n  }\n\n  function makeSocialNetworkGraph() {\n    const numNodes = 50;\n    const nodes = Array.from({ length: numNodes }, (v, i) =&gt; ({\n      id: i,\n      x: Math.random(),\n      y: Math.random(),\n      name: `User ${i}`,\n      features: `Embeddings: ${Math.random().toFixed(4)}, ${Math.random().toFixed(4)}`\n    }));\n\n    const links = [];\n    for (let i = 0; i &lt; numNodes; i++) {\n      for (let j = i + 1; j &lt; numNodes; j++) {\n        if (Math.random() &lt; 0.1) {\n          links.push({\n            source: i,\n            target: j,\n            connection: 'friend',\n            features: `Connection strength: ${Math.random().toFixed(4)}`\n          });\n        }\n      }\n    }\n\n    const graph = { features: 'Social Network Graph Embeddings: ...' };\n\n    return [nodes, links, graph];\n  }\n\n  class GraphDescription {\n    constructor(elementId, makeGraphFunction) {\n      this.parent = d3.select(`#${elementId}`);\n      this.svg = this.parent.append('svg')\n                            .attr('width', 400)\n                            .attr('height', 400);\n      const [nodes, links, graph] = makeGraphFunction();\n      this.showGraph(nodes, links, graph);\n    }\n\n    showGraph(nodes, links, graph) {\n      const pos = (x) =&gt; x * 350 + 25;\n      const tooltip = d3.select('#tooltip');\n      const self = this;\n      const graphHolder = this.svg.append('g')\n                                  .attr('transform', 'translate(10, 10)');\n\n      const graphBorder = graphHolder.append('rect')\n        .attr('width', 350)\n        .attr('height', 350)\n        .attr('x', 0)\n        .attr('y', 0)\n        .attr('rx', 20)\n        .attr('class', 'graph')\n        .on('mouseover', function(event) { self.showGraphTooltip(event, graph, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      const edgeElements = graphHolder.selectAll('line.edge')\n        .data(links)\n        .enter()\n        .append('line')\n        .classed('edge', true)\n        .attr(\"x1\", (d) =&gt; pos(nodes[d.source].x))\n        .attr(\"x2\", (d) =&gt; pos(nodes[d.target].x))\n        .attr(\"y1\", (d) =&gt; pos(nodes[d.source].y))\n        .attr(\"y2\", (d) =&gt; pos(nodes[d.target].y))\n        .on('mouseover', function(event, d) { self.showEdgeTooltip(event, d, nodes, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      graphHolder.selectAll('line.overlay')\n        .data(links)\n        .enter()\n        .append('line')\n        .classed('overlay', true)\n        .attr(\"x1\", (d) =&gt; pos(nodes[d.source].x))\n        .attr(\"x2\", (d) =&gt; pos(nodes[d.target].x))\n        .attr(\"y1\", (d) =&gt; pos(nodes[d.source].y))\n        .attr(\"y2\", (d) =&gt; pos(nodes[d.target].y))\n        .on('mouseover', function(event, d) { self.showEdgeTooltip(event, d, nodes, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      const nodeElements = graphHolder.selectAll('circle.node')\n        .data(nodes)\n        .enter()\n        .append('circle')\n        .classed('node', true)\n        .attr('r', 7)\n        .attr('cx', (d) =&gt; pos(d.x))\n        .attr('cy', (d) =&gt; pos(d.y))\n        .on('mouseover', function(event, d) { self.showNodeTooltip(event, d, tooltip); })\n        .on('mouseout', function() { self.hideTooltip(tooltip); });\n\n      graphHolder.selectAll('circle.overlay')\n        .data(nodes)\n        .enter()\n        .append('circle')\n        .classed('overlay', true)\n        .attr('r', 12)\n        .attr('cx', (d) =&gt; pos(d.x))\n        .attr('cy', (d) =&gt; pos(d.y))\n        .on('mouseover', function(event, d) {\n          d3.select(nodeElements.nodes()[d.id]).classed('hovered', true);\n          d3.select(this).classed('hovered', true);\n          self.showNodeTooltip(event, d, tooltip);\n        })\n        .on('mouseout', function(event, d) {\n          d3.select(nodeElements.nodes()[d.id]).classed('hovered', false);\n          d3.select(this).classed('hovered', false);\n          self.hideTooltip(tooltip);\n        });\n\n      graphBorder.on('mouseover', function(event) {\n        d3.select(this).classed('hovered', true);\n        edgeElements.classed('hovered', true);\n        nodeElements.classed('hovered', true);\n        self.showGraphTooltip(event, graph, tooltip);\n      })\n      .on('mouseout', function() {\n        d3.select(this).classed('hovered', false);\n        edgeElements.classed('hovered', false);\n        nodeElements.classed('hovered', false);\n        self.hideTooltip(tooltip);\n      });\n    }\n\n    showNodeTooltip(event, d, tooltip) {\n      tooltip.style('display', 'block')\n             .style('left', (event.pageX + 5) + 'px')\n             .style('top', (event.pageY + 5) + 'px')\n             .html(`&lt;strong&gt;ID:&lt;/strong&gt; ${d.id}&lt;br&gt;&lt;strong&gt;Features:&lt;/strong&gt; ${d.features}`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    showEdgeTooltip(event, d, nodes, tooltip) {\n      const sourceNode = nodes[d.source];\n      const targetNode = nodes[d.target];\n      tooltip.style('display', 'block')\n            .style('left', (event.pageX + 5) + 'px')\n            .style('top', (event.pageY + 5) + 'px')\n            .html(`&lt;strong&gt;Source:&lt;/strong&gt; ${sourceNode.id} (${sourceNode.features})&lt;br&gt;&lt;strong&gt;Target:&lt;/strong&gt; ${targetNode.id} (${targetNode.features})`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    showGraphTooltip(event, graph, tooltip) {\n      tooltip.style('display', 'block')\n            .style('left', (event.pageX + 5) + 'px')\n            .style('top', (event.pageY + 5) + 'px')\n            .html(`&lt;strong&gt;Graph:&lt;/strong&gt; ${graph.features}`);\n      d3.select(event.target).classed('hovered', true);\n    }\n\n    hideTooltip(tooltip) {\n      tooltip.style('display', 'none');\n      d3.selectAll('.hovered').classed('hovered', false);\n    }\n  }\n\n  const gridImageGraph = new GraphDescription('grid-image', makeGridImage);\n  const socialNetworkGraph = new GraphDescription('social-network-graph', makeSocialNetworkGraph);\n&lt;/script&gt;\n\"\"\"\n\ndisplay(HTML(html_code))"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#그래프-이론-그래프는-어떻게-정의됩니까",
    "href": "notebooks/WS07_GNNsForProteins.html#그래프-이론-그래프는-어떻게-정의됩니까",
    "title": "섹션 1: 그래프 이론",
    "section": "그래프 이론: 그래프는 어떻게 정의됩니까?",
    "text": "그래프 이론: 그래프는 어떻게 정의됩니까?\n\n\nCode\n# @title A graph **G** is a data structure that can be defined as **G = (V, E)**, where **V** represents the vertices (or nodes) and **E** represents the edges. Edges in a graph can be directed or undirected based on whether directional dependencies exist between nodes.\nfrom IPython.display import HTML, display\n\n# Load D3.js\ndisplay(HTML(\"\"\"\n&lt;script src=\"https://d3js.org/d3.v6.min.js\"&gt;&lt;/script&gt;\n\"\"\"))\n\nhtml_code = \"\"\"\n&lt;div id=\"container\" style=\"display: flex; align-items: flex-start;\"&gt;\n  &lt;div id=\"graph-description\"&gt;&lt;/div&gt;\n  &lt;div id=\"text-description\"&gt;&lt;/div&gt;\n&lt;/div&gt;\n\n&lt;style&gt;\n  .line-holder {\n    display: flex;\n    flex-direction: column;\n    margin: 10px 0;\n    width: 100%;\n  }\n  .line-holder div {\n    margin: 5px 0;\n  }\n  .letter {\n    font-weight: bold;\n    margin-right: 10px;\n    font-size: 2em;\n  }\n  .desc {\n    font-size: 2em;\n    margin-right: 10px;\n    margin-left: 5px;\n    white-space: pre-wrap;\n  }\n  .line-holder .top-line {\n    display: flex;\n    align-items: center;\n  }\n  .eg {\n    font-size: 1.5em;\n    margin-left: 20px;\n  }\n  .selected {\n    background-color: #ddd;\n  }\n  svg {\n    margin: 20px;\n    border: 1px solid black;\n  }\n&lt;/style&gt;\n\n&lt;script&gt;\nfunction makeGraph() {\n  const nodes = [\n    { id: 0, x: 0.8, y: 0.5 },\n    { id: 1, x: 0.6, y: 0.9 },\n    { id: 2, x: 0.2, y: 0.8 },\n    { id: 3, x: 0.2, y: 0.4 },\n    { id: 4, x: 0.6, y: 0.1 }\n  ];\n  const links = [\n    { a: nodes[0], b: nodes[0] },\n    { a: nodes[0], b: nodes[4] },\n    { a: nodes[4], b: nodes[0] },\n    { a: nodes[4], b: nodes[1] },\n    { a: nodes[1], b: nodes[2] },\n    { a: nodes[2], b: nodes[1] },\n    { a: nodes[2], b: nodes[2] },\n    { a: nodes[2], b: nodes[4] },\n    { a: nodes[4], b: nodes[2] },\n    { a: nodes[3], b: nodes[3] },\n    { a: nodes[3], b: nodes[0] },\n    { a: nodes[0], b: nodes[3] },\n    { a: nodes[4], b: nodes[4] },\n    { a: nodes[4], b: nodes[1] },\n    { a: nodes[1], b: nodes[4] },\n    { a: nodes[4], b: nodes[3] },\n    { a: nodes[3], b: nodes[4] }\n  ];\n  return [nodes, links];\n}\n\nclass GraphDescription {\n  constructor() {\n    this.parent = d3.select('#graph-description');\n    this.svg = this.parent.append('svg')\n                          .attr('width', 400)\n                          .attr('height', 400);\n    const [nodes, links] = makeGraph();\n    this.showGraph(nodes, links);\n    this.showText();\n  }\n\n  showGraph(nodes, links) {\n    const pos = (x) =&gt; x * 300 + 50;\n\n    const graphHolder = this.svg.append('g')\n                                .attr('transform', 'translate(25, 25)');\n\n    // Make global box\n    graphHolder.append('rect')\n      .attr('width', 350)\n      .attr('height', 350)\n      .attr('x', 0)\n      .attr('y', 0)\n      .attr('rx', 20)\n      .attr('fill', '#fff')\n      .attr('stroke', '#ddd')\n      .style(\"stroke-width\", 2)\n      .attr('stroke-dasharray', \"4, 4\")\n      .on('mouseover', () =&gt; this.highlightGlobal())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n\n    // Make edges\n    graphHolder.selectAll('line.vis')\n      .data(links)\n      .enter()\n      .append('line')\n      .classed('vis', true)\n      .style(\"stroke\", \"#bbb\")\n      .style(\"stroke-width\", 1)\n      .attr(\"x1\", (d) =&gt; pos(d.a.x))\n      .attr(\"x2\", (d) =&gt; pos(d.b.x))\n      .attr(\"y1\", (d) =&gt; pos(d.a.y))\n      .attr(\"y2\", (d) =&gt; pos(d.b.y));\n\n    graphHolder.selectAll('line.target')\n      .data(links)\n      .enter()\n      .append('line')\n      .classed('target', true)\n      .style(\"stroke\", \"rgba(0, 0, 0, 0)\")\n      .style(\"stroke-width\", 40)\n      .attr(\"x1\", (d) =&gt; pos(d.a.x))\n      .attr(\"x2\", (d) =&gt; pos(d.b.x))\n      .attr(\"y1\", (d) =&gt; pos(d.a.y))\n      .attr(\"y2\", (d) =&gt; pos(d.b.y))\n      .on('mouseover', () =&gt; this.highlightEdges())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n\n    // Make nodes\n    graphHolder.selectAll('circle')\n      .data(nodes)\n      .enter()\n      .append('circle')\n      .attr('r', 10)\n      .attr('cx', (d) =&gt; pos(d.x))\n      .attr('cy', (d) =&gt; pos(d.y))\n      .style('fill', '#fff')\n      .style(\"stroke-width\", '1px')\n      .style(\"stroke\", '#bbb')\n      .on('mouseover', () =&gt; this.highlightNodes())\n      .on('mouseout', () =&gt; this.unhighlightAll());\n  }\n\n  showText() {\n    const textHolder = d3.select('#text-description');\n\n    const makeLine = (letter, desc, eg, mouseover) =&gt; {\n      const div = textHolder.append('div')\n        .classed('line-holder', true)\n        .attr('id', letter);\n      const topLine = div.append('div').classed('top-line', true);\n      topLine.append('div').text(letter).classed('letter', true);\n      topLine.append('div').text(desc).classed('desc', true);\n      div.append('div').text(eg).classed('eg', true);\n\n      div.on('mouseover', mouseover);\n      div.on('mouseout', () =&gt; this.unhighlightAll());\n    }\n\n    makeLine('V', ' Vertex (or node) attributes', 'e.g., node identity, number of neighbors', () =&gt; this.highlightNodes());\n    makeLine('E', ' Edge attributes and directions', 'e.g., edge identity, edge weight', () =&gt; this.highlightEdges());\n    makeLine('G', ' Graph attributes', 'e.g., number of nodes, longest path', () =&gt; this.highlightGlobal());\n  }\n\n  highlightEdges() {\n    d3.select('#E').classed('selected', true);\n    this.parent.selectAll('line.vis')\n      .style(\"stroke\", \"#000\")\n      .style(\"stroke-width\", 10);\n  }\n\n  highlightNodes() {\n    d3.select('#V').classed('selected', true);\n    this.parent.selectAll('circle')\n      .style(\"stroke-width\", 6)\n      .style(\"stroke\", '#000')\n      .attr(\"r\", 11);\n  }\n\n  highlightGlobal() {\n    d3.select('#G').classed('selected', true);\n    this.parent.selectAll('rect')\n      .style(\"stroke\", '#000')\n      .style(\"stroke-width\", 40);\n  }\n\n  unhighlightAll() {\n    d3.selectAll('.line-holder').classed('selected', false);\n    this.parent.selectAll('line.vis')\n      .style(\"stroke\", \"#bbb\")\n      .style(\"stroke-width\", '1px');\n\n    this.parent.selectAll('circle')\n      .style(\"stroke-width\", '1px')\n      .style(\"stroke\", '#aaa')\n      .attr(\"r\", 10);\n\n    this.parent.selectAll('rect')\n      .style(\"stroke-width\", 2)\n      .style(\"stroke\", '#ddd');\n  }\n}\n\nconst graph = new GraphDescription();\n&lt;/script&gt;\n\"\"\"\n\ndisplay(HTML(html_code))\n\n# adapted from https://distill.pub/2021/gnn-intro/\n\n\n\n\n\n\n\n\n  \n  \n\n\n\n\n\n\n\nAdapted from: A Gentle Introduction to Graph Neural Networks"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#그래프-이론-그래프-예측-작업",
    "href": "notebooks/WS07_GNNsForProteins.html#그래프-이론-그래프-예측-작업",
    "title": "섹션 1: 그래프 이론",
    "section": "그래프 이론: 그래프 예측 작업",
    "text": "그래프 이론: 그래프 예측 작업\n그래프에서 어떤 종류의 문제를 해결할 수 있습니까?\n\n노드 수준 작업은 그래프의 노드를 분류하는 것을 포함합니다.\n엣지 수준 작업(또는 링크 예측)은 그래프의 노드 쌍 간의 관계를 포함합니다.\n그래프 수준 작업은 전체 그래프를 아우르는 속성을 포함합니다.\n\n\n\nCode\n# @markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\nimport json  # Required for converting Python dictionaries to JSON strings\n\n# Words and definitions to match\nwords = [\"Node-level task\", \"Edge-level task\", \"Graph-level task\"]\ndefinitions = [\n    \"Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\",\n    \"Predict the overall stability of the protein.\",\n    \"Predict the B-factor for each residue.\",\n]\n\ncorrect_definitions = {\n    \"Node-level task\": \"Predict the B-factor for each residue.\",\n    \"Edge-level task\": \"Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\",\n    \"Graph-level task\": \"Predict the overall stability of the protein.\",\n}\n\nexplanation = \"\"\"\n&lt;p&gt;&lt;/p&gt;\n&lt;ol&gt;\n    &lt;li&gt;&lt;strong&gt;Node-level task:&lt;/strong&gt; Predict the B-factor for each residue.&lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Edge-level task:&lt;/strong&gt; Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).&lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Graph-level task:&lt;/strong&gt; Predict the overall stability of the protein.&lt;/li&gt;\n&lt;/ol&gt;\n\"\"\"\n\n# Create the matching quiz with dropdowns\ndef create_matching_quiz(words, definitions, explanation):\n    html_code = \"&lt;h3&gt;Match the problem to solve with the correct task level:&lt;/h3&gt;\"\n\n    for i, word in enumerate(words):\n        html_code += f\"&lt;p&gt;&lt;strong&gt;{word}:&lt;/strong&gt; \" \\\n                     f\"&lt;select id='dropdown_{i}'&gt;\" \\\n                     f\"&lt;option value='' disabled selected&gt;Select task&lt;/option&gt;\"\n        for definition in definitions:\n            html_code += f\"&lt;option value='{definition}'&gt;{definition}&lt;/option&gt;\"\n        html_code += \"&lt;/select&gt;&lt;/p&gt;\"\n\n    # Add submit button, feedback, and explanation\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswers()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        {explanation}\n      &lt;/div&gt;\n    &lt;script&gt;\n    // JavaScript function to check answers\n    function checkAnswers() {{\n        // Pass the Python data into JavaScript variables\n        var correct_answers = {json.dumps(correct_definitions)};\n        var feedback = document.getElementById('feedback');\n        var correct = true;\n\n        // Loop through each dropdown and check if the selected answer is correct\n        Object.keys(correct_answers).forEach(function(word, index) {{\n            var dropdown = document.getElementById('dropdown_' + index);\n            var selected_answer = dropdown.value;\n\n            if (selected_answer === correct_answers[word]) {{\n                dropdown.style.color = \"green\";\n            }} else {{\n                dropdown.style.color = \"red\";\n                correct = false;\n            }}\n        }});\n\n        // Update feedback based on correctness\n        if (correct) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect. Try again.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n\n    // JavaScript function to show explanation\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Call the function to create the matching quiz\ncreate_matching_quiz(words, definitions, explanation)\n\n\nMatch the problem to solve with the correct task level:Node-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.Edge-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.Graph-level task: Select taskPredict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).Predict the overall stability of the protein.Predict the B-factor for each residue.\n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \n\n\n    Node-level task: Predict the B-factor for each residue.\n    Edge-level task: Predict different types of residue interactions (e.g., hydrogen bond, ionic bond, hydrophobic interaction).\n    Graph-level task: Predict the overall stability of the protein."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#그래프-이론-인접-행렬로의-표현",
    "href": "notebooks/WS07_GNNsForProteins.html#그래프-이론-인접-행렬로의-표현",
    "title": "섹션 1: 그래프 이론",
    "section": "그래프 이론: 인접 행렬로의 표현",
    "text": "그래프 이론: 인접 행렬로의 표현\n신경망을 위해 그래프를 계산적으로 표현하는 가장 일반적인 방법 중 하나는 인접 행렬(adjacency matrix)을 사용하는 것입니다. 이는 크기 n x n의 이진 행렬 \\((A)\\)로, 노드 i와 j 사이에 엣지가 있으면 각 요소 A[i][j]는 1이고 그렇지 않으면 0입니다. 이 튜토리얼에서는 인접 행렬을 사용할 것입니다.\n\n\n\n\n\n\n\\[\\begin{array}{l|llll}\n    & n_0 & n_1 & n_2 & n_3 \\\\ \\hline\nn_0 & 0    & 1    & 0    & 1    \\\\\nn_1 & 0    & 0    & 1    & 0    \\\\\nn_2 & 1    & 0    & 0    & 0    \\\\\nn_3 & 1    & 0    & 0    & 0\n\\end{array}\\]\n\n\n\n\n\nCode\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create a directed graph\nG = nx.DiGraph()\nG.add_edges_from([\n    (0, 1),\n    (0, 2),\n    (1, 2),\n    (1, 3),\n    (2, 3),\n    (3, 0),\n    (3, 1)\n])\n\n# Draw the graph\npos = nx.spring_layout(G)\nnx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=700, arrows=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nimport random\nfrom IPython.display import display, HTML\n\ndef create_quiz(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Shuffle the answers\n    # random.shuffle(answers)\n\n    # Generate the HTML code\n    html_code = f\"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{question}&lt;/h3&gt;\n    \"\"\"\n\n    # Add each answer as a radio button\n    for i, answer in enumerate(answers):\n        html_code += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section\n    html_code += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Import necessary libraries\nfrom IPython.display import display, HTML\n\n# Base64 encoded image string (your full string goes here)\nimage_base64 = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA5oAAACECAYAAADxwVT9AAAMP2lDQ1BJQ0MgUHJvZmlsZQAASImVVwdYU8kWnluSkJDQQpcSehNEpASQEkILIL0INkISIJQYA0HFji4quHYRARu6KqLYAbEjioVFsffFgoKyLhbsypsU0HVf+d75vrn3v/+c+c+Zc+eWAUD9JFcszkE1AMgV5UtiQwIYY5NTGKRuQAAGAAFUYM3l5YlZ0dERANrg+e/27gb0hHbVUab1z/7/app8QR4PACQa4jR+Hi8X4oMA4FU8sSQfAKKMt5iaL5Zh2IC2BCYI8SIZzlDgKhlOU+C9cp/4WDbELQCoULlcSQYAapchzyjgZUANtT6InUV8oQgAdQbEvrm5k/kQp0JsC33EEMv0mWk/6GT8TTNtSJPLzRjCirnITSVQmCfO4U7/P8vxvy03RzoYwxo2aqYkNFY2Z1i3W9mTw2WYCnGvKC0yCmItiD8I+XJ/iFFKpjQ0QeGPGvHy2LBmQBdiZz43MBxiI4iDRTmREUo+LV0YzIEYrhB0mjCfEw+xPsSLBHlBcUqfTZLJscpYaH26hM1S8ue4EnlcWawH0uwEllL/daaAo9TH1Aoz45MgpkBsWSBMjIRYDWKnvOy4cKXP6MJMduSgj0QaK8vfEuJYgSgkQKGPFaRLgmOV/iW5eYPzxTZlCjmRSrw/PzM+VFEfrIXHlecP54JdFohYCYM6gryxEYNz4QsCgxRzx7oFooQ4pc4HcX5ArGIsThHnRCv9cXNBToiMN4fYNa8gTjkWT8yHC1Khj6eL86PjFXnihVncsGhFPvhyEAHYIBAwgBS2NDAZZAFhe29DL7xS9AQDLpCADCAAjkpmcESSvEcEj3GgEPwJkQDkDY0LkPcKQAHkvw6xiqMjSJf3FshHZIOnEOeCcJADr6XyUaKhaIngCWSE/4jOhY0H882BTdb/7/lB9jvDgkyEkpEORmSoD3oSg4iBxFBiMNEON8R9cW88Ah79YXPBmbjn4Dy++xOeEjoIjwjXCZ2E25OERZKfshwDOqF+sLIWaT/WAreGmm54AO4D1aEyrosbAkfcFcZh4X4wshtk2cq8ZVVh/KT9txn8cDeUfmRnMkrWI/uTbX8eqWav5jakIqv1j/VR5Jo2VG/2UM/P8dk/VJ8Pz+E/e2KLsANYK3YKO48dxRoAAzuBNWJt2DEZHlpdT+SrazBarDyfbKgj/Ee8wTsrq2Sec61zj/MXRV++YJrsHQ3Yk8XTJcKMzHwGC34RBAyOiOc0nOHi7OIKgOz7onh9vYmRfzcQ3bbv3Pw/APA5MTAwcOQ7F3YCgH0e8PE//J2zZcJPhyoA5w7zpJICBYfLDgT4llCHT5oBMAEWwBbOxwW4A2/gD4JAGIgC8SAZTITZZ8J1LgFTwUwwDxSDUrAcrAEVYCPYAnaA3WA/aABHwSlwFlwEl8F1cBeuni7wAvSBd+AzgiAkhIbQEQPEFLFCHBAXhIn4IkFIBBKLJCOpSAYiQqTITGQ+UoqsRCqQzUgNsg85jJxCziMdyG3kIdKDvEY+oRhKRbVRY9QaHYEyURYajsajE9AMdApaiC5Al6LlaDW6C61HT6EX0etoJ/oC7ccAporpYmaYI8bE2FgUloKlYxJsNlaClWHVWB3WBO/zVawT68U+4kScjjNwR7iCQ/EEnIdPwWfjS/AKfAdej7fgV/GHeB/+jUAjGBEcCF4EDmEsIYMwlVBMKCNsIxwinIHPUhfhHZFI1CXaED3gs5hMzCLOIC4hrifuIZ4kdhAfE/tJJJIByYHkQ4oicUn5pGLSOtIu0gnSFVIX6YOKqoqpiotKsEqKikilSKVMZafKcZUrKs9UPpM1yFZkL3IUmU+eTl5G3kpuIl8id5E/UzQpNhQfSjwlizKPUk6po5yh3KO8UVVVNVf1VI1RFarOVS1X3at6TvWh6keqFtWeyqaOp0qpS6nbqSept6lvaDSaNc2flkLLpy2l1dBO0x7QPqjR1ZzUOGp8tTlqlWr1alfUXqqT1a3UWeoT1QvVy9QPqF9S79Uga1hrsDW4GrM1KjUOa9zU6Neka47UjNLM1VyiuVPzvGa3FknLWitIi6+1QGuL1mmtx3SMbkFn03n0+fSt9DP0Lm2ito02RztLu1R7t3a7dp+Olo6rTqLONJ1KnWM6nbqYrrUuRzdHd5nuft0bup/0jPVYegK9xXp1elf03usP0/fXF+iX6O/Rv67/yYBhEGSQbbDCoMHgviFuaG8YYzjVcIPhGcPeYdrDvIfxhpUM2z/sjhFqZG8UazTDaItRm1G/sYlxiLHYeJ3xaeNeE10Tf5Msk9Umx016TOmmvqZC09WmJ0yfM3QYLEYOo5zRwugzMzILNZOabTZrN/tsbmOeYF5kvsf8vgXFgmmRbrHaotmiz9LUcozlTMtayztWZCumVabVWqtWq/fWNtZJ1gutG6y7bfRtODaFNrU292xptn62U2yrba/ZEe2Ydtl26+0u26P2bvaZ9pX2lxxQB3cHocN6h47hhOGew0XDq4ffdKQ6shwLHGsdHzrpOkU4FTk1OL0cYTkiZcSKEa0jvjm7Oec4b3W+O1JrZNjIopFNI1+72LvwXCpdro2ijQoeNWdU46hXrg6uAtcNrrfc6G5j3Ba6Nbt9dfdwl7jXufd4WHqkelR53GRqM6OZS5jnPAmeAZ5zPI96fvRy98r32u/1l7ejd7b3Tu/u0TajBaO3jn7sY+7D9dns0+nL8E313eTb6Wfmx/Wr9nvkb+HP99/m/4xlx8pi7WK9DHAOkAQcCnjP9mLPYp8MxAJDAksC24O0ghKCKoIeBJsHZwTXBveFuIXMCDkZSggND10RepNjzOFxajh9YR5hs8JawqnhceEV4Y8i7CMkEU1j0DFhY1aNuRdpFSmKbIgCUZyoVVH3o22ip0QfiSHGRMdUxjyNHRk7M7Y1jh43KW5n3Lv4gPhl8XcTbBOkCc2J6onjE2sS3ycFJq1M6hw7YuyssReTDZOFyY0ppJTElG0p/eOCxq0Z1zXebXzx+BsTbCZMm3B+ouHEnInHJqlP4k46kEpITUrdmfqFG8Wt5vancdKq0vp4bN5a3gu+P381v0fgI1gpeJbuk74yvTvDJ2NVRk+mX2ZZZq+QLawQvsoKzdqY9T47Knt79kBOUs6eXJXc1NzDIi1RtqhlssnkaZM7xA7iYnHnFK8pa6b0ScIl2/KQvAl5jfna8Ee+TWor/UX6sMC3oLLgw9TEqQemaU4TTWubbj998fRnhcGFv83AZ/BmNM80mzlv5sNZrFmbZyOz02Y3z7GYs2BO19yQuTvmUeZlz/u9yLloZdHb+UnzmxYYL5i74PEvIb/UFqsVS4pvLvReuHERvki4qH3xqMXrFn8r4ZdcKHUuLSv9soS35MKvI38t/3VgafrS9mXuyzYsJy4XLb+xwm/FjpWaKwtXPl41ZlX9asbqktVv10xac77MtWzjWspa6drO8ojyxnWW65av+1KRWXG9MqByT5VR1eKq9+v5669s8N9Qt9F4Y+nGT5uEm25tDtlcX21dXbaFuKVgy9OtiVtbf2P+VrPNcFvptq/bRds7d8TuaKnxqKnZabRzWS1aK63t2TV+1+Xdgbsb6xzrNu/R3VO6F+yV7n2+L3Xfjf3h+5sPMA/UHbQ6WHWIfqikHqmfXt/XkNnQ2Zjc2HE47HBzk3fToSNOR7YfNTtaeUzn2LLjlOMLjg+cKDzRf1J8svdUxqnHzZOa754ee/paS0xL+5nwM+fOBp893cpqPXHO59zR817nD19gXmi46H6xvs2t7dDvbr8fandvr7/kcanxsuflpo7RHcev+F05dTXw6tlrnGsXr0de77iRcOPWzfE3O2/xb3Xfzrn96k7Bnc93594j3Cu5r3G/7IHRg+o/7P7Y0+neeexh4MO2R3GP7j7mPX7xJO/Jl64FT2lPy56ZPqvpduk+2hPcc/n5uOddL8QvPvcW/6n5Z9VL25cH//L/q61vbF/XK8mrgddL3hi82f7W9W1zf3T/g3e57z6/L/lg8GHHR+bH1k9Jn559nvqF9KX8q93Xpm/h3+4N5A4MiLkSrvxXAIMNTU8H4PV2AGjJANDh/owyTrH/kxui2LPKEfhPWLFHlJs7AHXw/z2mF/7d3ARg71a4/YL66uMBiKYBEO8J0FGjhtrgXk2+r5QZEe4DNkV9TctNA//GFHvOH/L++Qxkqq7g5/O/AH4lfFxdgsSLAAAAimVYSWZNTQAqAAAACAAEARoABQAAAAEAAAA+ARsABQAAAAEAAABGASgAAwAAAAEAAgAAh2kABAAAAAEAAABOAAAAAAAAAJAAAAABAAAAkAAAAAEAA5KGAAcAAAASAAAAeKACAAQAAAABAAADmqADAAQAAAABAAAAhAAAAABBU0NJSQAAAFNjcmVlbnNob3Rh9dS7AAAACXBIWXMAABYlAAAWJQFJUiTwAAAB1mlUWHRYTUw6Y29tLmFkb2JlLnhtcAAAAAAAPHg6eG1wbWV0YSB4bWxuczp4PSJhZG9iZTpuczptZXRhLyIgeDp4bXB0az0iWE1QIENvcmUgNi4wLjAiPgogICA8cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPgogICAgICA8cmRmOkRlc2NyaXB0aW9uIHJkZjphYm91dD0iIgogICAgICAgICAgICB4bWxuczpleGlmPSJodHRwOi8vbnMuYWRvYmUuY29tL2V4aWYvMS4wLyI+CiAgICAgICAgIDxleGlmOlBpeGVsWURpbWVuc2lvbj4xMzI8L2V4aWY6UGl4ZWxZRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpQaXhlbFhEaW1lbnNpb24+OTIyPC9leGlmOlBpeGVsWERpbWVuc2lvbj4KICAgICAgICAgPGV4aWY6VXNlckNvbW1lbnQ+U2NyZWVuc2hvdDwvZXhpZjpVc2VyQ29tbWVudD4KICAgICAgPC9yZGY6RGVzY3JpcHRpb24+CiAgIDwvcmRmOlJERj4KPC94OnhtcG1ldGE+CsvkLMoAAAAcaURPVAAAAAIAAAAAAAAAQgAAACgAAABCAAAAQgAALZ3OtNeYAAAtaUlEQVR4AexdCdAVxdW9KO4SF+JGjEYQCxOTKtdgNMEFJRgRpZSKWywNKiYSwBINFmpUDEaKRQhuuCBxxY1oJBCjBjdwBZfEjYgLStwhagJu89/T9d/+5s2beW9m3jy+N+Hcqu97Mz09vZyZvnNP9+3uDoGKUIgAESACRIAIEAEiQASIABEgAkSACBSEQAcSzYKQZDJEgAgQASJABIgAESACRIAIEAEi4BAg0eSLQASIABEgAkSACBABIkAEiAARIAKFIkCiWSicTIwIEAEiQASIABEgAkSACBABIkAESDT5DhABIkAEiAARIAJEgAgQASJABIhAoQiQaBYKJxMjAkSACBABIkAEiAARIAJEgAgQARJNvgNEgAgQASJABIgAESACRIAIEAEiUCgCmYnmihUrZN11160qxHPPPSf//Oc/pWfPnrLllltWXS8q4F//+pc888wz0rVrV+nevXtRyTIdIkAEmoTAl19+KdhFqWPHjlU5PPDAA7J8+XI5+OCDY69X3ZAz4O9//7ssWbJEdt99d9l0001zpsLbiAARWFUIJNka7733njzyyCOy/fbby0477dS04iD/uXPnyiabbCJ77LFH0/JpRsJJ2DUjL6ZJBFoJgaR3nxyl/Z5SKqL52muvycSJE2XhwoXyxhtvuNLuvffe8otf/MIRSwRcdNFFcuWVV8q1114rvXr1alqNZs2aJaeeeqoMGzZMfvWrXzUtn6ISnjlzplxzzTUuuQkTJki3bt2KSprpEIGWReCrr76S6667TmbPni1PPPGEKyc6oA499FAZMmSIrLfeei7soIMOkhdffFHwEdhggw2aVp/f/OY3Mn36dLnxxhu9zmpaZjkThp6AvgjLFltsId/+9rflyCOPbGoHXjhPHhOB9kIgja1x//33y6BBg+SUU06RESNGNK2o77zzjuy5556uc+qWW25pWj5FJfy3v/1N/vCHP8jjjz8un376qdOnP/7xj52+3WabbYrKhukQgZZDIM27T47S9timTp0qd999twtYZ511HC/B4B30RVN0BfbRrCUPPvhgoL2GwXbbbReo0g1OOumk4MADD3TnZ511lr91zJgxLkwfuA9rxsGcOXNcPpMnT25G8oWnedRRR7nyAr/LL7+88PSZIBFoNQT+85//BGoE+vd+4MCBwbHHHuv1yFtvveWL3LdvXxfvk08+8WHNOLjgggtcPvPnz29G8oWkaWWErthtt908XjjH3/PPP19IPkyECLQiAmltjfvuu8+1h4svvrip1dCRU5cPvuGtLpdddpkrK/QEdKoScadDcH7XXXe1evFZPiKQG4G07z45ShvE5557rtMXxu3MxsCvDhC0RSzoCC5tiaK9Yt7YufTSSyviPf3008H111/vw1bVQ/QZluBg2bJlFQ9zwIABJSg1i0gEGkPgqquucu+9ejYE6q7qE4M+Uc+I4N133/Vhq4po+gxb+MCIpo5sulLqqHCg0xGCX/7ylw7PMhi8LQwvi9bCCGSxNVYV0WxhuCqKplOJnH6Akfjwww/7azplIbjzzjsrwvxFHhCB/wEEsrz75ChtD9yIpnp5BbAz1HsjuPrqq70eQadfkVLTdRZusGr8CNxk4XZWS8LD0pjToApPli5dKt/73vdECZasueaaVbfDZQ5zLeAus/nmm8suu+wie+21V1U8BNgwr13E/MwePXrYadXvf//7X/nrX/8qixYtErjAbL311rL//vvLjjvuWBX3pZdeEh3pcHE322wz5yoDl5lG5U9/+pNz74WLMdx9UF/kg7rGyWOPPSZvvvmmu4Q5IU0Zwo7LmGFEoCAE0O7w7sJ169Zbb5Vdd921ZsrmOgu3/Keeesq52X7++edO5yS54D/00EOyYMECwVwtNa5c3B122KEqn48++sjpofAFzCFHG08SzAFXJSsvv/yyrFy50s0FRxnhwhoV6C7MF8fcT7id7LPPPm7eWDRe2vPRo0c7N/uzzz5bjj/+eH+bdurJ4Ycf7lzh4GIcln//+9/yl7/8xQV16tRJ+vTpE77MYyJQCgSy2Bph11m4lMNt7oUXXpBtt91W1HtCNt5446o6Q1fAHnjllVfcGhOwA+AmttZaa1XFxTca8U0wpzvJLkEcNcicnsE8cNgyX//6150NAbspavdoJ5srL2yBtddeW77zne8I9Es0nuWd5vfkk0+We++917kRw504jbz66qsCvQKB7oLtRSECZUMgy7tPjtL2dG0q0XnnnSfqbeYv3HDDDQL7A3Pf//jHP0qHDh38NRyAh8EugvzkJz/xU6BcQK1/tVirGjeO4aZxvbDegtNOO82zYhuOHTp0aFU2GA216+HfM888M/jiiy+q4ofj4PiSSy6pimMBaowF++23X2z6Tz75pEVzvxipjaaNc30QjulXRM54gnojrXnz5gXjxo1zx2p8J6ZiIxe4R+eiJsbjBSLQqgg8+uij7j3HaCZ6yuqJjWjqXKuqdnjzzTdX3A69EBcP7UXnUFXExcmzzz5blSbaYpLo/EjvwYE0w39hnQQ3X50nXnHd4iKNvBId0bR0rrjiCpfXcccdZ0H+Vw1sXw7oPAoRKCMCWWwNG9GEe2jU9Qt6B677YdEOrKp4aK/wMIJ7bFSQrrVn/ML1P0nef//9QDuFKuLbvVEvMF34LLYcSB/eT3lEO/Z83m+//XbqJHQup78PoxsUIlA2BLK+++QobU84PKLZFhoEH3/8sdcLcfokrG8xCppWarrOYp4QlCaMmXpiDxHxQaZQiBkzZvhCa0+fT0IXFPLh99xzTwC3GR3RcHNAcT/CoqIjDQHmdpmCTCKa+MhgLinSUWbuXPfwQmJ+0wknnBCE52hpD6eLh/gwkAEySLXNQY0rR7RcSefK+n0dcYx8USbMcU0SEs0kZBheFgTQ9vGeY45mGjGiCV2DNqIj+sHvfvc7lwbCwmQVbRNpo72CREJvWBjCw3M/kTfIIcLwZx1gSURTRxhc2kgHnWAffPBBoKOFAVxIQOB0lNVX58ILL3RxdTQlUI+JAMYm3E5MCaMOecSIJtIFBtBf1mGGtNXjoSpZEs0qSBhQQgSy2BpGNNFW0SEM20BHNX1buf322z0C0AHWhsaPH+8IHdz50WmD++NI1ocffuh0hrnl1SKa1uGEOP/4xz8CXfEyeP311137/f3vf+/Lod5dXr/ALoKtAX0HmySpHP7mGgewq3A//rKI2VGN5J0lP8YlAkUjkPXdJ0dpewJJRBMx0FkHvYAOuqiYjYPrhRDNMFFKk6A9RBhJYTGFDlJnYoakDmVbkPtVFzBXwVpzGUH+UMkkoomJrLiOXsY4CRuM9gGKjtjCGEUatcoRl3Y4TN3qXBpGLMN4wkCOExLNOFQYViYE0C7RduIMuLh6GNEML+712WefuTSQjrq/+tv69+/vwtV1zofhwAgaDMkkMcWaRDQHDx7s0taVs6uSCOuMsMEYnn+Km6zutcpRlXgowOqBekf/pk2bFmDOVVRINKOI8LxsCIS/jWlsjTDRDI9eYrE9tBt0BJlgFBFh/fr1syD3i3ni1sZA+uIEBBZxkogmOq9xHcZX3GJmYb0BQoy4o0aNqsgKnVlWjiS7oOKGyIl5kMCWySIkmlnQYtxWRCDru0+O0vYUzR7CHM2o1PIuKZxoQumZAkTvfj2xh4gVoMICAxLphF3bQL4QFh5dxD02FI7KJEk9ommudSCttQQfBqvf6aefHsBl1/4sDVwPu8zVSi96zYzG8IJJVm98KOMEHzz0puIPH18KESgbAuYijvc/jRjRjPaeocMK7Q+L4ZhYewURDYt16qCjJklMsSYRTRtRQdurJRg5sXKYvrBfc6HDKEUeMZ0BDEFi0WMLHWkr+GLl3qhAP5nOyOt+F02T50RgVSKQ1dYwohltDxjxR9scPny4Lz46aBA2adIkH2YHZlDBmyFO6hFNjJwi7TS6zjrJQFpNX9iv6R6MoGYVeFygDPDCyiKwtUxv5CG4WfJiXCLQDASyvvvkKG1PweyhOKJpthcWEouK6Qz8xnV8R+PbeU3XWWOvujCGxU/8tYeIHsSwmKLX/et8sBmXcDuLiuWZ1MtYj2jax6PeVgDhkQAMFSf95VHCcPcz99077rjDuQXDNVgn3rqPwsiRI6PV5jkR+J9AAO0cho/ucZuqPqYL0LMfFuuUMd0TXsE5HA/HupiXyxPGXJKYYo0jmhgVQZnxV0/CIwFJOgOjo3nEiKatOmtpYGQEeaF8WO2bQgT+1xCw776191r1M6IJz6iw2AhjWPeYXRKd7437zO01arNYmvWIpu6L7dpkmu0ArH4glUl6I9rxbuWo9Qu8oBeQPoUIrE4IZH33TRdE2/vqyFHMHoojmubpGWcr5X2/ahJN64WLuqrFZWYPET3+YYl7iLXIoBl80VELS7Me0bRRhegIid1vv5hHgbyg9IsWzNWwesT94qOQd6S06LIyPSJQJAJJrmpJeRjRjLqeRYmmeTugPUXFFv2Juu2H45lijVOeaIvWTuu1S3QcIW7UBS6cV97jJKKJ9DBKg3zD88/y5sP7iECrIZDF1jCiGd1HM45oYjsltJs4MmiL/sTNfQY+9YimLdIV9eKKw9ZGLeF6X6SgQ950Fz0aikSWabU6AlnffXKUtidq9lCUaIanLYXX1Wm7M99RTaJpcynjVo2NZpflIdrCHNGVVbHKEZQmlHKS1COaZqzFfVjCaYaNy/A8j3CcvMdTpkxx9YALHR5k+M9GOuNGJoAH5nnhD6M0FCJQNgSwMI4ZPrqVQN3ipyWaSMiMteg8LlsQ6IwzzkjMzxRrHNHETbYAWD1PCLRb1K8Ze1qa7oqOaKJ8hlPYFR/hmNZgOiP60cB1ChEoAwJZbI0sRNMWJ0P7Dwu8jtDJjLaMTuc4qUc058yZ4+5P48Fga1U8/vjjcVk1FIb5p6jHTTfdlDoddM6Z3kgzkJA6YUYkAqsQgSzvPjlK24MxeyhqM+j2JU6XgKeE55jbnehcM70RHRywOHG/NYmmKVoosagygoEDH2mTLA8RbixmrIVHEGx+F+ZMJkk9omkThGGUhjeGR3pY7RZ1MoGLDcoRN39j+fLlQRo3Hksr/Gsvf3jzZLtuH9SxY8dakP/lYkAeCh6UGAEjTJhDFXU9x6JgYTdZI1BRpRUd0QQctl0QRilMoD8w7wntuNbWQaZYk4imjXygcyjqTQHvCJuPEF7VOs7VLapjrJxpfg23KNFEmVE//EV7GcNTALIuCJKmTIxDBFYFAllsjSxEE51daDfwIsK8IhOb3wWDKmyD2HX8WpmSFgNCeuYSGx0VhT6DZ5OJTSkA4YwacDjH1Jq8Yosooi5x+gE6IirhKQBREh6Ny3Mi0KoIZHn3yVHanqLZQ2GiCb5inflJnlOm76BTox3+balXH3VAUK19NtXoEWwkDsHGwj169BBdvl909E0OOeQQ0RXe3LXwZqjaU+jC8E9HFgWbgiKeura5cDXk3ObmqsjdZusHHHCAYLNjXfbbXcfmw926dXPH+Dd79mzRRUHcufbEuc2JsfG6baKMDZqPPvpoH3/IkCGihFQ6d+7swrFBu84HdWXRXj/5/ve/7+KqG4tPY//993dlUYNSsJnybbfdJmp0irrI+XTTHITTRJ3WW2+9ituUCMsxxxzjNprXD2bFNZ0z4nBFoI6KihrhFdd5QgTKgAA2JUdbX7x4sXvPlQDJOuusI2qMiZI2t7l5ly5dXFWgU9DedO9b2WCDDXz1bCNmHTWQ7t27u3Bstt6nTx93jPa+ww47ON2gREy23HJLmTt3bsUG7FdddZXfXBj6APmo277b2B2JYLP0ffbZx6WnBFJ69+4t0EmqTN1mxNhQHZua417tdJKOHTu6uNBPKB8E5cDm7+q25uqmrsOirnS+nC5Syn/Qs9C3qsTdJurQRbo1i9tkHkkgT11ApCI11AkYQnBfVKdUROYJEWhhBNLaGtrpLer2KrpIlujCfb5G+N5qJ6/70153Hz5s2DBRrwfXPqCXlASKdi676+p+63SCRYYeUhLqTnXFa9ceoVvC9oVOz5H111/fxVEC6W2En/3sZ04nQYfAfkBesEUgaMuHHXaYqMeEs6FgO3Xq1MnpSGyMvummmwp0XR7R0VmHBfQSdCg2Uod+VYLp0gQWwCUs6hkh55xzjgtCuXVV3PBlHhOBUiCQ5d0nR2l7pGjvSjKdToTugR2hgwIugnasOb625pprtt3w/0ff/e53fTztaJfNN9+8Kk5sQDX3rA7BKIS5mYDJ4g/MNjyCYCN10TmaYMyIH151FjlgbzsbibA00SP35JNPVhXARjIsXvQ3uuIaeiixWXI0HuaBRN1ksOedubWE42OkBa4xWcVcdZK2VwnPNYuWxRYnQDmibsVZy8H4RKA9EcCoZVy7hcspvAVMbPQ/OvJpK62GV53FPXA9g54It1U16IK4uU/h3rdwfDvGCGJYMEJhbv0WB78oS3TUA6OZNmk+HBe6JK8nhI1ohtNDDyP0JPRKdKQVZbeFkHAPRzTDT5PHZUQgja1hI5rRbYRsjmZ41VlgAC8EzKkOtyschxcoNKzM2yoaN3weHhnFfRhVsZEAiwcdFbWFoOPUwKsqB+4Nb+9kZcnyC48LpBHVedAJcJONClzwrawc0Yyiw/MyIZD23SdHaXuqNqJpOgB2GPRm2Eu1LXbbUVi/FDqiGWan6LVX1wzZZJNN5Bvf+Ibv4Q/HyXqsRqco4XLMGD2HRQp6OzAioB8GN+KxxRZbJCavBNDVDfegNxB1pBABItAYAujJx8imbmTudEZR7QqjBhg53XbbbWWjjTZqrJCRu+FxAZ2E36233rpm+tCJ6i7rRjigN2ykI5IkT4kAEciAQDNsDXzjX331VTfq981vflPieuwzFLEqqs5PF92WyI1Owj5KSh86EfpFFzMR2CQYFVhjjTWq0ssbAK8qNQJd2rCpOnTokDcp3kcESoVAM959cpTGX4G6rrONZ8EUiAARIAJEgAgQASJABIgAESACRGB1QoBEc3V62qwrESACRIAIEAEiQASIABEgAkRgFSBAorkKQGYWRIAIEAEiQASIABEgAkSACBCB1QkBEs3V6WmzrkSACBABIkAEiAARIAJEgAgQgVWAAInmKgCZWRABIkAEiAARIAJEgAgQASJABFYnBEg0V6enzboSASJABIgAESACRIAIEAEiQARWAQKpiSY2NMdm6yZYxn/XXXe1U/42iMATTzzhtoCwZIAtt0owNPhbVgSwaTiW/Tfp0aOHbLbZZnbK3wYQwBYGumenTwG4Al8KESgzArQ1mvv0XnnlFcH2UCbdu3d327/ZOX+JQBkRePvtt0X33fZF32qrrWT77bf35zzIj8DKlStF9zD3Cay11lrSs2dPf17vIDXRfO6556R///4+PSinOXPm+HMeNIbA7rvvLh988IFPRDeulq5du/pzHhCBMiJw4oknim6w7ot+2WWXSZ8+ffw5D/IjcNttt8kZZ5zhEzjssMNk3Lhx/pwHRKCMCNDWaO5TGzFihNx+++0+E93IXo444gh/zgMiUEYErrnmGhk9erQv+gknnCCjRo3y5zzIjwD2/d133319Ap07dxYMjqWVzEQTmZ122mmy4YYbus3Swxl9/vnnMmHCBGdYotcM5AkK7PDDDw9Hy3X81Vdfydy5c+XGG2+UTz/9VI455hg56KCDcqUVvQkbv1933XWyYMECWXvttWXatGnRKLnOn3zySbniiitk/vz50qlTJ9lrr70ESh4bNEflpZdeki+++ELOP/989wBJNKMI8byMCBjRRDvo0qWLbLfddlUj9ehdHzt2rDz44INuVB89ZaeccorssssuDVcZm7TfcccdMmvWLAmCwH2IiurAwWjttdde6zZp33nnnV3bbrjAmkDaMn/00UeCXlzoWuhkEs0i0Gca7Y2AEc1atgbKOGPGDLn11ludpxVG8vfbbz8ZNmyYdOzYseEqtHfbzlqBLDp0yZIlgk3oZ86cKVdffbWQaGZFm/FbEQEjmkOHDpXevXs7OzvqPUWOUvnk0nKUzz77zNkZuLtfv36SlWjC+Eolzz77bKBGYqAGTWx8JUnBoEGDXJyddtopGDhwoDvGParMYu9JG3jPPfcEBx54oE8PaerISNrbE+MtXbo0OPfccyvSRdpFyLx583y6+mCCXr16uXP8qstbYhaGoboAJMbhBSJQFgTsfV68eHFskdVAqmgb6jXh2432mMXekyZQ3e+CK6+8MoAuQpu2P3X/T3N7zTjQhVYvS/e4446reU+ai3nLrIa5q1+Sbk6TN+MQgVZBoJ6tgXLi+29tD7aGtfPBgwcHsEXySqu17TT1yKtDYZcBQyXsabJhHCLQ0gjY+6ydT7HlJEephCUvR4HO2G233SoTq3NWGNF84IEHnNLac889g2XLlrlsn376af8x0B60OkVJvmwfFBhzOhzu0iyCaOoopksLH6mJEyf6siaXJP0VkEuUG3lA8JIff/zxLmz8+PGJCZkBS6KZCBEvlAgBe5+TiKb2prs2oSOYgXotuJqZETlgwIDcNX3++ed9e1b3GU9miyCaMGbRtnUEJUDaOC6CaOYtM4lm7teEN7YgAvWI5ocffujb9jPPPONqgDAYP2iLDz/8cO5atVrbTlORvDrUDHMSzTQoM06rI2DvcxLRJEepfIJ5OQp0bLsRzVNPPdUp+VtuucXXBg8chcLfzTff7MOzHqCn3j4oRgiLIJoYKdXh9kBdcYMVK1b4smYtXzS+usG6tPAwzHhWdzhv7CI8qdfVDHMSzSiqPC8jAvY+xxFNdWPxxmH4fT/99NN9W1y0aFGuauucguCCCy4I4LUAUfd9l2YRRPOSSy4JZs+e7drwo48+6tItgmjmLTOJZq5XhDe1KAL1iOb111/v2tzw4cN9DdR93OsMdZ3z4VkPWq1t1yt/IzrUDHMSzXoo83oZELD3OYlokqO0PcVGOEq7Ek3rTXzttddcbXSlSe/OgoLpohVttWzgqEiiGS5GkUQTiht11vmYPgsrN8LxB6MyTswwDxvecfEYRgTKgIC9z3FEE+842kK4d+yRRx7xBiOu6fzKQqpZJNEMF6hIohlOF8dpy0yiGUWO52VGoB7RRMczdIMZlF9++WXFVJ2wPmkEh1Zo2/XK34gONcOcRLMeyrxeBgTsfTa9EC0zOUobIo1wlKjN1pZq8lEhrrMYnUPm+NNJoy43+xighxDhcBstQoywFTGiGS5PkUTz0ksvdXVG3SG6BYE7P+mkkwK4AwIPuBXHiRnmJJpx6DCsbAjY+xxHNHUiumsLmGMFsVF/fBBs7rQuIlRIldOStqyZtYIxSqKZ9akxfisjUI9oHnvssU5vYI4RxDynJk+e7MLxfQX5bFRaoW3Xq0MjOtQMcxLNeijzehkQsPc5jmiSo1Q+wUY4CvRr1s68QogmRi+ROf4gmCOB4zFjxgQLFy50x3379q2sac6zMhBNuOyh/rp6rfvg2cJIcOOzOaa6qmwsAmaYk2jGwsPAkiFg73Mc0YT7KdoJOmAgumK1O9ftUIKpU6e64wsvvLCQGpNoFgIjEyECTUegHtG0hQExp/m9995znlNYRCxsTOqKzA2XswxEsxEdaoY5iWbDrwoTaAEE7H2OI5rkKJUPqBGOAputXYgmVj1D5vjDyom9dGVVLAqEuY/W246FM4qQMhDNs88+22Exffp0t6IbcMG8EgiMapzjAxEnZpiTaMahw7CyIWDvcxzRxBxptAUsBGSj/phHAcHcaVw777zzCqkyiWYhMDIRItB0BOoRTdgX0A26BYlbBd+OUTAc4w8EtFEpA9FsRIeaYU6i2eibwvtbAQF7n+OIJjlK5RNqhKNAv7YL0Vy5cqVX8LYCmu6J52pmS+geddRRlTXNeVYGomnuwliREivawsg1Vx4b3UzausEMcxLNnC8Ib2spBOx9jiOaphvQPvCHtqJ72rryWxuaMmVKIfUh0SwERiZCBJqOQD2iad/Qiy66yNkd48aNc2UK2yFJi+1lKXwZiGYjOtQMcxLNLG8F47YqAvY+xxHNsG4gRwkCs6/ycJR2I5p48WwfKxQCq0aa3H333e5jEF4hzq7l+S0D0bzhhhs88QYeWBHPxHpjk4ikGeZJ1y0d/hKBMiBg73Mc0bRRTLQR/IU/ELZtSHgV60bqS6LZCHq8lwisOgTqEU1bPRI6A99TzO2GYGoKwmCLFCFlIJqN6FAzzEk0i3hbmEZ7I2Dvc9iOCJeJHKUNjUY4CnRsu4xoovhHHnmkV/LY08rEDEZMPi1CykA058+f74nmpEmTfLXtQ4gHBRfjODHDnEQzDh2GlQ0Be5/jiCZc69EW8AePB9sKCHW0DhksdlGEkGgWgSLTIALNR6Ae0cQ31fQGvrUm1qmNxYKKkDIQzUZ0qBnmJJpFvC1Mo70RsPc5iWiSo7Q9oUY4CnRvuxFNPFwUAJuAmtsK/KKtF+HNN99sq2UDR2Ugmqg/5qgCj/BcTLj4IGzkyJGJCJhhTqKZCBEvlAgBe5/jiCaqgS2A0CYwOd0EbvcIA9k0l3O7lveXRDMvcryPCKxaBOoRTWyhBv2AP9snF99cc6mdOXNmIQUuA9FERfPqUDPMSTQLeV2YSDsjYO9zEtEkR2l7QI1wFOjdrESzA7KWFKKL+oiu7CaHHXaYKGGquuOTTz6Rgw8+WN544w3p2bOn7LHHHqI9jKIGphx66KEyfvz4qnvSBjz00EOiq1C66LqBuyiBlc6dO8uOO+7ownSPTlFCmza5injDhg0THYEVNWhF5zu4a3vvvbf73X777eWcc86piJ/2RIemRSfcygYbbCA//elP5eOPPxZV6O52ncDvyx5N78QTTxRddVN0VVrp2rVr9DLPiUCpELD3+f7775dvfetbVWXXlSPlkEMOceFKBmXTTTcVtB3tqRedgyVqPFbdkzYAOgO6A7JgwQKXZvfu3WWLLbaQNdZYQ3SLJFlvvfXSJufj6TxS0ekB7nzZsmWCOkBMb6A+qEseyVNmwzBJN+cpB+8hAu2FQD1bA+UaMmSI4Du6zTbbOPtCtwsTXe3enc+aNUvWX3/9XMVvtbadphLW/hE3iw7VBddk9OjRonPW5IgjjkiTFeMQgZZFwN7niy++OPb7S45S+ejychTwEvAvXWemMsEaZ4URTeQBAjh48GDRHkmfJRTfb3/7W+nYsaMPy3qgPZSi+3Im3qY9GbLvvvsmXq914Qc/+IErd1wckNe77ror7lKqMHvxLTIezuWXXy677rqrBVX9mmFOolkFDQNKiIC9z0lEE1V67LHHROddyQcffOBqiM4ZdB6pC1xDNf71r3/tO3fiEgL53GijjeIu1QxbsmSJ/OhHP0qMc/LJJ8uZZ56ZeL3WhTxlNkOTRLMWsrxWFgTSEE3dr1vUM0juvPNOX62dd97ZdR5tvvnmPizrQau17bTlz6NDzT4h0UyLMuO1MgL2PicRTZSdHKXyCRpmFpqGo6wSomkFqkXCdA8reeuttwQjguuuu67dslr+YqQUo7AwnrfeeutEDPbff383+msRSDQNCf6WGQEjmlaHK664Qg444AA7rfhV93rRhT2kW7dusuaaa1Zc40k1AvAYGTp0qL9Aoumh4EGJETCiaVWoZWtAX+g0E/dt3Xjjje2W1fY3jQ6Fl5Zut+YxItH0UPCgxAhESZNO25GzzjortkbkKG2wpOEob7/9tvfYwp1NG9FET8BNN93kS4dew6OPPtqf86AxBGCA6wJBPpGf//zn8rWvfc2f84AIlBEBjDjonCpfdLjf0yXcw9HQAUYy7733Xp8GDPIkEu8j8YAItDgCtDWa+4D+/Oc/y4svvugz6du3r/To0cOf84AIlBEBXTjQT5VB+XUeofzwhz8sY1Varswg5tOmTfPlAjcBR0krqV1n0ybIeESACBABIkAEiAARIAJEgAgQASKweiNAorl6P3/WnggQASJABIgAESACRIAIEAEiUDgCJJqFQ8oEiQARIAJEgAgQASJABIgAESACqzcCJJqr9/Nn7YkAESACRIAIEAEiQASIABEgAoUjQKJZOKRMkAgQASJABIgAESACRIAIEAEisHojkJpoYkXUp556yqOFDZFr7QfpI/IgFQLY/HTFihU+LrDNu+m0T4QHRKCdEXjhhRfk/fff96XA6oabbbaZP+dBfgTeeecdefnll30CwJWrR3o4eFBSBGhrNPfBvfLKKxV7h3fv3l223HLL5mbK1IlAkxHAFhzY6shkq622clss2jl/8yOwcuVKefzxx30Ca621lvTs2dOf1ztITTSje1tBOc2ZM6de+ryeEoHdd9/db1iPW7iPZkrgGK2lEYjuo3nZZZdJnz59WrrMZSncbbfdJmeccYYvLvfR9FDwoMQI0NZo7sMbMWKE3H777T4T7qPpoeBBiRGI7qN5wgknyKhRo0pco9Yp+uuvvy777ruvL1DT9tE05Y/MTjvtNNlwww1l22239Rnj4PPPP5cJEybIfffdJ+g1A3k64ogj5PDDD6+Il+cEe+Rgr8n58+dLp06dZK+99hIoTOzn2ai8++67ct1118mCBQtk7bXXrtgvppG0v/rqK5k7d67ceOON8umnn8oxxxwjBx10UGySL730knzxxRdy/vnnC0Y3STRjYWJgyRAwoom226VLF9luu+1iR+ox8nnttdfKkiVLZOedd3Ztu4iqzpgxQ2699VbnjYHRvv3220+GDRsmHTt2bDj5ZpUZm9DfcccdMmvWLAmCQEaPHh279yj2tkIvLnQtdDKJZsOPlAm0AAJpbA0Us1ltG/t4jh07Vh588EHnZYSe+1NOOUV22WWXhtFJ27bzZJRWH0HHLl++XGbOnClXX321kGjmQZv3tBoCRjSHDh0qvXv3dtwg6j1VRo7SzDKn5SifffaZszPwzPv16ydZiSYMmVTy7LPPBmokBmrQxMZXkhQMGjTIxdGNw4OBAwe6Y9yjyiz2nrSB8+bN82lpJYNevXq5c/yq+1jaZKriLV26NDj33HN92igr/oqQe+65JzjwwAMr0tbRnLpJG4bqAlA3LiMQgVZHwN7nxYsXxxYVesXiWPs77rjjYuNmDUR7szShj6CXcD548OAA+iqvNKvM6jIYXHnllb6cVnadslCzqGqYu3ol6eaaN/MiEWgxBOrZGihus9q2kswK+6J///5eh2gHcG6k8rbtNBnm1Uewy6BjlLCnyYZxiEBLI2Dvs3Ysx5azjBylmWXOy1GgM3bbbbdYjJMCCyOaDzzwgFNae+65Z7Bs2TKX39NPP+2VtPagJZWhbjjIJSqno44uLsA//vjjXdj48ePr3p8UAekhXRigEydO9GVNip8lHOniD0azDuG7YxLNLAgy7v8CAkYik4gmSB/aiY40Burm4ttMo3X/8MMPXVpI+5lnnnHJIQwKEmEPP/xw7iyaVebnn3/elxlYWIcaiWbuR8UbS4hAPaLZzLato3uuDeoIZqC9/Q49I7UDBgzIjWbetp0mw7z6yAxzEs00KDNOqyNg73MS0SwjR2lmmWEH4S8rR8E97UY0Tz31VFfoW265xb+PeOBWmZtvvtmHZzlQl1KXBipmil/dT7wRhvC8oxNg9DrcHqhba6AL8fiyZilfUlyMLpiBaySWRDMJLYb/ryJQj2hecsklwezZs10bfvTRR10bhOJrVK6//nqX1vDhw31S6mLq27i61/jwrAfNKrPOgwguuOCCAJ4WEJ1y4MpLopn1CTF+mRGoRzSb1bbVRc13RIU9ik4//XSvNxYtWpQL2rxtO01mefWRGeYkmmlQZpxWR8De5ySiWUaO0qwy41nm5SjtSjRtpOC1115z76OuNFnhAqaLVuR6T6EEUTGdj+nvN+JmJBZKvFEpmmiGy2PlJdEMo8Lj1QGBekQzjEGRRBNKFPrBPjpffvllhTt/1h65cDnDx0WWOZwujkk0o4jwfHVAoB7RbFbbBrmMGlGPPPKIJ5m4pnOnC3kEadt21syy6CMzzEk0s6LM+K2IgL3P9s2PlrGMHKVZZY5ik4WjRHVkNK2480JcZzGiiMzxp5NGXT72MUBvG8Lh6ppHLr30Unc/0oHocv7u/KSTTgrgyoK04aLbqJBoNoog7ycC1Qi0F9E89thjnW7A/G6IeVdMnjzZhUNvgHw2KlkMu6x5pTVGOUczK7KM38oI1COazWrbuuCg0w2Yzw0xzykYe7aWgy5qVgh0adt21syy6CMzzEk0s6LM+K2IgL3PcUSzjBylmWWOPr9SEE2MXsJwwx8E859wPGbMmGDhwoXuuG/fvtG6pTqHKxnSmjZtmjMMbZEhuJfZ3EddoTVVWrUikWjWQofXiEA+BNqLaNpCXJgb9d577znvCizsEVbeumprvkqF7spi2IVuS3WY1hgl0UwFJyOVBIF6RLNZbRsu/LA10IkN0RX03bmuoh9MnTrVHV944YWFoJi2bWfNLIs+MsOcRDMryozfigjY+xxHNMvIUZpZ5ujzKwXRxEptUND4w+pqWMQCiwJh7qMZQVjsI4+cffbZLt3p06e71dGQB+ZoQPBBwDk+EI0KiWajCPJ+IlCNQHsRTVtIR5f8d3MRoCdwDMEx/kBAG5Ushl3WvNIao6Zj4UVCIQJlR6Ae0WxW28aaDdALWAjIPKcwRwqCtRxw7bzzzisE3rRtO2tmWfSRGeYkmllRZvxWRMDe5ziiWUaO0swyR59fKYjmypUrvfFmq7bpHlSuLrY1yVFHHRWtW6pzc73FKoxYHRYK2lzebHSzkWXHrRAkmoYEf4lAcQi0F9E03XDRRRc53TRu3DhXqbCuwuhmo5LFsMuaV1pjlEQzK7KM38oI1COazWrbZqug3eEP9obuse2gMjtkypQphUCXtm1nzSyLPjLDnEQzK8qM34oI2PscRzTD3/2ycJRmljn6/EpBNFFo26MOvX5Ypc3k7rvvdoZeePVHu5bm94YbbvAkFmlj5UgT69kMrxBn17L+kmhmRYzxiUB9BNqLaNpqbdAZ0BOYbwWByz3CoK+KkCyGXdb80hqjJJpZkWX8VkagHtFsVtu2UUzoB/yFDVbbeim8qn4jGKZt21nzyKKPzDAn0cyKMuO3IgL2PofbbbicZeQozSpzGBccl4ZoHnnkkd6Awz5XJqagsahPHpk/f74nmpMmTfJJmMGIDwLcdRsVEs1GEeT9RKAagfYimtAVZjBCh5hYxxcWFClCshh2WfNLa4ySaGZFlvFbGYF6RLNZbRtTfUxnwAPLtlMDVtapjQWDipC0bTtrXln0kRnmJJpZUWb8VkTA3uckollGjtKsMkefX2mIJh4ulHS/fv38vpbwMTZG/uabb0brluoc7m2Y74m0w3Mx4QqHsJEjR6ZKp14kEs16CPE6EciOQHsRTWyzZEaj7UsJXWJudzNnzsxemZg7shh2MbfXDEprjJJo1oSRF0uGQD2i2cy2jW3UoDewCKEJpgEhDGTTpu3Ytby/adt21vSz6CMzzEk0s6LM+K2IgL3PSUSzjBylWWWOPr9mE83/AwAA///5rVoBAAAhSElEQVTtnXuMFEUex38CXnjcIYEIeiEcAptAcv9wKFk90CCHhgvyMCwBOWIUUEwgiwRBiGA8V0UM6yuKSNDVwKH4gEAAPQRzgoIvUM4cqFxQXioKrgFOBbGvvpX8ypmemZ2enml3evpbyaS7qrurqz5dVfP7dlVXneMZJwHcv//9bxkxYoSMGjVKFi1alHHFyZMnZdiwYXLgwAGprq6W/v37y7p162T//v0ycuRIqa+vz7gmaMCKFStk3rx50q5dOxk7dqycOHFCVq1aZS9fv3699OnTJ2hUGedNnz5djh8/LmfPnpXt27fb4wMGDLDbXr16yfz58zOuCRKwdetWWbp0qT1137598uWXX0qnTp1cWmfNmiV//OMfM6KaPHmybN68WV577TXp0aNHxnEGkECcCGh53rJli3Tv3j0j6UePHpWZM2fa8MbGRvnoo4/svtbB4cOHy+jRozOuCxIwbdo0QfvQrVs32wbt3LlTtm3bZv0bNmyQtm3bBokm45wo04w2A20H3K5du+TUqVNSVVUlXbp0kRYtWsjixYulTZs2aWkCM3DK1TannUwPCZQ5gXy2BpIfVd3WuoR7oN3p2LGjwP5APVywYIGMGTMGh0K5MHU7yI3CtkdPPfWU1NXVyf333y81NTVBbsVzSKBsCWh5XrhwYVabIY4aJco0h9Uo0CXQMu+++27gsnBOqYQm7ggxNWXKFNm9e7dLABrre++9V1q1auXCwuxoIdJrkdEnnnhC+vXrp0GhtpdddplNd7aLIQTXrl2b7VDesDVr1siMGTNynrds2TIZNGhQxnE1zCk0M9AwIIYEtDznEpqHDh2Syy+/PGfObr75Zpk9e3bO400dOH36tMyZM0dWr17tTuvbt68Va507d3Zhhe5Emebbb7/dvUTLli6Iz/POOy/tkBrHFJppWOiJKYEgQjOqug1kb7/9tkydOlWOHTtmCeIFN14MT5gwoSiiYep2kBuGbY/UpqLQDEKZ55Q7AS3PuYQm0h9HjRJVmsNqlF9FaGpha0qEffvtt3L48GFBj2Dr1q31kqK36HVE7yAa/q5duxYdXzlFMHjwYNv7q2mi0FQS3MaZgApNzcOSJUtkyJAh6v1Vtt9//73897//tW1Ghw4dfpV7/ho3wYiR2tpadysKTYeCOzEmoEJTs9CUrRFl3T548KAg/p49e0rLli01ObHfYpTW8uXLXT4oNB0K7sSYgApNzcKkSZNk7ty56k3bxlGjRJXmNDA5PEeOHBEdZYZTIuvRhKpeuXKlSwZ6BMaPH+/83CmOAAzw//3vfy6SiRMnSvv27Z2fOyQQRwLoTfzss89c0jH8nkPCHY6idtCTuWnTJhcHDPJfW8S7m3OHBEpEgLZGiUDmiGbjxo2yd+9ed3To0KHSu3dv5+cOCcSRwHvvvec+O0H6L774Yhk4cGAcs1J2aYbIbWhocOmCNoFGCeoCD50NGiHPIwESIAESIAESIAESIAESIAESSDYBCs1kP3/mngRIgARIgARIgARIgARIgARKToBCs+RIGSEJkAAJkAAJkAAJkAAJkAAJJJsAhWaynz9zTwIkQAIkQAIkQAIkQAIkQAIlJ0ChWXKkjJAESIAESIAESIAESIAESIAEkk2AQjPZz5+5JwESIAESIAESIAESIAESIIGSEwgsNLH0xvvvv+8S0LZtW+nXr5/zc6c4Au+++6788MMPLhKwBWM6EogzgT179sg333zjsoBp9M8//3zn5054Al999ZV88sknLgJw5TIFDgd3YkqAtka0D+7TTz+1C9frXaqqquSCCy5QL7ckEEsCWOsR62Wru/DCC6VXr17q5bYIAj/++KO88847LoZzzz1XqqurnT/fTmCh6V9EGY3Tq6++mi9+Hg9I4JJLLpFjx465s1977TWuN+hocCeuBCZPniybN292yV+8eLFcffXVzs+d8ARefPFFmTVrlotg1KhRsmjRIufnDgnEkQBtjWif2m233SYvvfSSu8n9998vNTU1zs8dEogjgaeeekrq6upc0m+88Ua54447nJ874Ql8/vnnMmjQIBdBp06dBJ1jgZ0X0O3evdu76KKLPPPwPLNQuGcWYc+48vTp055ptLyrrrrKnjtmzBjvhRdeyDgvTIDJlDdp0iTPLEruXXrppd7MmTM980Y/TFRp10SZ5rNnz3pbtmyx6R43bpy3fv36tHuneswCypYrmIGzeTOTepj7JBBLAqizKM///Oc/bfk+depURj6++OILb8aMGZ5ZYNnWb1xjRk9knBcm4Pnnn/dGjx5t02AWJvceeOAB78yZM2GiSrsmyjSbHh1v+fLl3nXXXeeh3cjVFhw/ftwyXb16tc0fGNKRQNwJBLE1kMeg9SQMj//85z+eEWS2/i1cuDBMFFmvKYf26ODBg7bdMEa5bTdWrVqVNa0MJIE4EVi2bJktzw899JAt30ePHs1IfpT2fiVrFNOjaZlC+8Geg61WiJOgJ2vjn8uY+emnn6ygQiIgBlUwwY8CUIzbvn27zRziuuaaa7wrrrjC+rEtRmxGmWaIShXcSDd+pjcnLwY1zHMZl3kj4AkkUEYEtDzv378/a6q+/PLLtPo8YsQIV9fRcBfjUN+07qE9QrsE/5QpUzzU/bAuqjTDcH7yySddOjXt+US36QGy+crVNofNJ68jgeYgkM/WCFtPguQF99Y2S+vf9ddfH+TSvOeUW3ukhjmFZt5HxxNiQEDLc67OrSjt/SRpFLSLzSY0X3/9dWvsoLexsbHRFsudO3c6Q++7774LXVQhLpG5Z555xsaBAnPDDTfYsPr6+tDxRpnm1D8p9ALDT6EZ+lHxwpgSUKMtl9DECAjUjVtuucX7+eefbS7VILv22mtD5xq9fVoHP/zwQxsPwtBAInzbtm2h444qzfq2EOkzQ36cAKfQDP2oeGEMCeQTmmHrSRAUeAmF+nfllVfaOoj9UgjNcmyP1DCn0AxSMnhOuRPQ8pxLaEZp7ydJo6BNbDahOXXqVNtAY2iIOjxwJAq/5557ToML2n788cf2emRMDdHvv//eGWEID9s7EVWakUH0LqiBi658MKDQLOjR8+QKINCU0MQQVhV+qT34GBav7ca+fftCUcDQU8Rx6623uuvNJBgu3traWhdeyE6UaTbfQXh33323h2G5cDrkl0KzkCfEc+NOIJ/QDFtPgnB5+OGHvVdeecXaFG+99ZZtL0ohNMuxPVLDnEIzSMngOeVOQMtzLqEZlb2fNI0Cu6rZhKYajPrtpplpMm0ImJm0IlQ5RSOIjOF7CXUq3NQYxR9PGBdVmv1p0fRSaPrJ0F/pBJoSmhCX/kbrzTffdGIQx15++eVQiPCiB9frnw6+l04dzl9oQ6mJiDLNeg/dUmgqCW6TRCCf0PSzCFpP/Nfl85dSaJZje6SGOYVmvpLA43EgoOVZ//P9aY7K3k+aRvHbbH7O2fwl+UYTPYq4OX742BZOG1a8IUQ4hrqGcY8//ri9HvHAmen8rf+mm27yMLQOcWOIbqEuyjT700Kh6SdCf1IINCU033vvPVt/IQDhdKQC/hDuvPNOe2zJkiWhUE2YMMFej28n4HR0xaOPPmrD0W5AfBbqokyzPy1BDWh+o+knR3+cCVSi0CzH9kgNcwrNONcWpl0JaHnOJjSjtPeTplFgOxX6or4kQhO9l7g5fnD4/gn79913n/fBBx/Yfcz4GMZhKBniamhosIah9kpgeJl++2iWAik46ijT7E8MhaafCP1JIdCU0MQQNdRtvDSCe/DBB63fLIfiLV261O7fc889oVDpRFz4nuvrr7+2oysw0VDqH863335bcNxRptmfGApNPxH6k0CgEoVmObZHaphTaCahVlV+HrU8ZxOaUdr7SdMosNmaRWhiFkbcHD/MCHeFmQ0WkwJhKQN9246P68O4efPm2XifffZZT7uo8b0DHAxU3BPGX6EuyjT700Kh6SdCf1IINCU0MTMz6i8mAtKRCviOAs6siWWP3XXXXaFQoQ1C3Hv27HGjK7APh3D8IEALdVGm2Z8WCk0/EfqTQKAShWY5tkdqmFNoJqFWVX4etTxnE5pR2vtJ0yiwnZpFaGKNFTXedEbGN954w5ZsnfYXa8KFcTr0FrMwYnkCGF865E17N8MsgxBlmv35pND0E6E/KQSaEpraNqBO44f6rWtfab1/7LHHQqHStmHBggW2bVq0aJGNJ7Xeo3ezUBdlmv1podD0E6E/CQQqUWiWY3ukhjmFZhJqVeXnUctzNqGZ+r9PjZJZFgrRKM0mNJFsXaMOicCskerWrVtnDb3U2R/1WJDtihUrnIhF3Jg5Up2+JUydsVKPBdlGlWb/vQt5iGqYh82T/970k0BzEtDynG15E+3F1JdUqX8QeLGE8NRZrAvJh84whzjQTuD7TzgMuUcY6n4YF2Wa/emh0PQToT8JBCpRaJZje6SGOYVmEmpV5edRy3OqHZGa66js/aRpFNhPzdKjiYc5btw4Z8BhzSh1ajDig9kwbseOHU5oPvLIIy4KNRiRaQzXDeOiSrM/LRSafiL0J4VAU0ITQ+tRf/HDiAddvghs9CUSJt8J49BWaNxoQ9Tpiy9MzhHGRZlmf3ooNP1E6E8CgUoUmuXYHqlhTqGZhFpV+XnU8pxLaEZl7ydNo8CuajahiYeLBGDhUh2ShnHR+hbh4MGDoUo64sL3nog79VtMDIVD2Jw5c0LFi4uiSrM/QRSafiL0J4VAU0ITDLBsEeoxPqhXh2H3CIPY1GHyeizoFsssIQ78dF1KtCU6hG3NmjVBo8o4L6o0+29EoeknQn8SCFSi0CzH9kgNcwrNJNSqys+jludcQjMqez9pGgU2VaFC8xwUPwngzKQ+YmZtlFGjRokReRlXnDx5UoYNGyYHDhyQ6upq6d+/v5jeAzFD5mTkyJFSX1+fcU3QANM1LeaDW2nXrp2MHTtWTpw4IaZxtJebyTmkT58+QaNKOy/KNG/dulXMzJn2fmbReTGiWzp16uTSatYVFSPC09IDz+TJk8XMuilmJl3p0aNHxnEGkECcCGh53rJli3Tv3j0j6WZWWBk+fLgNN8JKOnbsKKjvpudQzPeVYoRhxjVBA6ZNmyZoH7p162bbILMMkpgZsa1/w4YN0rZt26BRpZ0XZZrRZqDtgNu1a5flUFVVJV26dJEWLVqIWYtX2rRpkzU9udrmtJPpIYEyJ5DP1kDyw9STINk234iL+fTHntrY2Cio63ADBgywW7RVaKfCuHJrj8yEa1JXVyfmmzWpqakJkyVeQwJlQ0DL88KFC7PW0Sjt/SRpFOgSaBkzN07wZx/0PUeQt4zoOcASAtqTgC3e/p85cybobXKep28rNG4o6rDD6lJvElWaV69encZB061bY3inJsPtaw8Qv9F0SLgTYwJanrN9o6nZwtAT1GetGxgFgVmmi3WYAEDX89W4sfbuV199VWzUXlRpnj17tuOgaU7dGuM3I+06szfySkcCcScQxNYIU0+CcMHIq9T65t/H5GJhXbm1R2pTsUcz7BPldeVEQMtzrh5NpDUqex9x6/21zahUjYL8Rd6jqRIWvXFr165Vb9rWrE8nhw8fll69eknr1q3TjhXjMcPoBL2D6Nns2rVrMVFlXBtVmjNulCNg8ODBtvdXD7NHU0lwG2cC2qOpeViyZIkMGTJEvWlbY+SJmbRHevbsKS1btkw7VowHcZoXN7bN6NChQzFRZVwbVZozbpQlACNGamtr3RH2aDoU3IkxAe3R1Cw0ZWvoOXHaNnd7NH/+fDFLxDlk7NF0KLgTYwLao6lZMC+5Ze7cuepN20Zl71eqRjly5Igb1QGQhfZoBh46i6GfK1eudA+rc+fOMn78eOfnTnEEYICbSY1cJBMnTpT27ds7P3dIII4ETM++mO+TXNIx/J5Dwh2OonYwrG/Tpk0uDhjkuUS8O4k7JFDmBGhrRPuANm7cKHv37nU3GTp0qPTu3dv5uUMCcSRgRji6z06QftPrJgMHDoxjVsouzRDmDQ0NLl3QJtAoQV1goRk0Qp5HAiRAAiRAAiRAAiRAAiRAAiSQbAIUmsl+/sw9CZAACZAACZAACZAACZAACZScAIVmyZEyQhIgARIgARIgARIgARIgARJINgEKzWQ/f+aeBEiABEiABEiABEiABEiABEpOgEKz5EgZIQmQAAmQAAmQAAmQAAmQAAkkm0BgoYkZUd9//31HC4ud9+vXz/m5UxwBLH76ww8/uEjANuyC8i4S7pBAMxPYs2ePfPPNNy4VmN3w/PPPd37uhCdg1gOVTz75xEUArpw90uHgTkwJ0NaI9sF9+umngpl91VVVVckFF1ygXm5JIJYEsAQHljFTd+GFF9olFtXPbXgCZg1geeedd1wE5557rlRXVzt/vp3AQtO/thUap1dffTVf/DwekMAll1wix44dc2dzHU2HgjsxJuBfR3Px4sVy9dVXxzhH5ZP0F198UWbNmuUSxHU0HQruxJgAbY1oH95tt90mL730krsJ19F0KLgTYwL+dTRvvPFGueOOO2Kco/JJ+ueffy6DBg1yCYpsHU1t/HGzGTNmyG9/+1v5wx/+4G6sO0ePHpVnnnlGdu3aJb/5zW/S1l7Rc8Jsf/75Z/nXv/4l//jHP+TUqVPyt7/9Tf7617+GiSrtmjNnzsiDDz4omzdvFrzpg+CrqamR0aNHp50XxoN1fbA+5o4dO+R3v/ud/PnPfxY08liD1O8+/vhj+emnn+Tvf/+7oHeTQtNPiP44ElChiXrw+9//Xi666KKsPfXo+Xz66afl0KFD0rdvX1tPSpFfLI7+8ssvy4YNG8TzPKmrqyvJOp7oEXjggQfkjTfesCMR8HbvlltukT/96U9FJ3vVqlXywgsv2BEk6KG88sorZfr06dKqVau0uLG2Fd7iot1Cm0yhmYaHnpgSCGprRFW3gS2q9iho3S700RXSHqGN/e6772TNmjWybNkyodAslDbPL0cCKjRra2vlL3/5i7Wzs42eokb55ekF1SinT5+2dgauvOaaa6RQoQnjK5DbvXu3Z4xEzxg0Wc//4osvvDvvvNOeg/P0l/XkAgPXr1/vXXXVVS5OxG16RgqMJfN0I+y8SZMm2XjNYufemDFj3D1MA5x5QQEh27dvd3GZB+NdccUV1o+tGfKWMyZNjxkCkPMcHiCBuBDQ8rx///6sSUa7oudom3H99ddnPbeQQDP8znvyySc91GuNF1sz/L+QaLKea4y6tPo8YsQIdw/zkijrNUED0a5petEeafqnTJniob3K5oxhbq/J1TZnu4ZhJFCuBPLZGlHW7ajaI7AOU7eDPKOw7RFsHLQ1RvwGuQ3PIYGyJqDl2bykzZpOapR0LGE1CtqMiy++OD2yPL6SCU3Ti2kbLRhGDz30kDOW8tw/0GE1vGCAmu5wG3cphObrr79u47r00ku9xsZGm5adO3e6tJu3foHSl+0kiEukG1zgYCTecMMNNqy+vj7bJTZMjW4KzZyIeCBGBLQ85xKaEFCoJ6bXzjPDXOx+KYTmRx99ZONC3IhXX/SUQmiaHgAbt+nB9MxIC/s01Ii89tprQz+d48ePuzR/+OGHNh6EoVFHPrZt25Y1bgrNrFgYGFMC+YRmlHU7qvYobN0O8gjDtkdqmFNoBqHMc8qdgJbnXEKTGiX9CYbVKLBFmk1ootfRdF17ZlirZya1cQZTetbC+fCmXg0vFbGlEJpTp0616Xz++eddwlBIARK/5557zoUXsmOGwdrr8TDUEDXDfJyxi/BcvRNqmFNoFkKc55YrAS3PuYTmww8/7L3yyiu2Prz11lu23pRCaJpvCry7777bw1tMODMU3sZdrNA0Q+2d8EutozNnzrTxo93Yt29fqMexfPlyG8ett97qrjfDYl28ZkiQC0/dodBMpcH9uBPIJzSjqtvgFlV7FLZu53uWxbRHaphTaOajzONxIKDlOZfQpEb55SkWo1GaVWj+kgWv5EIzNe5SCk3tKfjss8/sLczsmG6oGmCaiTZSbx14Hw03rjffY7prNN0Ixw9/ltmcGuapRmy28xhGAnEgoOU5l9BMzUMphWZqvNgvldBEvUT9TX2j9+abbzoxiGPmm1D/7QP58UIN1+sf5dmzZ9OG86feMzVCCs1UGtyPO4F8QtOfv1LVbX+8pWyPwtZtf5r8/mLaIzXMKTT9VOmPIwEtz/r/2VQeSt0ZlnovtfVL0RlWjhrFb/+k5j3XfsmGzqbeIA4PET2KAIaf+dDVJl//DPBWE+EY6hrGPf744/Z6xANnliCw/ptuusnD0DrEjSG62Zwa5hSa2egwLG4EtDxXitA0H8/b+ovvJ+F0pAL+EPQbdTPxUajHNGHCBBs3vp2A09EVjz76qA1HuwHx6XcUmn4i9MeZQCUKzbB1O99zLKY9UsOcQjMfZR6PAwEtz5UiNMtVo8AOyfXSO1c5SazQRO8lgOEHh++fsH/fffd5H3zwgd0fOnRoLm5NhmPIHuJqaGiwhqFOMoRhfPqNqZlVNmscaphTaGbFw8CYEdDyXClCE8N8Ubfx0gjOzFht/WbWam/p0qV2/5577gn1lHTCM3yD9vXXX9vRFZhoKPUPx8w0mxE3hWYGEgbEmEAlCs2wdTvfYyymPVLDnEIzH2UejwMBLc+VIjTLVaPA/qHQDFgjMFMbgOGHWewwWQgmBcI3pmq4YYKSMG7evHk23meffdbO6IZ74BsNOBio8OMPIptTw5xCMxsdhsWNgJbnShGa+M4D9RcTAelIBXzrDYdv1HHsrrvuCvWYdMIis7SCnd0bcWEfDvv4QYD6nbZXGJFBRwJxJ1CJQjNs3c73LItpj9Qwp9DMR5nH40BAy3OlCM1y1SiwQyg0A9aIH3/80RlvOmubWRPPXq3T/l533XUBY0s/TYfeYrZLzMKLb0h0yJv2buZaBkENcwrNdKb0xZOAludKEZraNqBO44f6bdblsg9H6/1jjz0W6mFp27BgwQLbNi1atMjGk9pWZZtEjEIzFG5eVKYEKlFohq3b+R5RMe2RGuYUmvko83gcCGh5rhShmfq/X04ahUKzwNqga9QBHGaNVLdu3Tpr6KXO/qjHgmxXrFjhRCzixsyR6vTNZi4hqYZ5ruMaD7ckEAcCWp4rRWhqLybqNX6pf2q6PEvqLNaFPCOdBRvxop3A959wGHKPMLRX2RyFZjYqDIsrgUoUmmHrdr5nWEx7pIY5hWY+yjweBwJanlP/k3OlOw7zyCDt5ahRYIuwRzNXycoSPm7cOGfAYZ0rdWowYlKfMG7Hjh1OaD7yyCMuCjUY8aAwXDebU8OcQjMbHYbFjYCW50oRmhhaj/qLH0Y86PJFeC76EgkTdIRxaCs0brQh6vTFFyYUyeYoNLNRYVhcCVSi0Axbt/M9w2LaIzXMKTTzUebxOBDQ8lxJQrMcNQpsFArNAmoECiSgYeFSHZKGcdH6FuHgwYMFxPbLqYgL33si7tRvMTEUDmFz5sz55WTfnhrmFJo+MPTGkoCW50oRmngIWLYI9RiTfqnDsHuEQWzqMHk9FnSLZZYQB366/ifaEh12t2bNmqxRUWhmxcLAmBKoRKEZtm4HeYRh2yM1zCk0g1DmOeVOQMtzJQnNctQosE8KFZrnoPBIAGeMGTEzIMqoUaPECKasV0yfPl1Mz6AYQ0vMtwP2nAEDBthtr169ZP78+Vmvyxe4detWMTM62tPMYuhixKB06tRJ+vTpY8PMepdixGG+aDKOnzx5UoYNGyYHDhyQ6upq6d+/v5jeAzFGsYwcOVLq6+szrgkaYIbPipkUSNq1aydjx46VEydOiGnQ7eXmA36Xdn98kydPFjODpZhZaaVHjx7+w/STQKwIaHnesmWLdO/ePSPt5vtGMcPWbXhjY6OYGVftvrYbw4cPF/MtZMZ1QQLQZqDtgNu1a5eYt/9SVVUlXbp0kRYtWohZ50ratGkTJKq0c5BGpAsOaevYsaOgviN+832lGGGYdn4hnmnTpgnah27dutk2yCyDJGZGbOvfsGGDtG3bNiM6TU9TbXPGRQwggTIlEMTWiKpuR9kehanbQR6R1n+cW0h7ZCYvk7q6OjHff0lNTU2QW/EcEihbAlqeFy5cmNNmoEb55fGF1SjQJdBfZp6ZXyLLs1dSoXnZZZdZEZjtnhCCa9euzXYob5h5ky9mRsWc55k3GTJo0KCcx5s6ANE6ZcoUMW9R3WlorO+9915p1aqVCwuzowVfr8XDeeKJJ6Rfv34alLFVw5xCMwMNA2JIQMtzLqF56NAhufzyy3Pm7Oabb5bZs2fnPN7Ugdtvv9293Ml2HsTneeedl+1Q3rC3335bzHdXcuzYMXsuXijhhZcZ3pr32qZOMGv6ihnxIKtXr3an9e3b14rizp07u7DUHTU0KTRTqXA/rgSCCM2o6naU7VGYuh30GYZpj9Q+odAMSpnnlTMBLc9NCU1qlPQnqMw0NIhG+VWEpiaoGOGocZTT1qxPJ4cPHxb0vLZu3bpkSUPvLnphYYh27do1Z7yDBw+2Pal6AoWmkuA2zgRUaGoelixZIkOGDFFv7LdmeL2YSXukZ8+e0rJly5LlB3Ga4fO2zejQoUPWeDH6ora21h2j0HQouBNjAio0NQuVZmsEqdua90K3QdojjCwzy625qCk0HQruxJiAXzSZz3Zk7ty5Mc5RetKbU6McOXJEdJQZUhVZjyZ6/lauXOlyjrfr48ePd37uFEcABriZIMhFMnHiRGnfvr3zc4cE4kgAPXPm+ySXdAy/55Bwh6OoHfRkbtq0ycUBg7ySRLzLGHcSRYC2RrSPe+PGjbJ37153k6FDh0rv3r2dnzskEEcCZhI+96kM0m++I5SBAwfGMStll2aI3IaGBpcuaBNolKAu8NDZoBHyPBIgARIgARIgARIgARIgARIggWQToNBM9vNn7kmABEiABEiABEiABEiABEig5AQoNEuOlBGSAAmQAAmQAAmQAAmQAAmQQLIJUGgm+/kz9yRAAiRAAiRAAiRAAiRAAiRQcgIUmiVHyghJgARIgARIgARIgARIgARIINkEKDST/fyZexIgARIgARIgARIgARIgARIoOQEKzZIjZYQkQAIkQAIkQAIkQAIkQAIkkGwCFJrJfv7MPQmQAAmQAAmQAAmQAAmQAAmUnACFZsmRMkISIAESIAESIAESIAESIAESSDYBCs1kP3/mngRIgARIgARIgARIgARIgARKToBCs+RIGSEJkAAJkAAJkAAJkAAJkAAJJJsAhWaynz9zTwIkQAIkQAIkQAIkQAIkQAIlJ0ChWXKkjJAESIAESIAESIAESIAESIAEkk2AQjPZz5+5JwESIAESIAESIAESIAESIIGSE6DQLDlSRkgCJEACJEACJEACJEACJEACySbwfxnCsuqv9IH1AAAAAElFTkSuQmCC\"\n\n# HTML to embed the image\nhtml_code = f'&lt;img src=\"{image_base64}\" style=\"width:800px; height:auto;\"/&gt;'\n\n# Display the image\ndisplay(HTML(html_code))\n\n\n# Example usage\nquestion = \"Which of the following is the correct adjacency matrix for the graph?\"\ncorrect_answer = \"A\"\ndecoy_answers = [\"B\", \"C\", \"D\"]\n\nexplanation = \"\"\"\n&lt;p&gt;The adjacency matrix &lt;em&gt;A&lt;/em&gt; for a directed graph is defined such that &lt;em&gt;A&lt;sub&gt;ij&lt;/sub&gt; = 1&lt;/em&gt; if there is a directed edge from node &lt;em&gt;i&lt;/em&gt; to node &lt;em&gt;j&lt;/em&gt;, and &lt;em&gt;A&lt;sub&gt;ij&lt;/sub&gt; = 0&lt;/em&gt; otherwise.&lt;/p&gt;\n\n&lt;ol&gt;\n    &lt;li&gt;&lt;strong&gt;Node 0:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 0 to nodes 1 and 2, so &lt;em&gt;A&lt;sub&gt;01&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;02&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 0 to node 3, so &lt;em&gt;A&lt;sub&gt;03&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 1:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 1 to nodes 2 and 3, so &lt;em&gt;A&lt;sub&gt;12&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;13&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 1 to node 0, so &lt;em&gt;A&lt;sub&gt;10&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 2:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There is an edge from node 2 to node 3, so &lt;em&gt;A&lt;sub&gt;23&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 2 to nodes 0 and 1, so &lt;em&gt;A&lt;sub&gt;20&lt;/sub&gt; = 0&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;21&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n    &lt;li&gt;&lt;strong&gt;Node 3:&lt;/strong&gt;\n        &lt;ul&gt;\n            &lt;li&gt;There are edges from node 3 to nodes 0 and 1, so &lt;em&gt;A&lt;sub&gt;30&lt;/sub&gt; = 1&lt;/em&gt; and &lt;em&gt;A&lt;sub&gt;31&lt;/sub&gt; = 1&lt;/em&gt;.&lt;/li&gt;\n            &lt;li&gt;There are no edges from node 3 to node 2, so &lt;em&gt;A&lt;sub&gt;32&lt;/sub&gt; = 0&lt;/em&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/li&gt;\n&lt;/ol&gt;\n\n&lt;p&gt;Thus, the correct adjacency matrix is choice A:&lt;/p&gt;\n\"\"\"\n\n# Base64 encoded image string (replace this with your actual base64 string)\nimage_base64 = \"&lt;base64_string&gt;\"\n\n# Call the function to create and display the quiz with the embedded image\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n\n\n\n    \n      Which of the following is the correct adjacency matrix for the graph?\n    \n        \n          \n          A\n        \n        \n        \n          \n          B\n        \n        \n        \n          \n          C\n        \n        \n        \n          \n          D\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \nThe adjacency matrix A for a directed graph is defined such that Aij = 1 if there is a directed edge from node i to node j, and Aij = 0 otherwise.\n\n\n    Node 0:\n        \n            There are edges from node 0 to nodes 1 and 2, so A01 = 1 and A02 = 1.\n            There are no edges from node 0 to node 3, so A03 = 0.\n        \n    \n    Node 1:\n        \n            There are edges from node 1 to nodes 2 and 3, so A12 = 1 and A13 = 1.\n            There are no edges from node 1 to node 0, so A10 = 0.\n        \n    \n    Node 2:\n        \n            There is an edge from node 2 to node 3, so A23 = 1.\n            There are no edges from node 2 to nodes 0 and 1, so A20 = 0 and A21 = 0.\n        \n    \n    Node 3:\n        \n            There are edges from node 3 to nodes 0 and 1, so A30 = 1 and A31 = 1.\n            There are no edges from node 3 to node 2, so A32 = 0.\n        \n    \n\n\nThus, the correct adjacency matrix is choice A:"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#그래프-이론-방향성-및-무방향성-그래프",
    "href": "notebooks/WS07_GNNsForProteins.html#그래프-이론-방향성-및-무방향성-그래프",
    "title": "섹션 1: 그래프 이론",
    "section": "그래프 이론: 방향성 및 무방향성 그래프",
    "text": "그래프 이론: 방향성 및 무방향성 그래프\n그래프의 엣지는 노드 간에 방향성 의존성이 존재하는지 여부에 따라 방향성(directed) 또는 무방향성(undirected)일 수 있습니다. 방향성 그래프에서 엣지에는 소스 노드와 목적지 노드가 있습니다. 무방향성 그래프에서 정보는 양방향으로 흐르므로 대칭성을 가질 수 있습니다. 즉, 무방향성 그래프에서 A[i][j] = A[j][i]입니다. 다시 말해, n0에서 n1 사이의 무방향성 엣지를 갖는 것은 n0에서 n1로, n1에서 n0로의 두 개의 방향성 엣지를 갖는 것과 같습니다.\n아래는 화살표로 방향을 나타내는 각각의 예입니다.\n\n\nCode\n# @title The graph on the left is directed, with 4 nodes and 5 edges. Note that there is an edge from node 0 to 1, but not from node 1 to 0. The graph on the right is undirected.\n\n\n# Create a directed graph (using DiGraph) with 4 nodes and 5 edges\nG_directed = nx.DiGraph()\nG_directed.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)])\n\n# Create an undirected graph with the same 4 nodes but undirected edges\nG_undirected = nx.Graph()\nG_undirected.add_edges_from([(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)])\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\n\n# Plot directed graph on the left\npos = nx.spring_layout(G_directed)\nnx.draw(G_directed, pos, ax=axes[0], with_labels=True, node_color='lightblue', node_size=700, arrows=True)\naxes[0].set_title(\"Directed Graph\")\n\n# Print directed adjacency matrix\nA_directed = nx.adjacency_matrix(G_directed).todense()\ndirected_matrix = (f'Adjacency matrix:\\n{A_directed}\\n')\n# Adjust the ha parameter to 'left' for left alignment\naxes[0].text(0.05, -0.2, directed_matrix, ha='left', va='center', transform=axes[0].transAxes, fontsize=10)\n\n# Plot undirected graph on the right\nnx.draw(G_undirected, pos, ax=axes[1], with_labels=True, node_color='lightgreen', node_size=700, arrows=True)\naxes[1].set_title(\"Undirected Graph\")\n\n# Print undirected adjacency matrix\nA_undirected = nx.adjacency_matrix(G_undirected).todense()\nundirected_matrix = (f'Adjacency matrix (symmetric):\\n{A_undirected}\\n')\n# Adjust the ha parameter to 'left' for left alignment\naxes[1].text(0.05, -0.2, undirected_matrix, ha='left', va='center', transform=axes[1].transAxes, fontsize=10)\n\n# Subplots so they plot side by side\nplt.subplots_adjust(bottom=0.3)\n\n# Show the plots\nplt.show()\n\n\n\n\n\n\n\n\n\n\n작업: 방향성 그래프(G_directed)의 엣지를 방향성에서 무방향성으로 변경해 보세요\n힌트: NetworkX에는 그래프에 엣지를 추가하는 내장 함수가 있습니다. add_edge를 사용해 보세요.\n\n\n도움말 확인하려면 클릭\n\n역방향 엣지 1 -&gt; 0을 추가하여 0 -&gt; 1 엣지를 무방향성으로 만듭니다.\nG_directed.add_edge(1, 0)\n\n\nCode\n# Before updating edge\nprint(\"Original edges:\", list(G_directed.edges))\n\n# YOUR CODE HERE\n# raise NotImplementedError()\n\n# Check after updating edge\nprint(\"Updated edges:\", list(G_directed.edges))\nnx.draw(G_directed, with_labels=True, node_color='lightblue', node_size=400, arrows=True)\n\n\nOriginal edges: [(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)]\nUpdated edges: [(0, 1), (0, 2), (1, 2), (2, 3), (3, 1)]\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How many edges did the graph have before? How many are there now?\"\ncorrect_answer = \"There were 5 edges before, and now there are 6.\"\ndecoy_answers = [\n    \"There were 5 edges before, and now there are 5.\",\n    \"There were 5 edges before, and now there are 7.\",\n    \"There were 5 edges before, and now there are 4.\"\n]\n\nexplanation = \"Having an undirected edge between n0 to n1 is the same as having two directed edges from n0 to n1 and n1 to n0. When the directed edge was replaced with an undirected edge, it added a reverse edge between the same two nodes.\"\n\n\n# Create the quiz\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How many edges did the graph have before? How many are there now?\n    \n        \n          \n          There were 5 edges before, and now there are 6.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 5.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 7.\n        \n        \n        \n          \n          There were 5 edges before, and now there are 4.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Having an undirected edge between n0 to n1 is the same as having two directed edges from n0 to n1 and n1 to n0. When the directed edge was replaced with an undirected edge, it added a reverse edge between the same two nodes."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#그래프-이론-특징-임베딩으로-노드와-엣지-추가-정의",
    "href": "notebooks/WS07_GNNsForProteins.html#그래프-이론-특징-임베딩으로-노드와-엣지-추가-정의",
    "title": "섹션 1: 그래프 이론",
    "section": "그래프 이론: 특징 임베딩으로 노드와 엣지 추가 정의",
    "text": "그래프 이론: 특징 임베딩으로 노드와 엣지 추가 정의\n노드, 특징 또는 그래프 임베딩을 추가하여 그래프를 추가로 정의할 수 있습니다. graphein 패키지를 사용하여 단백질 엣지 및 노드 특징을 추가하고 대화형 그래프로 시각화할 수 있습니다.\n아래는 정의된 매개변수 세트입니다. 이를 사용하여 단백질 그래프를 구축하고 추가되는 노드 및 엣지 특징을 시각화할 수 있습니다.\nGraphein의 ProteinGraphConfigs는 노드, 엣지 및 전체 그래프를 구성하는 방법을 정의하기 위해 구성 함수 리스트의 사전을 받습니다.\n그래프 및 엣지 메타데이터 함수는 nx.Graph 객체를 입력으로 받아 노드 특징이 추가된 동일한 nx.Graph 객체를 반환합니다. 노드 메타데이터 함수는 G.nodes(data=True)에서 노드, 데이터 튜플을 받아 pd.Series를 반환합니다.\n\n\nCode\n# @title graphein graph\n\nfrom IPython.display import display\nfrom graphein.protein.graphs import construct_graph\nfrom graphein.protein.visualisation import plotly_protein_structure_graph\nfrom functools import partial\n\nparams_1 = {\n    \"granularity\": \"CA\",\n    \"edge_construction_functions\": [\n        gp.add_peptide_bonds,\n    ],\n    \"node_metadata_functions\": [\n        gp.amino_acid_one_hot,\n        gp.meiler_embedding,\n        partial(gp.expasy_protein_scale, add_separate=True)\n    ],\n}\n\nconfig_1 = gp.ProteinGraphConfig(**params_1)\n\nprotein_graph_1 = construct_graph(config=config_1, pdb_code='1L2Y')\n\nplot_1 = plotly_protein_structure_graph(\n    protein_graph_1,\n    colour_edges_by=\"kind\",\n    colour_nodes_by=\"degree\",\n    label_node_ids=False,\n    plot_title=\"\",\n    node_size_multiplier=1\n    )\n\ndef convert_to_networkx_and_print_info(graph):\n    # Convert to NetworkX graph\n    nx_graph = nx.Graph(graph)\n\n    # Number of nodes and edges\n    n_node = nx_graph.number_of_nodes()\n    n_edge = nx_graph.number_of_edges()\n\n    # Node and edge features (assuming attributes are stored as dicts)\n    node_features = nx.get_node_attributes(nx_graph, \"features\")  # Replace \"features\" with the correct attribute if needed\n    edge_features = nx.get_edge_attributes(nx_graph, \"features\")  # Replace \"features\" with the correct attribute if needed\n\n    # Edges\n    senders = list(nx_graph.edges())\n    receivers = [(v, u) for u, v in senders]  # For undirected graphs, this is the reverse\n\n    # Global features (if applicable)\n    global_context = nx_graph.graph.get(\"globals\", None)\n\n    # Create the information string\n    info = (\n        f\"Number of nodes: {n_node}\\n\"\n        f\"Number of edges: {n_edge}\\n\"\n        # f\"Node features: {node_features}\\n\"\n        # f\"Edge features: {edge_features}\\n\"\n        f\"Edges (senders): {senders}\\n\"\n        f\"Edges (receivers): {receivers}\\n\"\n        # f\"Graph-level features (globals): {global_context}\"\n    )\n\n    return info\n\n  # Convert to NetworkX and gather info strings\ninfo_1 = convert_to_networkx_and_print_info(protein_graph_1)\n\nprint(\"Graph 1 Information:\")\nprint(info_1)\n\n\n\n\n\n\n\n\nGraph 1 Information:\nNumber of nodes: 20\nNumber of edges: 19\nEdges (senders): [('A:ASN:1', 'A:LEU:2'), ('A:LEU:2', 'A:TYR:3'), ('A:TYR:3', 'A:ILE:4'), ('A:ILE:4', 'A:GLN:5'), ('A:GLN:5', 'A:TRP:6'), ('A:TRP:6', 'A:LEU:7'), ('A:LEU:7', 'A:LYS:8'), ('A:LYS:8', 'A:ASP:9'), ('A:ASP:9', 'A:GLY:10'), ('A:GLY:10', 'A:GLY:11'), ('A:GLY:11', 'A:PRO:12'), ('A:PRO:12', 'A:SER:13'), ('A:SER:13', 'A:SER:14'), ('A:SER:14', 'A:GLY:15'), ('A:GLY:15', 'A:ARG:16'), ('A:ARG:16', 'A:PRO:17'), ('A:PRO:17', 'A:PRO:18'), ('A:PRO:18', 'A:PRO:19'), ('A:PRO:19', 'A:SER:20')]\nEdges (receivers): [('A:LEU:2', 'A:ASN:1'), ('A:TYR:3', 'A:LEU:2'), ('A:ILE:4', 'A:TYR:3'), ('A:GLN:5', 'A:ILE:4'), ('A:TRP:6', 'A:GLN:5'), ('A:LEU:7', 'A:TRP:6'), ('A:LYS:8', 'A:LEU:7'), ('A:ASP:9', 'A:LYS:8'), ('A:GLY:10', 'A:ASP:9'), ('A:GLY:11', 'A:GLY:10'), ('A:PRO:12', 'A:GLY:11'), ('A:SER:13', 'A:PRO:12'), ('A:SER:14', 'A:SER:13'), ('A:GLY:15', 'A:SER:14'), ('A:ARG:16', 'A:GLY:15'), ('A:PRO:17', 'A:ARG:16'), ('A:PRO:18', 'A:PRO:17'), ('A:PRO:19', 'A:PRO:18'), ('A:SER:20', 'A:PRO:19')]\n\n\n\n아래 단백질 그래프의 노드와 엣지 위로 마우스를 가져가면 특징 임베딩을 볼 수 있습니다.\n\n\nCode\ndisplay(plot_1)\n\n\n\n\n\n                                \n                                            \n\n\n\n\nGNN에 입력하기 위해 노드와 엣지의 임베딩된 특징을 특징 행렬로 변환하는 것이 일반적입니다. 각 노드 또는 엣지의 특징 벡터는 그 특성을 수치로 표현한 것입니다. 숫자 벡터이지만 이 숫자들은 단백질 그래프의 원자 속성과 같은 의미 있는 정보를 인코딩합니다. GNN은 이러한 특징 벡터에서 특징을 학습합니다."
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gnn-이론-메시지-전달",
    "href": "notebooks/WS07_GNNsForProteins.html#gnn-이론-메시지-전달",
    "title": "섹션 1: 그래프 이론",
    "section": "GNN 이론: 메시지 전달",
    "text": "GNN 이론: 메시지 전달\n\n 메시지 전달\n그래프 합성곱 네트워크(GCN)에 대한 간단한 메시지 전달 방정식은 다음과 같이 표현할 수 있습니다:"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-구현-입력-데이터-로드-및-준비",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-구현-입력-데이터-로드-및-준비",
    "title": "섹션 1: 그래프 이론",
    "section": "GCN 구현: 입력 데이터 로드 및 준비",
    "text": "GCN 구현: 입력 데이터 로드 및 준비\n우리는 Amemiya et al. (2011)의 PSCDB 데이터셋을 사용할 것입니다. 이 데이터셋에는 결합 및 비결합 형태의 839개 단백질 구조 쌍의 pdb 파일이 포함되어 있으며 7가지 구조적 재배열 동작 클래스(결합 도메인 동작, 독립 도메인 동작, 결합 로컬 동작, 독립 로컬 동작, 리간드 매립 동작, 유의미한 동작 없음, 기타 유형 동작)에 걸쳐 있습니다.\n이 데이터셋은 다음 동작 유형의 비결합 pdb 파일만 포함하도록 필터링되었습니다: 독립 도메인 동작, 독립 로컬 동작, 유의미한 동작 없음.\n\n\n\nScreenshot 2024-09-17 at 12.33.12 PM.png\n\n\nGCN의 목표는 단백질 구조에서 결합 유형을 예측하는 분류 모델을 만드는 것입니다.\n이미지는 다음 기사에서 각색됨: PSCDB: 리간드 결합 시 단백질 구조 변화를 위한 데이터베이스\n\n\nCode\n# Get the .pdb files\n!git clone https://github.com/courtel/GNN_dl_workshop\n\n# Load the DataFrame from the CSV file that contains motion info\ndf_pscdb = pd.read_csv('/GNN_dl_workshop/df_pscdb.csv')\n\n# Display the DataFrame to confirm it's loaded correctly\nprint(df_pscdb.head())\n\n\nfatal: destination path 'GNN_dl_workshop' already exists and is not an empty directory.\n   Unnamed: 0  level_0  index PSCID  \\\n0           1        1      1  CD.2   \n1           4        4      4  CD.5   \n2           5        5      5  CD.6   \n3           6        6      6  CD.7   \n4           8        8      8  CD.9   \n\n                                   Protein Name Free form Bound form  Ligands  \\\n0                              ADENYLATE KINASE    4ake_A     2eck_A  ADP,AMP   \n1                           ELONGATION FACTOR 2    1n0v_D     1n0u_A      SO1   \n2                              CITRATE SYNTHASE   5csc_AB    6cts_AA    2xCIC   \n3                  ARACHIDONATE 15-LIPOXYGENASE    2p0m_A     2p0m_B      RS7   \n4  3-PHOSPHOSHIKIMATE 1-CARBOXYVINYLTRANSFERASE    2bjb_A     2o0d_A      S3P   \n\n   Classification(?)            motion_type free_pdb Free Chains Bound PDB  \\\n0             200003  coupled_domain_motion     4ake           A      2eck   \n1             110002  coupled_domain_motion     1n0v           D      1n0u   \n2             102001  coupled_domain_motion     5csc          AB      6cts   \n3             101201  coupled_domain_motion     2p0m           A      2p0m   \n4             101101  coupled_domain_motion     2bjb           A      2o0d   \n\n  Bound Chains  \n0            A  \n1            A  \n2           AA  \n3            B  \n4            A  \n\n\n\n\nCode\n# @title **Data Prep Functions**\n\n# Define a function to map motion class\nmotion_type_mapping = {row['free_pdb']: row['motion_type'] for _, row in df_pscdb.iterrows()}\n# print(\"motion_type_mapping\", motion_type_mapping)\n\n# Convert motion types to numerical labels\nmotion_type_to_label = {motion_type: idx for idx, motion_type in enumerate(df_pscdb['motion_type'].unique())}\n\ndef classification_mapping(df_pscdb, data):\n    protein_name = data.name\n    motion_type = motion_type_mapping[protein_name]\n    # print(\"motion type\", motion_type)\n    graph_label = motion_type_to_label[motion_type]\n    # print(\"graph label\", graph_label)\n    # data.graph_y = torch.tensor([graph_label])\n    data.graph_y = graph_label\n    # print(\"graph_y\", data.graph_y)\n    return data\n\n# Define a function to parse pdb and convert to networkx then to pytorch object\ndef get_distance_matrix(coords):\n    diff_tensor = np.expand_dims(coords, axis=1) - np.expand_dims(coords, axis=0)\n    distance_matrix = np.sqrt(np.sum(np.power(diff_tensor, 2), axis=-1))\n    return distance_matrix\n\ndef pdb_to_graph(pdb_path, distance_threshold=6.0, contain_b_factor=True):\n    atom_df = PandasPdb().read_pdb(pdb_path)\n    atom_df = atom_df.df['ATOM']\n    residue_df = atom_df.groupby('residue_number', as_index=False)[['x_coord', 'y_coord', 'z_coord', 'b_factor']].mean().sort_values('residue_number')\n    coords = residue_df[['x_coord', 'y_coord', 'z_coord']].values\n    distance_matrix = get_distance_matrix(coords)\n    adj = distance_matrix &lt; distance_threshold\n    u, v = np.nonzero(adj)\n    u, v = torch.from_numpy(u), torch.from_numpy(v)\n    graph = nx.Graph()\n\n    # Add nodes with coordinates and b_factor included in the node feature matrix 'x'\n    for i, row in residue_df.iterrows():\n        node_features = [row['x_coord'], row['y_coord'], row['z_coord']]\n        if contain_b_factor:\n            node_features.append(row['b_factor'])  # Append b_factor to the features\n        graph.add_node(i, x=torch.tensor(node_features, dtype=torch.float))\n\n    # Add edges based on the adjacency matrix\n    for src, dst in zip(u.numpy(), v.numpy()):\n        distance = distance_matrix[src, dst]\n        graph.add_edge(src, dst, edge_attr=torch.tensor([distance], dtype=torch.float))\n\n    return graph\n\n\ndef get_pmolecule(file_path, distance_threshold=6.0, contain_b_factor=True, train=True):\n    # Create a graph using pdb_to_graph\n    graph = pdb_to_graph(file_path, distance_threshold, contain_b_factor)\n\n    # Convert to PyTorch data object\n    data = from_networkx(graph)\n\n    # Set the name attribute manually\n    data.name = os.path.basename(file_path).replace('.pdb', '')\n\n    # Check that there are feats\n    if 'x' not in data:\n        print(\"warning, no x node features\")\n\n    if 'edge_attr' not in data:\n        print(\"warning, no edge attributes\")\n\n    # Add labels for motion_type\n    data = classification_mapping(df_pscdb, data)\n\n    return data\n\ndef get_pyg_data_list(pdb_directory, pdb_id=None, distance_threshold=6.0, contain_b_factor=True, train=True):\n    pdb_files = [f for f in os.listdir(pdb_directory) if f.endswith('.pdb')]\n    pyg_data_list = []\n\n    if train:\n        for pdb_file in pdb_files:\n            file_path = os.path.join(pdb_directory, pdb_file)\n            graph = get_pmolecule(file_path, distance_threshold, contain_b_factor)\n            pyg_data_list.append(graph)\n        return pyg_data_list\n    else:\n        file_path = os.path.join(pdb_directory, f'{pdb_id}.pdb')\n        pyg_data = get_pmolecule(file_path, distance_threshold, contain_b_factor, train=False)\n        return pyg_data\n\n\nGCN의 입력으로 .pdb 파일을 준비하는 것은 원래 단백질을 나타내는 노드와 엣지의 그래프를 생성하기 위해 정보를 추출하는 것을 포함합니다. 그런 다음 데이터는 특징이 텐서로 표현되는 pytorch 데이터 객체로 변환됩니다.\n아래는 위의 함수를 사용한 데이터 준비 워크플로의 간단한 3가지 잔기 단백질을 사용한 예입니다.\n\n\n\nmatrix_flow.jpg\n\n\n\n\nCode\n# Create directories to store train and test sets\ntrain_pdb_dir = '/GNN_dl_workshop/pdb_files/train_set'\ntest_pdb_dir = '/GNN_dl_workshop/pdb_files/test_set'\n\n# Run code to put our .pdb files through the data prep workflow\ntrain_pyg_data_list = get_pyg_data_list(train_pdb_dir)\ntest_pyg_data_list = get_pyg_data_list(test_pdb_dir)\n\n\nGCN의 최종 입력인 pytorch 데이터 객체에 우리가 예상하는 데이터가 포함되어 있는지 다시 확인합니다.\n\n\nCode\n# Double check pyg_data\nfor i, pyg_data in enumerate(train_pyg_data_list):\n    if i &gt;= 5:\n        break\n    print(pyg_data)\n\n\nData(x=[527, 4], edge_index=[2, 2749], edge_attr=[2749, 1], name='2o1p', graph_y=1)\nData(x=[282, 4], edge_index=[2, 24478], edge_attr=[24478, 1], name='1rif', graph_y=1)\nData(x=[421, 4], edge_index=[2, 36621], edge_attr=[36621, 1], name='1sry', graph_y=3)\nData(x=[233, 4], edge_index=[2, 1417], edge_attr=[1417, 1], name='2z42', graph_y=5)\nData(x=[469, 4], edge_index=[2, 2547], edge_attr=[2547, 1], name='1bbw', graph_y=1)\n\n\n여기서 첫 번째 단백질에는 4개의 속성(X, Y, Z 좌표 및 b_factor)을 가진 217개의 노드(잔기)가 포함된 특징 행렬(x)이 있음을 알 수 있습니다. Edge_index는 노드 간의 1133개 연결(엣지)이 있는 행렬입니다. 이것이 인접 행렬입니다. Edge_attr은 거리 행렬이며 1133개 엣지 모두에 대해 1개의 속성(거리)을 갖습니다. Graph_y는 동작 유형 분류에 매핑되는 숫자입니다.\n\n\nCode\n# @title Plot graphein graph of the first protein in the dataset, 2nrt. Here we can confirm that our pytorch data objecthas the correct number of nodes. If you hover over each node in the graph, you will see X, Y, Z coords. We should have something similar in the each of the matrices for the 217 nodes in our feature matrix.\nparams_1 = {\n    \"edge_construction_functions\": [\n        gp.add_peptide_bonds\n    ]\n}\n\n# Define a graphein graph config\nconfig = ProteinGraphConfig(granularity=\"CA\", **params_1)\n# Construct the protein graph\nprotein_graph = construct_graph(config=config, pdb_code='2nrt')\n\ng = construct_graph(config=config, pdb_code='2nrt')\np = plotly_protein_structure_graph(g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\n\nCode\n# @title Functions for model metrics (show code to see detail)\ndef log_metrics(epoch, optimizer_step, train_loss, train_accuracy, log_file='train_metrics_log.txt'):\n    with open(log_file, 'a') as f:\n        f.write(f'epoch: {epoch}, optimizer_step: {optimizer_step}, train_loss: {train_loss}, train_accuracy: {train_accuracy}\\n')\n\ndef get_results(log_file, number=4):\n    with open(log_file, 'r') as file:\n        lines = file.readlines()\n    results = []\n    for line in lines:\n        if line.startswith('epoch'):\n            result_1 = [_.start() for _ in re.finditer(':', line)]\n            result_2 = [_.start() for _ in re.finditer(',', line)] + [-1]\n            bla = []\n            for i in range(number):\n                if i == number-1:\n                    bla.append(float(line[result_1[i]+2:]))\n                else:\n                    bla.append(float(line[result_1[i]+2:result_2[i]]))\n            results.append(bla)\n    results = np.array(results)\n    return results\n\ndef plot_metrics(log_file='train_metrics_log.txt'):\n    metrics = get_results(log_file)\n    epochs = metrics[:, 0]\n    train_loss = metrics[:, 2]\n    train_accuracy = metrics[:, 3]\n\n    plt.figure(figsize=(20, 8))\n\n    # Plot Loss vs Epochs\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, '--', linewidth=2.0, c='C0')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.grid(True)\n    plt.legend(['Training Loss'])\n\n    # Plot Accuracy vs Epochs\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracy, linewidth=4.0, c='C1')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.ylim(0.0, 1.0)  # Set the y-axis scale from 0.0 to 1.0\n    plt.grid(True)\n    plt.legend(['Training Accuracy'])\n\n    plt.show()"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-구현-graphconv-레이어-정의",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-구현-graphconv-레이어-정의",
    "title": "섹션 1: 그래프 이론",
    "section": "GCN 구현: GraphConv 레이어 정의",
    "text": "GCN 구현: GraphConv 레이어 정의"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-아키텍처",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-아키텍처",
    "title": "섹션 1: 그래프 이론",
    "section": "GCN 아키텍처",
    "text": "GCN 아키텍처\n\n1. 입력 레이어\n\n노드 특징 행렬 (X): 이 행렬은 n x D 차원을 가지며, 여기서 n은 그래프의 노드 수이고 D는 노드당 특징 수입니다. 각 행은 노드의 특징 벡터를 나타냅니다.\n인접 행렬 (A): 이것은 n x n 행렬로, 각 요소 A[i][j]는 노드 i와 j 사이에 엣지가 있으면 1이고 그렇지 않으면 0입니다. 이 행렬은 그래프 구조를 포착합니다.\n\n\n\n2. 그래프 합성곱 레이어 (GCN)\n\n노드 집계 및 업데이트: 인접 행렬을 기반으로 노드의 가장 가까운 이웃에 대한 메시지 전달 및 집계가 수행되어 모든 노드가 이웃의 특징을 기반으로 업데이트됩니다. 이러한 특징은 각 레이어에 특정한 가중치에 의해 변환됩니다.\n비선형성(Non-linearity): ReLU와 같은 비선형 함수가 출력에 적용됩니다.\n\n\n\n3. 완전 연결 레이어 (Fully Connected Layer)\n\n최종 노드 특징은 GCN 레이어에서 사용된 특징을 특정 작업(예: 분류, 회귀)에 필요한 출력 차원으로 매핑하는 완전 연결 레이어를 통과합니다.\n\n\n\n4. 풀링 레이어 (작업에 따라 다름)\n\n필요한 특정 작업에 따라 풀링 레이어를 적용할 수 있습니다(예: 그래프 수준 작업).\n\n\n\n5. 출력 레이어\n\n출력: 출력 레이어는 길이 n x 1의 벡터를 생성하며, 각 요소는 각 노드에 대한 최종 출력에 해당합니다. 이는 GCN의 특정 응용 프로그램에 따라 점수, 분류 레이블 또는 기타 노드별 출력을 나타낼 수 있습니다.\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\ndef create_checkbox_quiz(question, correct_answers, decoy_answers, explanation):\n    html_code = f\"&lt;h3&gt;{question}&lt;/h3&gt;\"\n\n    # Create checkboxes for the correct answers and decoy answers\n    answers = correct_answers + decoy_answers\n    for i, answer in enumerate(answers):\n        html_code += f\"&lt;p&gt;&lt;input type='checkbox' id='checkbox_{i}' value='{answer}'&gt; {answer}&lt;/p&gt;\"\n\n    # Add submit button, feedback, and explanation\n    html_code += f\"\"\"\n    &lt;button onclick=\"checkCheckboxAnswers()\"&gt;Submit&lt;/button&gt;\n    &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n    &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n      &lt;h4&gt;Explanation:&lt;/h4&gt;\n      &lt;p&gt;{explanation}&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkCheckboxAnswers() {{\n        var correct_answers = {correct_answers};\n        var selected_answers = [];\n        var checkboxes = document.querySelectorAll(\"input[type='checkbox']\");\n\n        checkboxes.forEach((checkbox, i) =&gt; {{\n            if (checkbox.checked) {{\n                selected_answers.push(checkbox.value);\n            }}\n        }});\n\n        var feedback = document.getElementById('feedback');\n        var all_correct = selected_answers.length === correct_answers.length &&\n                          correct_answers.every(answer =&gt; selected_answers.includes(answer));\n\n        if (all_correct) {{\n            feedback.textContent = \"Correct!\";\n            feedback.style.color = \"green\";\n        }} else {{\n            feedback.textContent = \"Incorrect. Try again.\";\n            feedback.style.color = \"red\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(html_code))\n\n# Define the question, correct answers, decoy answers, and explanation\nquestion = \"Which components stored in our pytorch data objects will get updated after message passing?\"\ncorrect_answers = [\"x\", \"edge_attr\"]\ndecoy_answers = [\"edge_index\", \"name\", \"graph_y\"]\n\nexplanation = \"The 'x' is the node feature matrix and 'edge_attr' is the distance matrix which is our edge feature matrix. &lt;br&gt; Edge_index is our adjacency matrix which is used in message passing, but this matrix will stay the same. &lt;br&gt; The name (name of the protein) and graph_y (the motion label) will not change during message passing.\"\n\n# Create the quiz\ncreate_checkbox_quiz(question, correct_answers, decoy_answers, explanation)\n\n\nWhich components stored in our pytorch data objects will get updated after message passing? x edge_attr edge_index name graph_y\n    Submit\n    Show Explanation\n    \n    \n      Explanation:\n      The 'x' is the node feature matrix and 'edge_attr' is the distance matrix which is our edge feature matrix.  Edge_index is our adjacency matrix which is used in message passing, but this matrix will stay the same.  The name (name of the protein) and graph_y (the motion label) will not change during message passing.\n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How does using global_max_pool make the model invariant to node ordering?\"\ncorrect_answer = \"It aggregates node-level features into a single graph-level feature, disregarding node order.\"\ndecoy_answers = [\n    \"It selects the maximum feature from each residue, which automatically orders the residues.\",\n    \"It ensures that all residues are processed in a fixed order before pooling, making the model invariant.\",\n    \"It normalizes node feature vectors, so the order doesn’t matter.\"\n    ]\nexplanation =\"\"\n\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How does using global_max_pool make the model invariant to node ordering?\n    \n        \n          \n          It aggregates node-level features into a single graph-level feature, disregarding node order.\n        \n        \n        \n          \n          It selects the maximum feature from each residue, which automatically orders the residues.\n        \n        \n        \n          \n          It ensures that all residues are processed in a fixed order before pooling, making the model invariant.\n        \n        \n        \n          \n          It normalizes node feature vectors, so the order doesn’t matter.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        \n      \n    \n    \n    \n\n\nImage Source: torch_geometric.nn conv.GCNConv"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-구현-모델-정의",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-구현-모델-정의",
    "title": "섹션 1: 그래프 이론",
    "section": "GCN 구현: 모델 정의",
    "text": "GCN 구현: 모델 정의\n\n\nCode\n\nclass GCNGraphLevel(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super(GCNGraphLevel, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = torch.nn.Linear(hidden_channels, out_channels)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = self.conv2(x, edge_index)\n\n        x = global_max_pool(x, batch)  # Need to pool node features to graph level for graph level tasks\n\n        out = self.lin(x)\n\n        return out"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#gcn-구현-학습-및-평가",
    "href": "notebooks/WS07_GNNsForProteins.html#gcn-구현-학습-및-평가",
    "title": "섹션 1: 그래프 이론",
    "section": "GCN 구현: 학습 및 평가",
    "text": "GCN 구현: 학습 및 평가\n\n\nCode\n# TRAIN\n# Create DataLoader\ntrain_loader = DataLoader(train_pyg_data_list, batch_size=32, shuffle=True)\n\n# Initialize model, loss, and optimizer\nmodel = GCNGraphLevel(in_channels=4, hidden_channels=16, out_channels=len(motion_type_to_label))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n\n# Clear the log file for training metrics\nlog_file = 'train_metrics_log.txt'\nopen(log_file, 'w').close()\n\noptimizer_steps = 0\n\nfor epoch in range(100):\n    model.train()\n    total_train_loss = 0\n    correct_train = 0\n    for data in train_loader:\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.graph_y)\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct_train += (pred == data.graph_y).sum().item()\n        optimizer_steps += 1\n\n    train_loss = total_train_loss / len(train_loader)\n    train_accuracy = correct_train / len(train_loader.dataset)\n\n    # Log the training metrics\n    log_metrics(epoch, optimizer_steps, train_loss, train_accuracy, log_file=log_file)\n    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n\n# Plot the training metrics after all epochs\nplot_metrics(log_file='train_metrics_log.txt')\n\n\nEpoch 0, Train Loss: 12.8542, Train Accuracy: 0.1500\nEpoch 1, Train Loss: 6.3447, Train Accuracy: 0.3167\nEpoch 2, Train Loss: 3.7893, Train Accuracy: 0.3000\nEpoch 3, Train Loss: 3.7614, Train Accuracy: 0.3333\nEpoch 4, Train Loss: 3.7813, Train Accuracy: 0.3167\nEpoch 5, Train Loss: 2.8323, Train Accuracy: 0.3333\nEpoch 6, Train Loss: 2.2536, Train Accuracy: 0.3333\nEpoch 7, Train Loss: 1.8248, Train Accuracy: 0.3500\nEpoch 8, Train Loss: 1.7774, Train Accuracy: 0.4167\nEpoch 9, Train Loss: 1.6448, Train Accuracy: 0.3667\nEpoch 10, Train Loss: 1.2767, Train Accuracy: 0.3833\nEpoch 11, Train Loss: 1.1132, Train Accuracy: 0.4667\nEpoch 12, Train Loss: 1.1480, Train Accuracy: 0.4833\nEpoch 13, Train Loss: 1.0810, Train Accuracy: 0.4833\nEpoch 14, Train Loss: 0.9532, Train Accuracy: 0.5167\nEpoch 15, Train Loss: 0.9317, Train Accuracy: 0.5167\nEpoch 16, Train Loss: 0.9816, Train Accuracy: 0.4833\nEpoch 17, Train Loss: 0.9383, Train Accuracy: 0.4833\nEpoch 18, Train Loss: 0.8855, Train Accuracy: 0.5833\nEpoch 19, Train Loss: 0.9045, Train Accuracy: 0.5833\nEpoch 20, Train Loss: 0.8739, Train Accuracy: 0.6000\nEpoch 21, Train Loss: 0.8435, Train Accuracy: 0.6500\nEpoch 22, Train Loss: 0.8285, Train Accuracy: 0.7167\nEpoch 23, Train Loss: 0.8412, Train Accuracy: 0.6000\nEpoch 24, Train Loss: 0.8264, Train Accuracy: 0.6000\nEpoch 25, Train Loss: 0.7839, Train Accuracy: 0.6167\nEpoch 26, Train Loss: 0.7857, Train Accuracy: 0.6667\nEpoch 27, Train Loss: 0.7749, Train Accuracy: 0.7333\nEpoch 28, Train Loss: 0.7649, Train Accuracy: 0.7167\nEpoch 29, Train Loss: 0.7465, Train Accuracy: 0.7167\nEpoch 30, Train Loss: 0.7388, Train Accuracy: 0.7000\nEpoch 31, Train Loss: 0.7299, Train Accuracy: 0.6667\nEpoch 32, Train Loss: 0.7232, Train Accuracy: 0.6500\nEpoch 33, Train Loss: 0.7085, Train Accuracy: 0.7167\nEpoch 34, Train Loss: 0.7005, Train Accuracy: 0.7500\nEpoch 35, Train Loss: 0.7064, Train Accuracy: 0.8000\nEpoch 36, Train Loss: 0.6970, Train Accuracy: 0.7500\nEpoch 37, Train Loss: 0.6793, Train Accuracy: 0.6667\nEpoch 38, Train Loss: 0.6814, Train Accuracy: 0.7167\nEpoch 39, Train Loss: 0.6840, Train Accuracy: 0.7500\nEpoch 40, Train Loss: 0.6615, Train Accuracy: 0.7667\nEpoch 41, Train Loss: 0.6658, Train Accuracy: 0.7833\nEpoch 42, Train Loss: 0.6540, Train Accuracy: 0.8000\nEpoch 43, Train Loss: 0.6485, Train Accuracy: 0.7833\nEpoch 44, Train Loss: 0.6251, Train Accuracy: 0.8167\nEpoch 45, Train Loss: 0.6155, Train Accuracy: 0.8167\nEpoch 46, Train Loss: 0.6059, Train Accuracy: 0.8167\nEpoch 47, Train Loss: 0.5930, Train Accuracy: 0.8667\nEpoch 48, Train Loss: 0.5906, Train Accuracy: 0.8667\nEpoch 49, Train Loss: 0.5836, Train Accuracy: 0.8500\nEpoch 50, Train Loss: 0.5747, Train Accuracy: 0.8500\nEpoch 51, Train Loss: 0.5631, Train Accuracy: 0.8667\nEpoch 52, Train Loss: 0.5603, Train Accuracy: 0.8667\nEpoch 53, Train Loss: 0.5553, Train Accuracy: 0.8667\nEpoch 54, Train Loss: 0.5519, Train Accuracy: 0.8833\nEpoch 55, Train Loss: 0.5464, Train Accuracy: 0.8833\nEpoch 56, Train Loss: 0.5261, Train Accuracy: 0.8667\nEpoch 57, Train Loss: 0.5216, Train Accuracy: 0.8667\nEpoch 58, Train Loss: 0.5193, Train Accuracy: 0.8667\nEpoch 59, Train Loss: 0.5110, Train Accuracy: 0.8833\nEpoch 60, Train Loss: 0.5131, Train Accuracy: 0.8833\nEpoch 61, Train Loss: 0.4983, Train Accuracy: 0.8667\nEpoch 62, Train Loss: 0.4830, Train Accuracy: 0.9000\nEpoch 63, Train Loss: 0.4783, Train Accuracy: 0.8833\nEpoch 64, Train Loss: 0.4704, Train Accuracy: 0.8833\nEpoch 65, Train Loss: 0.4688, Train Accuracy: 0.9000\nEpoch 66, Train Loss: 0.4667, Train Accuracy: 0.8833\nEpoch 67, Train Loss: 0.4519, Train Accuracy: 0.8833\nEpoch 68, Train Loss: 0.4655, Train Accuracy: 0.9000\nEpoch 69, Train Loss: 0.4454, Train Accuracy: 0.8833\nEpoch 70, Train Loss: 0.4393, Train Accuracy: 0.8833\nEpoch 71, Train Loss: 0.4304, Train Accuracy: 0.8833\nEpoch 72, Train Loss: 0.4180, Train Accuracy: 0.8833\nEpoch 73, Train Loss: 0.4270, Train Accuracy: 0.8833\nEpoch 74, Train Loss: 0.4265, Train Accuracy: 0.8667\nEpoch 75, Train Loss: 0.4053, Train Accuracy: 0.9000\nEpoch 76, Train Loss: 0.4087, Train Accuracy: 0.8833\nEpoch 77, Train Loss: 0.4053, Train Accuracy: 0.9000\nEpoch 78, Train Loss: 0.3857, Train Accuracy: 0.9000\nEpoch 79, Train Loss: 0.3923, Train Accuracy: 0.9167\nEpoch 80, Train Loss: 0.3872, Train Accuracy: 0.8833\nEpoch 81, Train Loss: 0.3822, Train Accuracy: 0.8833\nEpoch 82, Train Loss: 0.3630, Train Accuracy: 0.8833\nEpoch 83, Train Loss: 0.3543, Train Accuracy: 0.9000\nEpoch 84, Train Loss: 0.3656, Train Accuracy: 0.9000\nEpoch 85, Train Loss: 0.3574, Train Accuracy: 0.9167\nEpoch 86, Train Loss: 0.3587, Train Accuracy: 0.9000\nEpoch 87, Train Loss: 0.3434, Train Accuracy: 0.9167\nEpoch 88, Train Loss: 0.3382, Train Accuracy: 0.9167\nEpoch 89, Train Loss: 0.3434, Train Accuracy: 0.8833\nEpoch 90, Train Loss: 0.3268, Train Accuracy: 0.9333\nEpoch 91, Train Loss: 0.3211, Train Accuracy: 0.9167\nEpoch 92, Train Loss: 0.3376, Train Accuracy: 0.9000\nEpoch 93, Train Loss: 0.3212, Train Accuracy: 0.9167\nEpoch 94, Train Loss: 0.3384, Train Accuracy: 0.9167\nEpoch 95, Train Loss: 0.3035, Train Accuracy: 0.9167\nEpoch 96, Train Loss: 0.2945, Train Accuracy: 0.9333\nEpoch 97, Train Loss: 0.2991, Train Accuracy: 0.9333\nEpoch 98, Train Loss: 0.2932, Train Accuracy: 0.9333\nEpoch 99, Train Loss: 0.2990, Train Accuracy: 0.9000\n\n\n\n\n\n\n\n\n\n\n\nCode\n# EVAL\n# Create DataLoader\ntest_loader = DataLoader(test_pyg_data_list, batch_size=32)\n\n# Initialize model, loss, and optimizer\n# model = GCNGraphLevel(in_channels=3, hidden_channels=32, out_channels=len(motion_type_to_label))\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n# criterion = torch.nn.CrossEntropyLoss()\n\n# Clear the log file for training metrics\nlog_file = 'eval_metrics_log.txt'\nopen(log_file, 'w').close()\n\noptimizer_steps = 0\n\nfor epoch in range(10):\n    model.eval()\n    total_eval_loss = 0\n    correct_eval = 0\n    for data in test_loader:\n        optimizer.zero_grad()\n        out = model(data)\n        loss = criterion(out, data.graph_y)\n        loss.backward()\n        optimizer.step()\n\n        total_eval_loss += loss.item()\n        pred = out.argmax(dim=1)\n        correct_eval += (pred == data.graph_y).sum().item()\n        optimizer_steps += 1\n\n    eval_loss = total_eval_loss / len(test_loader)\n    eval_accuracy = correct_eval / len(test_loader.dataset)\n\n    # Log the training metrics\n    log_metrics(epoch, optimizer_steps, eval_loss, eval_accuracy, log_file=log_file)\n    print(f'Epoch {epoch}, Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_accuracy:.4f}')\n\n# Plot the training metrics after all epochs\nplot_metrics(log_file='eval_metrics_log.txt')\n\n\nEpoch 0, Eval Loss: 0.2103, Eval Accuracy: 0.9333\nEpoch 1, Eval Loss: 0.1836, Eval Accuracy: 0.9333\nEpoch 2, Eval Loss: 0.1494, Eval Accuracy: 1.0000\nEpoch 3, Eval Loss: 0.1198, Eval Accuracy: 1.0000\nEpoch 4, Eval Loss: 0.0984, Eval Accuracy: 1.0000\nEpoch 5, Eval Loss: 0.0851, Eval Accuracy: 1.0000\nEpoch 6, Eval Loss: 0.0745, Eval Accuracy: 1.0000\nEpoch 7, Eval Loss: 0.0606, Eval Accuracy: 1.0000\nEpoch 8, Eval Loss: 0.0477, Eval Accuracy: 1.0000\nEpoch 9, Eval Loss: 0.0394, Eval Accuracy: 1.0000\n\n\n\n\n\n\n\n\n\n\n 예상 결과:\n학습 중에 GNN은 에포크 0과 에포크 99에 대해 다음과 같은 결과를 가져야 합니다:   Epoch 0, Train Loss: 12.8542, Train Accuracy: 0.1500  Epoch 99, Train Loss: 0.2990, Train Accuracy: 0.9000  \n평가 중에 GNN은 에포크 0과 에포크 9에 대해 다음과 같은 결과를 가져야 합니다:  Epoch 0, Eval Loss: 0.2103, Eval Accuracy: 0.9333  Epoch 9, Eval Loss: 0.0394, Eval Accuracy: 1.0000 \n아래는 데이터셋에 존재하는 단백질의 각 동작 유형 수를 요약한 표입니다.\n\n\nCode\n# Results\nprint(df_pscdb['motion_type'].value_counts())\n\n\nmotion_type\nno_significant_motion        199\nindependent_local_motion      89\ncoupled_local_motion          81\nburying_ligand_motion         70\nindependent_domain_motion     41\ncoupled_domain_motion         37\nName: count, dtype: int64"
  },
  {
    "objectID": "notebooks/WS07_GNNsForProteins.html#questions",
    "href": "notebooks/WS07_GNNsForProteins.html#questions",
    "title": "섹션 1: 그래프 이론",
    "section": "Questions!",
    "text": "Questions!\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\n\ndef create_quiz_2q(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Generate the HTML code for the initial question\n    initial_question = \"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3&gt;A GCN is invariant to the order of residues (nodes).&lt;/h3&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption1\" name=\"initQuiz\" value=\"True\"&gt;\n        &lt;label for=\"initOption1\"&gt;True&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption2\" name=\"initQuiz\" value=\"False\"&gt;\n        &lt;label for=\"initOption2\"&gt;False&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;button onclick=\"checkInitialAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"initFeedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div id=\"mainQuiz\" style=\"display:none;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{0}&lt;/h3&gt;\n    \"\"\".format(question)\n\n    # Add each answer as a radio button for the main question\n    for i, answer in enumerate(answers):\n        initial_question += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section for the main quiz\n    initial_question += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkInitialAnswer() {{\n        var radios = document.getElementsByName('initQuiz');\n        var correctAnswer = \"True\";\n        var feedback = document.getElementById('initFeedback');\n        var mainQuizDiv = document.getElementById('mainQuiz');\n        var selectedOption = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                break;\n            }}\n        }}\n\n        // Provide feedback for the initial question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                mainQuizDiv.style.display = \"block\";  // Show the main quiz\n            }} else {{\n                feedback.textContent = \"Incorrect. Try again!\";\n                feedback.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback for the main question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(initial_question))\n\nquestion = \"WHY do we want the GCN to be invariant to the order of residues (nodes)?\"\ncorrect_answer = \"The order of residues in the graph does not affect the overall structure of the protein, so the GCN should not depend on the node order.\"\ndecoy_answers = [\n    \"The order of residues in the graph affects the spatial structure of the protein, so GCN should be sensitive to the node order.\",\n    \"The GCN needs to process each residue in a specific order, which determines the protein’s motion type.\",\n    \"Residue order affects the degree of each node, and the GCN should account for this.\"\n    ]\nexplanation =\"In a protein structure, what matters is the graph's structure (how residues are connected) rather than the order in which nodes (residues) are listed in the dataset. The model should be invariant to node ordering because the classification task (protein motion type) depends on the overall graph structure, not on the specific node order in the input.\"\n\n\ncreate_quiz_2q(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      A GCN is invariant to the order of residues (nodes).\n      \n        \n        True\n      \n      \n        \n        False\n      \n      Submit\n      \n    \n    \n      WHY do we want the GCN to be invariant to the order of residues (nodes)?\n    \n        \n          \n          The order of residues in the graph does not affect the overall structure of the protein, so the GCN should not depend on the node order.\n        \n        \n        \n          \n          The order of residues in the graph affects the spatial structure of the protein, so GCN should be sensitive to the node order.\n        \n        \n        \n          \n          The GCN needs to process each residue in a specific order, which determines the protein’s motion type.\n        \n        \n        \n          \n          Residue order affects the degree of each node, and the GCN should account for this.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        In a protein structure, what matters is the graph's structure (how residues are connected) rather than the order in which nodes (residues) are listed in the dataset. The model should be invariant to node ordering because the classification task (protein motion type) depends on the overall graph structure, not on the specific node order in the input.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nfrom IPython.display import display, HTML\n\ndef create_quiz_2q(question, correct_answer, decoy_answers, explanation):\n    # Combine the correct answer with the decoy answers\n    answers = [correct_answer] + decoy_answers\n\n    # Generate the HTML code for the initial question\n    initial_question = \"\"\"\n    &lt;div style=\"font-family: Arial, sans-serif; margin: 30px; max-width: 1000px;\"&gt;\n      &lt;h3&gt;The GCN is equivariant to permutations of nodes.&lt;/h3&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption1\" name=\"initQuiz\" value=\"True\"&gt;\n        &lt;label for=\"initOption1\"&gt;True&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;div style=\"margin-bottom: 10px;\"&gt;\n        &lt;input type=\"radio\" id=\"initOption2\" name=\"initQuiz\" value=\"False\"&gt;\n        &lt;label for=\"initOption2\"&gt;False&lt;/label&gt;\n      &lt;/div&gt;\n      &lt;button onclick=\"checkInitialAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;p id=\"initFeedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;div id=\"mainQuiz\" style=\"display:none;\"&gt;\n      &lt;h3 style=\"word-wrap: break-word;\"&gt;{0}&lt;/h3&gt;\n    \"\"\".format(question)\n\n    # Add each answer as a radio button for the main question\n    for i, answer in enumerate(answers):\n        initial_question += f\"\"\"\n        &lt;div style=\"word-wrap: break-word; margin-bottom: 10px;\"&gt;\n          &lt;input type=\"radio\" id=\"option{i}\" name=\"quiz\" value=\"{answer}\"&gt;\n          &lt;label for=\"option{i}\" id=\"label{i}\"&gt;{answer}&lt;/label&gt;\n        &lt;/div&gt;\n        \"\"\"\n\n    # Add the submit button, explanation button, feedback section, and explanation section for the main quiz\n    initial_question += f\"\"\"\n      &lt;button onclick=\"checkAnswer()\"&gt;Submit&lt;/button&gt;\n      &lt;button onclick=\"showExplanation()\"&gt;Show Explanation&lt;/button&gt;\n      &lt;p id=\"feedback\" style=\"font-weight: bold;\"&gt;&lt;/p&gt;\n      &lt;div id=\"explanation\" style=\"display:none; margin-top: 20px; padding: 10px; border: 1px solid #ccc; background-color: #f9f9f9;\"&gt;\n        &lt;h4&gt;Explanation:&lt;/h4&gt;\n        &lt;p&gt;{explanation}&lt;/p&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n    &lt;script&gt;\n    function checkInitialAnswer() {{\n        var radios = document.getElementsByName('initQuiz');\n        var correctAnswer = \"True\";\n        var feedback = document.getElementById('initFeedback');\n        var mainQuizDiv = document.getElementById('mainQuiz');\n        var selectedOption = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                break;\n            }}\n        }}\n\n        // Provide feedback for the initial question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                mainQuizDiv.style.display = \"block\";  // Show the main quiz\n            }} else {{\n                feedback.textContent = \"Incorrect. Try again!\";\n                feedback.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function checkAnswer() {{\n        var radios = document.getElementsByName('quiz');\n        var correctAnswer = \"{correct_answer}\";\n        var feedback = document.getElementById('feedback');\n        var selectedOption = null;\n        var selectedLabel = null;\n\n        // Get the selected option\n        for (var i = 0, length = radios.length; i &lt; length; i++) {{\n            if (radios[i].checked) {{\n                selectedOption = radios[i].value;\n                selectedLabel = document.getElementById('label' + i);\n                break;\n            }}\n        }}\n\n        // Provide feedback for the main question\n        if (selectedOption) {{\n            if (selectedOption === correctAnswer) {{\n                feedback.textContent = \"Correct!\";\n                feedback.style.color = \"green\";\n                selectedLabel.style.color = \"green\";\n            }} else {{\n                feedback.textContent = \"Incorrect.\";\n                feedback.style.color = \"red\";\n                selectedLabel.style.color = \"red\";\n            }}\n        }} else {{\n            feedback.textContent = \"Please select an answer.\";\n            feedback.style.color = \"orange\";\n        }}\n    }}\n\n    function showExplanation() {{\n        var explanationDiv = document.getElementById('explanation');\n        explanationDiv.style.display = \"block\";\n    }}\n    &lt;/script&gt;\n    \"\"\"\n\n    display(HTML(initial_question))\n\nquestion = \"What does that mean?\"\ncorrect_answer = \"If you permute the order of the nodes, the output features will change in the same way, maintaining the relationships between nodes.\"\ndecoy_answers = [\n    \"If you permute the order of the nodes, the output features of the GCN will change randomly, making the model more robust.\",\n    \"The model ensures that the output features are completely independent of the order of nodes.\",\n    \"Equivariance ensures that the GCN’s predictions remain unchanged when nodes are permuted.\"\n    ]\nexplanation =\"Equivariance means that if the nodes (residues) are permuted (i.e., their order is changed), the output features will change correspondingly while preserving the relationships between the nodes. This is crucial in a graph-based problem like protein classification because we care about the structure of the graph (how nodes are connected) rather than the specific order in which nodes are presented.\"\n\n\ncreate_quiz_2q(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      The GCN is equivariant to permutations of nodes.\n      \n        \n        True\n      \n      \n        \n        False\n      \n      Submit\n      \n    \n    \n      What does that mean?\n    \n        \n          \n          If you permute the order of the nodes, the output features will change in the same way, maintaining the relationships between nodes.\n        \n        \n        \n          \n          If you permute the order of the nodes, the output features of the GCN will change randomly, making the model more robust.\n        \n        \n        \n          \n          The model ensures that the output features are completely independent of the order of nodes.\n        \n        \n        \n          \n          Equivariance ensures that the GCN’s predictions remain unchanged when nodes are permuted.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Equivariance means that if the nodes (residues) are permuted (i.e., their order is changed), the output features will change correspondingly while preserving the relationships between the nodes. This is crucial in a graph-based problem like protein classification because we care about the structure of the graph (how nodes are connected) rather than the specific order in which nodes are presented.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Why is equivariance important in this protein classification problem?\"\ncorrect_answer = \"It ensures that the GCN updates the features in a way that is consistent with the graph structure, regardless of node order.\"\ndecoy_answers = [\n    \"It helps the GCN handle missing data by adjusting the order of residues based on importance.\",\n    \"It guarantees that the GCN outputs remain constant even if the spatial structure of the protein changes.\",\n    \"It ensures that the GCN outputs are identical no matter how the nodes are ordered, which is crucial for protein classification.\"\n    ]\nexplanation =\"Equivariance ensures that the relationships between nodes (residues) in the graph are preserved, no matter how the nodes are ordered in the dataset. This is important because the spatial structure of the protein (how residues are connected) determines the motion type, not the order in which the residues are listed.\"\n\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Why is equivariance important in this protein classification problem?\n    \n        \n          \n          It ensures that the GCN updates the features in a way that is consistent with the graph structure, regardless of node order.\n        \n        \n        \n          \n          It helps the GCN handle missing data by adjusting the order of residues based on importance.\n        \n        \n        \n          \n          It guarantees that the GCN outputs remain constant even if the spatial structure of the protein changes.\n        \n        \n        \n          \n          It ensures that the GCN outputs are identical no matter how the nodes are ordered, which is crucial for protein classification.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Equivariance ensures that the relationships between nodes (residues) in the graph are preserved, no matter how the nodes are ordered in the dataset. This is important because the spatial structure of the protein (how residues are connected) determines the motion type, not the order in which the residues are listed.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Where in the GCN is `graph_y` used?\"\ncorrect_answer = \"It is used as the label for the entire graph and is applied in the loss function during training.\"\ndecoy_answers = [\n    \"It is used in every GCNConv layer to update the node features.\",\n    \"It is used in the message-passing step to decide which edges to include.\",\n    \"It is used in the ReLU activation function to control the non-linearity.\"\n]\nexplanation = \"Graph_y is the graph-level label that represents the motion type classification for the entire protein. It is not involved in message passing, but is used in the loss function during training.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Where in the GCN is `graph_y` used?\n    \n        \n          \n          It is used as the label for the entire graph and is applied in the loss function during training.\n        \n        \n        \n          \n          It is used in every GCNConv layer to update the node features.\n        \n        \n        \n          \n          It is used in the message-passing step to decide which edges to include.\n        \n        \n        \n          \n          It is used in the ReLU activation function to control the non-linearity.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Graph_y is the graph-level label that represents the motion type classification for the entire protein. It is not involved in message passing, but is used in the loss function during training.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How do the XYZ coordinates of residues get updated during message passing in each GCN layer?\"\ncorrect_answer = \"Each node updates its XYZ coordinates by aggregating the feature vectors from its neighboring nodes, then applying a linear transformation and non-linearity.\"\ndecoy_answers = [\n    \"The XYZ coordinates are updated by adding random noise in each GCNConv layer to make the model more robust.\",\n    \"Each node sends its XYZ coordinates to all other nodes in the graph, and they are averaged across the graph.\",\n    \"The XYZ coordinates are directly multiplied by the edge weights without aggregation.\"\n]\nexplanation = \"Each node aggregates information from its neighbors (defined by the graph’s edges) to update its features, including XYZ coordinates. This is followed by a linear transformation and ReLU non-linearity.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How do the XYZ coordinates of residues get updated during message passing in each GCN layer?\n    \n        \n          \n          Each node updates its XYZ coordinates by aggregating the feature vectors from its neighboring nodes, then applying a linear transformation and non-linearity.\n        \n        \n        \n          \n          The XYZ coordinates are updated by adding random noise in each GCNConv layer to make the model more robust.\n        \n        \n        \n          \n          Each node sends its XYZ coordinates to all other nodes in the graph, and they are averaged across the graph.\n        \n        \n        \n          \n          The XYZ coordinates are directly multiplied by the edge weights without aggregation.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Each node aggregates information from its neighbors (defined by the graph’s edges) to update its features, including XYZ coordinates. This is followed by a linear transformation and ReLU non-linearity.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What role do the edges (`edge_index`) play in message passing? What step are they involved in and how? Do they ever get updated?\"\ncorrect_answer = \"The edges determine which nodes communicate with each other during message passing and they do not get updated during the process.\"\ndecoy_answers = [\n    \"The edges represent the distance between residues and get updated after every GCNConv layer.\",\n    \"The edges determine which nodes get disconnected after each layer, reducing the size of the graph.\",\n    \"The edges are used only in the first GCNConv layer and are discarded afterward.\"\n]\nexplanation = \"The edges in `edge_index` define how nodes (residues) communicate with each other in the graph. They remain fixed throughout the GCN layers and are not updated.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      What role do the edges (`edge_index`) play in message passing? What step are they involved in and how? Do they ever get updated?\n    \n        \n          \n          The edges determine which nodes communicate with each other during message passing and they do not get updated during the process.\n        \n        \n        \n          \n          The edges represent the distance between residues and get updated after every GCNConv layer.\n        \n        \n        \n          \n          The edges determine which nodes get disconnected after each layer, reducing the size of the graph.\n        \n        \n        \n          \n          The edges are used only in the first GCNConv layer and are discarded afterward.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        The edges in `edge_index` define how nodes (residues) communicate with each other in the graph. They remain fixed throughout the GCN layers and are not updated.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"Why does the GCN have two GCNConv layers in the architecture? What might happen if you added more layers or reduced the number of layers?\"\ncorrect_answer = \"Adding more layers allows the model to aggregate information from nodes farther away, but too many layers can cause over-smoothing, where all node features become indistinguishable.\"\ndecoy_answers = [\n    \"Two layers ensure that the graph is fully connected, and adding more layers would improve the classification accuracy without any drawbacks.\",\n    \"The model uses two layers to ensure that each node’s features are multiplied by the motion type label twice, increasing accuracy.\",\n    \"Adding more layers will reduce overfitting, and having fewer layers will always lead to underfitting.\"\n]\nexplanation = \"More GCN layers allow nodes to aggregate information from more distant nodes, but too many layers can result in over-smoothing, where node features become too similar and lose their distinctiveness.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      Why does the GCN have two GCNConv layers in the architecture? What might happen if you added more layers or reduced the number of layers?\n    \n        \n          \n          Adding more layers allows the model to aggregate information from nodes farther away, but too many layers can cause over-smoothing, where all node features become indistinguishable.\n        \n        \n        \n          \n          Two layers ensure that the graph is fully connected, and adding more layers would improve the classification accuracy without any drawbacks.\n        \n        \n        \n          \n          The model uses two layers to ensure that each node’s features are multiplied by the motion type label twice, increasing accuracy.\n        \n        \n        \n          \n          Adding more layers will reduce overfitting, and having fewer layers will always lead to underfitting.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        More GCN layers allow nodes to aggregate information from more distant nodes, but too many layers can result in over-smoothing, where node features become too similar and lose their distinctiveness.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"How does `global_max_pool(x, batch)` help aggregate the information from multiple nodes in a protein to create a graph-level feature vector?\"\ncorrect_answer = \"It selects the maximum feature value from each node across the entire graph, creating a single vector that represents the entire graph (protein).\"\ndecoy_answers = [\n    \"It selects the minimum feature value across all nodes in a graph to create a graph-level feature.\",\n    \"It averages the feature vectors of all nodes and outputs the graph-level feature vector.\",\n    \"It sums all the node features together to create the graph-level feature vector.\"\n]\nexplanation = \"Global max pooling selects the maximum value from each node's features, creating a graph-level feature vector that summarizes the entire graph, making it ideal for tasks like protein classification.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      How does `global_max_pool(x, batch)` help aggregate the information from multiple nodes in a protein to create a graph-level feature vector?\n    \n        \n          \n          It selects the maximum feature value from each node across the entire graph, creating a single vector that represents the entire graph (protein).\n        \n        \n        \n          \n          It selects the minimum feature value across all nodes in a graph to create a graph-level feature.\n        \n        \n        \n          \n          It averages the feature vectors of all nodes and outputs the graph-level feature vector.\n        \n        \n        \n          \n          It sums all the node features together to create the graph-level feature vector.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Global max pooling selects the maximum value from each node's features, creating a graph-level feature vector that summarizes the entire graph, making it ideal for tasks like protein classification.\n      \n    \n    \n    \n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\n\nquestion = \"What is the role of the final linear layer (`self.lin`) in this GCN model, and how does it relate to the classification task?\"\ncorrect_answer = \"The linear layer directly predicts the motion type by converting the graph-level features into class probabilities.\"\ndecoy_answers = [\n    \"The linear layer increases the dimensionality of the node features to make the graph more complex.\",\n    \"The linear layer aggregates information from all edges to determine the motion type.\",\n    \"The linear layer is used to update the feature vectors of each residue and is not involved in classification.\"\n]\nexplanation = \"The final linear layer converts the graph-level feature vector into a set of class probabilities, which in this case represent the predicted motion type of the protein.\"\ncreate_quiz(question, correct_answer, decoy_answers, explanation)\n\n\n\n    \n      What is the role of the final linear layer (`self.lin`) in this GCN model, and how does it relate to the classification task?\n    \n        \n          \n          The linear layer directly predicts the motion type by converting the graph-level features into class probabilities.\n        \n        \n        \n          \n          The linear layer increases the dimensionality of the node features to make the graph more complex.\n        \n        \n        \n          \n          The linear layer aggregates information from all edges to determine the motion type.\n        \n        \n        \n          \n          The linear layer is used to update the feature vectors of each residue and is not involved in classification.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        The final linear layer converts the graph-level feature vector into a set of class probabilities, which in this case represent the predicted motion type of the protein.\n      \n    \n    \n    \n\n\n\n\nCode\n## **&lt;font color='#e59454'&gt;질문들!**\n\n\n\n    \n      How does the edge structure (e.g., 1133 edges for protein `2nrt`) affect the message passing process? How might different types of edges (e.g., chemical bonds, proximity in 3D space) influence the GCN’s ability to classify protein motion?\n    \n        \n          \n          The edges allow for message passing between nodes, and different types of edges (e.g., chemical bonds, spatial proximity) affect how well the GCN can capture meaningful interactions between residues.\n        \n        \n        \n          \n          The edges control the activation function used in each GCNConv layer and are updated after every message-passing step.\n        \n        \n        \n          \n          The edge structure determines the order in which the nodes are processed, with more edges resulting in more iterations.\n        \n        \n        \n          \n          The edges are only used for regularization and do not affect message passing or classification performance.\n        \n        \n      Submit\n      Show Explanation\n      \n      \n        Explanation:\n        Edges define which nodes can communicate during message passing. Different types of edges, such as those representing chemical bonds or spatial proximity, capture different kinds of relationships between residues, impacting how the GCN models protein motion.\n      \n    \n    \n    \n\n\n\nGNN을 좋아하고 더 알고 싶다면 훌륭한 리소스를 소개합니다:\n\nPetar Veličković는 슬라이드와 함께 제공되는 훌륭한 강의와 colab 노트북을 가지고 있습니다: \nYouTube 강의: 그래프 신경망의 이론적 기초 \n슬라이드: https://petar-v.com/talks/GNN-Wednesday.pdf \nColab 노트북: JAX/jraph를 사용한 그래프 신경망 소개 \n또한, 그가 GAT 리소스를 위해 만든 git 레포지토리가 있습니다(훌륭한 시각 자료가 포함됨): GAT - 그래프 어텐션 네트워크 (PyTorch) + 그래프 \n훌륭한 입문서: 그래프 신경망에 대한 완만한 소개\ncolab 노트북 및 비디오 모음: PyTorch Geometric 튜토리얼 프로젝트 \n다른 GNN 변형의 구현이 포함된 또 다른 유용한 colab 노트북: 기하학적 그래프 신경망에 대한 완만한 소개"
  },
  {
    "objectID": "notebooks/WS10_RFDiffusion_AllAtom.html",
    "href": "notebooks/WS10_RFDiffusion_AllAtom.html",
    "title": "섹션 1: 소개",
    "section": "",
    "text": "RFDiffusion All Atom을 사용한 드노보 단백질 설계\n##홈페이지로 돌아가기 RoseTTAFold All-Atom : Krishna et al., Science 384, 291 (2024)\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\n관심 있는 작은 분자를 결합하는 새로운 단백질 설계\nRFDiffusion All Atom 모델에 대한 플래그 식별 및 적용\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n 목차 \n섹션 1: 소개\n섹션 2: 작은 분자 바인더 설계\n\n\nCode\n!pip install torch==2.5.0\n\n\n\n 라이브러리 임포트\n\n\nCode\n#@markdown Please run as you read the introduction\n\n## 1) clone `rf_diffusion_all_atom` repo to local environment\n!git clone https://github.com/baker-laboratory/rf_diffusion_all_atom.git\n# move into repo\n%cd rf_diffusion_all_atom\n\n\n## 2) download container used to run RFAA\n'''\nauthor Sergey Lyskov - \"Minimal example of running Singularity Containers using\nNeurodesk tools inside Colab notebook.\nBased on https://colab.research.google.com/drive/1g5cnZxj1llRaHmOs4xSglqsXnFkQYuol\n-- big thanks to Neurodesk team for sharing this!\"\n'''\n\n#set environment variables\nimport os\n#set `LR_PRELOAD` to an empy string to prevent any preloaded libraries from interfering\nos.environ[\"LD_PRELOAD\"] = \"\";\n#set `APPTAINER_BINDPATH` to `/content` tp ensure colab's working dir is accessible in container\nos.environ[\"APPTAINER_BINDPATH\"] = \"/content\"\n#`LMOD_CMD` points to the system used to manage environment settings\nos.environ[\"LMOD_CMD\"] = \"/usr/share/lmod/lmod/libexec/lmod\"\n# download script from NeuroDesk\n!curl -J -O https://raw.githubusercontent.com/NeuroDesk/neurocommand/main/googlecolab_setup.sh\n# make script executable\n!chmod +x googlecolab_setup.sh\n# setup NeuroDesk env within colab\n!./googlecolab_setup.sh\n# set path for variable used by LMOD\nos.environ[\"MODULEPATH\"] = ':'.join(map(str, list(map(lambda x: os.path.join(os.path.abspath('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/'), x),os.listdir('/cvmfs/neurodesk.ardc.edu.au/neurodesk-modules/')))))\n\n# print Alpine Linux image from DockerHub inside container\n!apptainer exec docker://alpine cat /etc/alpine-release\n# check version of Alpine Linux inside image\n!singularity exec docker://alpine cat /etc/alpine-release\n\n# download singularity conatainer file for RF-AA\n!wget http://files.ipd.uw.edu/pub/RF-All-Atom/containers/rf_se3_diffusion.sif\n\n# run os-release in RF-SS container\n!singularity exec rf_se3_diffusion.sif cat /etc/os-release\n\n#!singularity run shub://vsoch/hello-world\n!singularity run docker://godlovedc/lolcow\n\n\n## 3) download model weights\n!wget http://files.ipd.uw.edu/pub/RF-All-Atom/weights/RFDiffusionAA_paper_weights.pt\n\n\n## 4) initialize git submodules\n!git submodule init\n!git submodule update\n\n\n\n\n\n\n\n\n\n이 워크숍에서는 RFAA 및 RFdiffusionAA의 원리를 살펴보고, 방법론과 응용 프로그램을 검토하여 계산 도구가 생체 분자 과학을 어떻게 재편하고 있는지에 대한 포괄적인 이해를 제공합니다.\n지난 몇 번의 워크숍에서 우리는 단백질 전용 시스템에 집중했습니다. 실제로 단백질은 다른 단백질과 복합체를 형성할 뿐만 아니라 다양한 세포 과정 중에 DNA, RNA, 작은 분자 및 기타 비단백질 리간드와 상호 작용합니다.\nRoseTTAFold All-Atom (RFAA)의 목표는 이러한 다양한 생체 분자 어셈블리의 일반화된 예측 및 설계를 가능하게 하는 모델을 개발하는 것이었습니다. 단백질에만 집중하는 이전 접근 방식과 달리 RFAA는 작은 분자 및 화학적 변형에 대한 원자 수준 그래프 표현을 단백질 및 핵산에 대한 서열 기반 설명과 통합합니다. 이러한 혁신을 통해 단백질-작은 분자 복합체, 공유 결합된 변형 단백질 및 여러 생체 분자 구성 요소를 포함하는 어셈블리의 정확한 모델링이 가능합니다.\n\n\n\nimage.png\n\n\nRFAA는 세 가지 트랙 아키텍처와 각 유형의 분자에 맞춘 특수 입력 표현을 통합하여 RoseTTAFold2 (RF2) 프레임워크를 향상시킵니다.\n입력 표현: * 단백질 및 핵산: RF2와 유사하게 잔기 또는 염기의 선형 서열로 표현됩니다. * 작은 분자: 노드가 원자에 해당하고 엣지가 결합을 나타내는 원자 그래프로 표현됩니다. 결합 유형(단일, 이중, 삼중, 방향족) 및 입체 화학 정보(키랄성)가 명시적으로 인코딩됩니다. * 공유 변형: 작은 분자 원자를 단백질 잔기에 연결하는 결합이 있는 잔기 그래프의 일부로 표현됩니다.\n멀티트랙 네트워크에는 위에 설명된 입력의 별도 표현을 인코딩하는 1D, 2D 및 3D 트랙이 포함되어 있습니다.\n\n1D 트랙: 아미노산, 핵산 및 화학 원소 유형을 포함한 서열 기반 정보를 인코딩합니다.\n2D 트랙: 화학적 결합 및 원자 간 거리와 같은 쌍별 관계를 인코딩합니다.\n3D 트랙: 각도 및 키랄성을 포함한 공간 구성을 인코딩합니다. 이 트랙은 원자 좌표를 정제하기 위해 반복적으로 업데이트됩니다.\n\n\n\n\nimage.png\n\n\n위에서 설명한 세 개의 트랙에 대한 초기 입력 후, 시스템은 핵산 염기, 아미노산 잔기 및 자유롭게 움직이는 원자의 연결되지 않은 가스로 표현됩니다. 아키텍처의 다양한 블록을 통해 이 연결되지 않은 가스는 그럴듯한 어셈블리 구조로 변모합니다.\nRFAA는 복잡한 생체 분자 어셈블리를 예측하기 위한 강력한 프레임워크를 제공하는 반면, RFdiffusion All-Atom (RFdiffusionAA)은 이러한 기능을 확장하여 작은 분자를 결합하는 단백질을 설계합니다. 확산 기반 작업을 위해 RFAA를 미세 조정함으로써 RFdiffusionAA는 특정 리간드에 맞춤화된 결합 포켓을 가진 드노보 단백질 구조를 생성합니다.\nRFdiffusionAA는 무작위 잔기 분포로 시작하여 디노이징 확산 확률 모델(DDPM)을 사용하여 노이즈 구조를 물리적으로 합당한 단백질 골격으로 반복적으로 정제합니다.\nRFdiffusionAA의 주요 혁신은 다음과 같습니다: * 무조건 설계 (Unconditional Design): 기존 스캐폴드를 수정하는 대신 처음부터 단백질을 생성할 수 있어 설계의 유연성이 향상됩니다. * 구조적 다양성: 동일한 표적 분자에 대해 다양한 설계를 생성하여 기능적 후보를 찾을 가능성을 높입니다. * 높은 호환성: RFdiffusionAA로 생성된 설계는 AlphaFold2의 예측과 잘 일치하여 계산 모델과 실험 모델 간의 강력한 일치를 나타냅니다.\n\n\n 섹션 2: 작은 분자 바인더 설계\n다음 PDB (7v11)에서 항응고제 OQO(청록색)는 혈액 응고 효소 Factor XIa(녹색)와 상호 작용합니다. OQO는 Factor XIa를 억제하고 출혈 위험을 최소화하면서 혈전증(혈액 응고 형성)을 줄입니다.\n\n\n\n7v11_rfdiff.png\n\n\nFactor XAi를 제거하고 OQO에 결합할 새로운 단백질을 설계해 봅시다. PDB 7v11에서 리간드 OQO에 대한 바인더를 생성하려면 다음을 실행합니다:\n예시 (리간드 바인더):\n인수 설명 * inference.deterministic=True는 결과가 재현 가능하도록 사용된 난수 생성기를 시드합니다. 즉, inference.design_startnum=X로 실행하면 동일한 결과가 생성됩니다. torch는 CPU/GPU 아키텍처 간의 재현성을 보장하지 않습니다: https://pytorch.org/docs/stable/notes/randomness.html * inference.num_designs=1은 1개의 설계가 생성되도록 지정합니다. * contigmap.contigs=[\\'150-150\\']은 생성된 단백질의 길이가 150이어야 함을 지정합니다. * diffuser.T=100은 취해진 디노이징 단계 수를 지정합니다.\n예상 출력: * output/ligand_only/sample_0.pdb 설계 PDB * output/ligand_only/sample_0_Xt-1_traj.pdb 부분적으로 디노이징된 중간 구조 * output/ligand_only/sample_0_X0-1_traj.pdb 각 단계에서 네트워크가 만든 실제 정보(ground truth)의 예측\n\n\nCode\nimport time\n\n\n\n\nCode\nstart_time = time.time()\n\n\n\n\nCode\n!singularity run --nv rf_se3_diffusion.sif -u run_inference.py inference.deterministic=True diffuser.T=100 inference.output_prefix=output/ligand_only/sample inference.input_pdb=input/7v11.pdb contigmap.contigs=[\\'150-150\\'] inference.ligand=OQO inference.num_designs=1 inference.design_startnum=0\n\n\n\nINFO:    underlay of /usr/bin/nvidia-smi required more than 50 (496) bind mounts\n\n/bin/sh: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n\nDGL backend not selected or invalid.  Assuming PyTorch for now.\n\nSetting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n\n[14:48:53] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /opt/conda/envs/env/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.13.1.so: cannot open shared object file: No such file or directory\n\nUsing backend: pytorch\n\n/opt/conda/envs/env/lib/python3.9/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'aa': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n\n  warnings.warn(msg, UserWarning)\n\n[2024-10-23 14:48:58,471][inference.model_runners][INFO] - Reading checkpoint from RFDiffusionAA_paper_weights.pt\n\nloading RFDiffusionAA_paper_weights.pt\n\nloaded RFDiffusionAA_paper_weights.pt\n\nOVERRIDING: You are changing diffuser.T from the value this model was trained with.\n\n[2024-10-23 14:49:04,423][inference.model_runners][INFO] - Loading checkpoint.\n\n[2024-10-23 14:49:04,728][diffusion][INFO] - No IGSO3 cache found at /content/rf_diffusion_all_atom/cached_schedules/T_100_omega_1000_min_sigma_0_02_min_b_1_5_max_b_2_5_schedule_linear.pkl.\n\n[2024-10-23 14:49:04,729][diffusion][INFO] - Calculating IGSO3.\n\n[2024-10-23 14:51:26,713][__main__][INFO] - Making design output/ligand_only/sample_0\n\n[2024-10-23 14:51:26,714][__main__][INFO] - making design 0 of 0:1\n\n[2024-10-23 14:51:26,714][inference.model_runners][INFO] - Using contig: ['150-150']\n\n==============================\n\n*** Open Babel Error  in openLib\n\n  /usr/lib/openbabel/3.1.0/png2format.so did not load properly.\n\n Error: libXrender.so.1: cannot open shared object file: No such file or directory\n\nWith this beta schedule (linear schedule, beta_0 = 0.02, beta_T = 0.14), alpha_bar_T = 0.00022250170877669007\n\nCalculating chi_beta_T dictionary...\n\nDone calculating chi_beta_T dictionaries. They are now cached.\n\nDone calculating chi_beta_T, chi_alphas_T, and chi_abars_T dictionaries.\n\n[2024-10-23 14:53:57,019][inference.model_runners][INFO] - 14:53:57: Timestep 100, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:55:40,755][inference.model_runners][INFO] - 14:55:40: Timestep 99, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:57:26,554][inference.model_runners][INFO] - 14:57:26: Timestep 98, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 14:59:08,414][inference.model_runners][INFO] - 14:59:08: Timestep 97, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:00:54,301][inference.model_runners][INFO] - 15:00:54: Timestep 96, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:02:35,334][inference.model_runners][INFO] - 15:02:35: Timestep 95, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:04:12,369][inference.model_runners][INFO] - 15:04:12: Timestep 94, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:05:48,541][inference.model_runners][INFO] - 15:05:48: Timestep 93, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:07:32,936][inference.model_runners][INFO] - 15:07:32: Timestep 92, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:09:16,928][inference.model_runners][INFO] - 15:09:16: Timestep 91, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n[2024-10-23 15:11:03,153][inference.model_runners][INFO] - 15:11:03: Timestep 90, current sequence: ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n\n\n\n\n\nCode\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Execution time: {elapsed_time:.2f} seconds\")\n\n\n\n\nCode\nfrom google.colab import files\n\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/sample_0.pdb')\n\n\n\n\nCode\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/traj/sample_0_Xt-1_traj.pdb')\n\n\n\n\nCode\nfiles.download('/content/rf_diffusion_all_atom/output/ligand_only/traj/sample_0_pX0_traj.pdb')\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget for displaying the full question\nquestion_label = widgets.Label(\n    value='Q1: RFAA extends RoseTTAFold2 by adding a _______ architecture that processes proteins, nucleic acids, small molecules, and covalent modifications.'\n)\n\n# Create fill-in-the-blank input field with wider text box\nquestion_1_input = widgets.Text(\n    placeholder='Enter your answer here',\n    layout=widgets.Layout(width='600px')  # Adjust the width here\n)\n\n# Create submit button for fill-in-the-blank\nsubmit_button_1 = widgets.Button(description=\"Submit Answer\")\n\n# Define what happens when the button is clicked\ndef on_button_clicked_1(b):\n    if question_1_input.value.lower() == 'three-track':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button_1.on_click(on_button_clicked_1)\n\n# Display the question label, input field, and submit button\ndisplay(question_label, question_1_input, submit_button_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget to display the full question\nquestion_label = widgets.Label(\n    value=\"Q2: How does RFdiffusionAA generate binding pockets for small molecules?\"\n)\n\n# Create a multiple-choice question\nquestion_2 = widgets.RadioButtons(\n    options=['A) By modifying pre-existing protein structures.',\n             'B) By using random selection of protein scaffolds.',\n             'C) By iteratively refining random residue distributions around target small molecules.',\n             'D) By relying on classical docking methods.'],\n    disabled=False,\n    layout=widgets.Layout(width='90%')  # Adjusting width for better display\n)\n\n# Create a submit button\nsubmit_button = widgets.Button(description=\"Submit Answer\", layout=widgets.Layout(width='30%'))\n\n# Define what happens when the button is clicked\ndef on_button_clicked(b):\n    if question_2.value == 'C) By iteratively refining random residue distributions around target small molecules.':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button.on_click(on_button_clicked)\n\n# Group the question, answer options, and button in a VBox layout\nquiz_layout = widgets.VBox([question_label, question_2, submit_button])\n\n# Display the question, options, and button\ndisplay(quiz_layout)\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget for displaying the full question\nquestion_label = widgets.Label(\n    value='Q3: One of the key advantages of RFdiffusionAA is that it generates proteins from _______ rather than modifying existing scaffolds.'\n)\n\n# Create fill-in-the-blank input field with wider text box\nquestion_3_input = widgets.Text(\n    placeholder='Enter your answer here',\n    layout=widgets.Layout(width='600px')  # Adjust the width here\n)\n\n# Create submit button for fill-in-the-blank\nsubmit_button_3 = widgets.Button(description=\"Submit Answer\")\n\n# Define what happens when the button is clicked\ndef on_button_clicked_3(b):\n    if question_3_input.value.lower() == 'scratch':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button_3.on_click(on_button_clicked_3)\n\n# Display the question label, input field, and submit button\ndisplay(question_label, question_3_input, submit_button_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n#@markdown **&lt;font color='#e59454'&gt;Question Time! Run to Show Question**\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create a Label widget to display the full question\nquestion_label = widgets.Label(\n    value=\"Q4: What do the encodings represent for the 2D track in RFAA?\"\n)\n\n# Create a multiple-choice question\nquestion_2 = widgets.RadioButtons(\n    options=['A) A 3D voxel grid representing the 3D spatial arrangement of atoms in the molecule.',\n             'B) A sequence of residues encoded using one-hot encoding for amino acids.',\n             'C) The pairwise relationship between inputs such as chemical bonds and distances between atoms.',\n             'D) A graph representation that captures molecular interactions in a graph neural network framework.'],\n    disabled=False,\n    layout=widgets.Layout(width='90%')  # Adjusting width for better display\n)\n\n# Create a submit button\nsubmit_button = widgets.Button(description=\"Submit Answer\", layout=widgets.Layout(width='30%'))\n\n# Define what happens when the button is clicked\ndef on_button_clicked(b):\n    if question_2.value == 'C) The pairwise relationship between inputs such as chemical bonds and distances between atoms.':\n        print(\"Correct!\")\n    else:\n        print(\"Try again!\")\n\nsubmit_button.on_click(on_button_clicked)\n\n# Group the question, answer options, and button in a VBox layout\nquiz_layout = widgets.VBox([question_label, question_2, submit_button])\n\n# Display the question, options, and button\ndisplay(quiz_layout)"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html",
    "title": "  섹션 1: 소개 – 약간의 역사",
    "section": "",
    "text": "##홈페이지로 돌아가기\n이 튜토리얼은 Andrej Karpathy의 비디오와 노트북을 각색했습니다.\n목표: 이 워크숍을 마치면 다음을 수행할 수 있습니다:\n\n자연어 데이터셋 준비 및 분석\n학습 루프에서 자동 회귀(autoregressive) 학습 체계 공식화\n언어 모델 정의 및 인스턴스화하고 아키텍처를 주요 언어 모델과 비교\n영어 언어에 대해 언어 모델 학습 후, 단백질 ‘언어’(아미노산 서열의 fasta 파일)에 대해 학습\n\n작업 내용을 저장하려면 이 노트북의 사본을 개인 Google 드라이브에 저장하세요.\n 목차 \n섹션 1: 소개 – 약간의 역사\n섹션 2: 데이터셋 준비\n\n토큰화를 사용하여 문자를 정수 표현으로 매핑\n다음 토큰 예측 학습 체계를 위한 데이터 포맷팅\n데이터를 학습(train) 및 검증(validation) 분할로 나누기\n\n섹션 3: 모델\n\n신경망 하이퍼파라미터 구성\n트랜스포머 블록의 주요 구성 요소 정의(어텐션, 선형 레이어, 피드포워드 레이어)\n\n섹션 4: 학습\n\n학습 루프 정의\n학습 및 검증 세트 모두에서 성능 평가\n\n\n\n\n\nCode\n# imports\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n!wget --no-check-certificate 'https://drive.google.com/uc?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh' -O proteinPoem.gif\n\n\n\n--2024-12-10 02:36:30--  https://drive.google.com/uc?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh\nResolving drive.google.com (drive.google.com)... 173.194.194.101, 173.194.194.139, 173.194.194.138, ...\nConnecting to drive.google.com (drive.google.com)|173.194.194.101|:443... connected.\nHTTP request sent, awaiting response... 303 See Other\nLocation: https://drive.usercontent.google.com/download?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh [following]\n--2024-12-10 02:36:30--  https://drive.usercontent.google.com/download?id=1HuiOrdFmgggUDgOGOXcsV8eHvN56jweh\nResolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.183.132, 2607:f8b0:4001:c64::84\nConnecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.183.132|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 163525 (160K) [image/gif]\nSaving to: ‘proteinPoem.gif’\n\nproteinPoem.gif     100%[===================&gt;] 159.69K  --.-KB/s    in 0.003s  \n\n2024-12-10 02:36:32 (61.6 MB/s) - ‘proteinPoem.gif’ saved [163525/163525]\n\n\n\n\n\n\n\nChatGPT가 전 세계를 강타했습니다! 텍스트 쿼리로부터 텍스트 응답을 생성하는 AI와 상호작용할 수 있는 시스템입니다. ChatGPT를 사용해 보면 텍스트가 왼쪽에서 오른쪽으로 순차적으로 생성되는 것을 볼 수 있습니다.\n\n\n\n그리고 같은 질문을 두 번 하면 약간 다른 대답을 볼 수 있습니다. 이는 ChatGPT가 확률론적 시스템(probabilistic system)이어서 하나의 프롬프트에 대해 여러 가지 반응을 줄 수 있기 때문입니다.\nChatGPT는 단어/문자/토큰의 시퀀스를 모델링하고 자연어에서 특정 단어가 서로 어떻게 뒤따르는지 학습하기 때문에 언어 모델이라고 부릅니다. ChatGPT 내부의 신경망은 트랜스포머(transformer)입니다. 더 구체적으로 말하면, 생성적 사전 학습 트랜스포머(Generatively Pretrained Transformer, GPT)로 알려진 트랜스포머 디코더입니다. 이 아키텍처를 제안한 원문 논문은 “Attention is All You Need”이며, 출판된 지 5년 만에 거의 모든 AI 응용 분야를 장악했습니다.\n단일 colab 노트북에서 ChatGPT를 바닥부터 구축하는 것은 불가능하지만, 몇 가지 핵심 개념을 배우기 위해 작은 트랜스포머 기반 언어 모델을 구축하고 학습시킬 수 있습니다.\n\n\n\n이 워크숍에서는 셰익스피어의 모든 작품을 하나의 파일(약 1MB)로 연결한 tiny shakespeare 데이터셋을 사용합니다. 이 문자들이 어떻게 서로 뒤따르는지 모델링하여 학습된 텍스트와 유사해 보이는 텍스트를 생성할 수 있기를 바랍니다.\n참고: 먼저 셰익스피어 데이터셋으로 이 노트북을 완료하는 것을 권장합니다. 그러나 토큰화(Tokenization) 방법(아래 설명됨)은 txt 파일(예: 영어, 스페인어, 중국어, 단백질 서열 등)의 대부분의 언어 유형에 대해 일반화할 수 있습니다. 이 노트북을 두 번째로 실행할 때는 원하는 txt 파일을 자유롭게 사용해 보세요.\n단백질 서열을 생성하도록 모델을 학습시키려면 아래의 두 번째 wget 명령의 주석을 해제하세요. 그러면 PDB 아카이브의 모든 항목에 대한 FASTA 형식의 서열이 포함된 파일이 다운로드됩니다.\n\n\nCode\ntorch.manual_seed(1337)\n\n# shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# protein dataset. I suggest first running model with shakespeare dataset\n#!wget https://files.wwpdb.org/pub/pdb/derived_data/pdb_seqres.txt\n\n\n--2024-11-25 15:43:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt             0%[                    ]       0  --.-KB/s               input.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.05s   \n\n2024-11-25 15:43:34 (22.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n\n\n\n\nCode\n# lets inspect our dataset\n# if training on protein sequences... replace `input.txt` with `pdb_seqres.txt`\nwith open('input.txt', 'r', encoding='utf-8') as f:\n  text = f.read()\n\n\n\n\nCode\n# roughly 1M characters\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nCode\n# first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n텍스트에 고유한 문자가 몇 개나 있나요?\n\n\nCode\n# here are all the unique characters that occur in text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('unique characters:')\nprint(''.join(chars))\nprint()\nprint('Vocab size:', vocab_size)\n\n\nunique characters:\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n\nVocab size: 65\n\n\n\n\n이제 셰익스피어의 전체 학습 세트를 토큰화합니다. 토크나이저는 원시 텍스트 데이터를 토큰이라는 더 작은 단위로 변환합니다. 텍스트를 개별 토큰으로 분해하여 처리하는데, 토크나이저의 설정에 따라 단어, 하위 단어 또는 문자를 나타낼 수 있습니다. 토크나이저는 토큰을 숫자 인덱스에 매핑하는 어휘(vocabulary)를 구축하여 모델의 수치적 표현으로 텍스트 데이터를 변환하는 것을 용이하게 합니다.\n\n\n\n토크나이저는 어휘 외 단어 처리, 시퀀스 경계를 위한 특수 토큰 포함과 같은 작업을 처리하며, 형태학적으로 풍부한 언어나 희귀 단어에 대해 하위 단어 토큰화 기술을 활용할 수 있습니다. 궁극적으로 텍스트 데이터를 모델이 학습하고 처리하기에 적합한 형식으로 변환하여 언어 모델 학습을 위해 준비합니다. 여기서는 모든 고유/개별 문자를 하나의 정수에 매핑하는 매우 기본적인 토크나이저를 사용할 것입니다.\n\n\nCode\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# sanity check that our decoder can properly decode our encoder\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n전체 텍스트 데이터셋을 인코딩하여 torch 텐서에 저장해 봅시다. 이제 전체 텍스트 데이터셋은 단일 정수 시퀀스로 늘어납니다.\n\n\nCode\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will be provided to GPT in this format\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\n학습할 준비를 합시다! 전체 텍스트를 한꺼번에 입력하지 않을 것입니다. 계산 비용이 너무 많이 들고 상대적으로 더 나쁜 일반화로 이어질 수 있기 때문입니다. 대신 데이터셋에서 무작위 청크를 샘플링하여 한 번에 데이터 청크로만 학습합니다. 이 청크의 최대 길이를 block size라고 하며, 이것이 우리가 정의할 첫 번째 하이퍼파라미터입니다.\n\n\nCode\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\nCode\nblock_size = 8\ntrain_data[:block_size+1] # first 9 characters from training set\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n언어 모델은 이전 토큰 시퀀스가 주어졌을 때 다음 토큰을 예측하는 임무를 맡습니다. 학습 데이터를 입력-출력 쌍으로 분할합니다. 여기서 입력 x는 토큰 시퀀스로 구성되고 출력 y는 텍스트 바로 뒤에 오는 토큰을 나타냅니다.\n따라서 위의 예에서 모델은 이전 토큰 순서 18, 47, 56, 57, 58, 1, 15, 47,가 주어졌을 때 토큰 58을 예측하는 임무를 맡습니다.\n하지만 1개의 예측 작업 대신 이를 8개의 예측 작업으로 늘릴 수 있습니다! 이 청크만으로 모델은 다음을 예측하도록 학습될 수 있습니다: * 입력 18 및 출력 47 * 입력 18, 47 및 출력 56 * 입력 18, 47, 56 및 출력 57 * 등…\n각 위치를 반복하고 문맥-목표(context-target) 쌍을 형성함으로써 모델은 텍스트 데이터의 순차적 특성을 이해하고 그에 따라 예측을 수행하도록 학습되어 언어 패턴 학습 및 일관된 텍스트 시퀀스 생성을 용이하게 합니다. 이 접근 방식은 학습 데이터의 양을 늘려 학습 효율성을 높일 뿐만 아니라 모델을 다양한 문맥 길이에 노출시켜 짧은 시퀀스부터 긴 시퀀스까지 학습할 수 있게 합니다. 추론 또는 생성 중에 모델은 최소 1글자의 문맥을 제공받아 블록 크기까지 예측할 수 있습니다. 그러나 문맥이 모델이 학습된 최대 길이인 블록 크기를 초과하면 잘라내기(truncation)가 필요합니다. 잘라내기는 입력 시퀀스 길이가 모델의 용량을 초과하지 않도록 보장하여 학습 프로세스와의 일관성을 유지하고 모델의 기능 내에서 효과적인 예측 생성을 가능하게 합니다.\n\n\nCode\n# training on each segment is actually training on multiple sets of characters\nx = train_data[:block_size] # inputs to transformer\ny = train_data[1:block_size+1] # next block size characters (offset by 1). The targets for each input\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n입력 텍스트 데이터의 이러한 순차적 특성을 종종 토큰화된 데이터셋 텐서의 시간 차원(time dimension)이라고 합니다. 시간 차원은 입력 텍스트가 언어 모델에 의해 처리되는 방식에 영향을 미칩니다. 학습 중에 모델은 “시간” 차원을 따라 토큰을 반복하며 시퀀스 내의 각 토큰의 문맥을 고려합니다. 마찬가지로 추론 또는 예측 중에 모델은 시퀀스의 선행 토큰을 기반으로 한 번에 하나의 토큰씩 순차적으로 출력 토큰을 생성합니다.\n텐서의 시간 차원을 살펴보았고 이제 배치 차원(batch dimension)을 살펴보겠습니다. 실제로는 실제로 트랜스포머에 텍스트 배치를 한 번에 공급할 것입니다. 이를 통해 GPU를 최대한 활용하여 여러 데이터 청크를 병렬로 처리할 수 있습니다. 참고로 병렬로 처리되는 데이터 청크 간에는 “통신”이 없습니다. 각각은 독립적으로 수행됩니다.\n\n\nCode\nbatch_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb, yb = get_batch('train')\nprint('input intro transformer:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninput intro transformer:\ntorch.Size([16, 8])\ntensor([[ 1, 39, 52, 42,  1, 45, 43, 50],\n        [ 1, 58, 46, 39, 58,  1, 42, 53],\n        [ 1, 61, 53, 59, 50, 42,  1, 21],\n        [59, 57, 40, 39, 52, 42,  1, 40],\n        [52, 42,  8,  0,  0, 23, 21, 26],\n        [45, 53, 42, 57,  0, 23, 43, 43],\n        [52,  1, 61, 39, 57,  1, 51, 53],\n        [39, 49, 12,  1, 27,  1, 58, 56],\n        [53, 44,  1, 57, 54, 43, 43, 41],\n        [57, 53, 52, 57,  8,  0,  0, 25],\n        [ 1, 42, 43, 44, 43, 41, 58,  1],\n        [21,  1, 61, 39, 52, 42, 43, 56],\n        [43, 43, 51,  5, 42,  1, 40, 59],\n        [45, 50, 63,  1, 52, 53, 61, 12],\n        [52, 53, 58,  8,  0, 25, 63,  1],\n        [53, 58,  6,  1, 51, 63,  1, 50]])\ntargets:\ntorch.Size([16, 8])\ntensor([[39, 52, 42,  1, 45, 43, 50, 42],\n        [58, 46, 39, 58,  1, 42, 53,  1],\n        [61, 53, 59, 50, 42,  1, 21,  1],\n        [57, 40, 39, 52, 42,  1, 40, 47],\n        [42,  8,  0,  0, 23, 21, 26, 19],\n        [53, 42, 57,  0, 23, 43, 43, 54],\n        [ 1, 61, 39, 57,  1, 51, 53, 56],\n        [49, 12,  1, 27,  1, 58, 56, 39],\n        [44,  1, 57, 54, 43, 43, 41, 46],\n        [53, 52, 57,  8,  0,  0, 25, 17],\n        [42, 43, 44, 43, 41, 58,  1, 53],\n        [ 1, 61, 39, 52, 42, 43, 56,  6],\n        [43, 51,  5, 42,  1, 40, 59, 56],\n        [50, 63,  1, 52, 53, 61, 12,  0],\n        [53, 58,  8,  0, 25, 63,  1, 61],\n        [58,  6,  1, 51, 63,  1, 50, 53]])\n----\nwhen input is [1] the target: 39\nwhen input is [1, 39] the target: 52\nwhen input is [1, 39, 52] the target: 42\nwhen input is [1, 39, 52, 42] the target: 1\nwhen input is [1, 39, 52, 42, 1] the target: 45\nwhen input is [1, 39, 52, 42, 1, 45] the target: 43\nwhen input is [1, 39, 52, 42, 1, 45, 43] the target: 50\nwhen input is [1, 39, 52, 42, 1, 45, 43, 50] the target: 42\nwhen input is [1] the target: 58\nwhen input is [1, 58] the target: 46\nwhen input is [1, 58, 46] the target: 39\nwhen input is [1, 58, 46, 39] the target: 58\nwhen input is [1, 58, 46, 39, 58] the target: 1\nwhen input is [1, 58, 46, 39, 58, 1] the target: 42\nwhen input is [1, 58, 46, 39, 58, 1, 42] the target: 53\nwhen input is [1, 58, 46, 39, 58, 1, 42, 53] the target: 1\nwhen input is [1] the target: 61\nwhen input is [1, 61] the target: 53\nwhen input is [1, 61, 53] the target: 59\nwhen input is [1, 61, 53, 59] the target: 50\nwhen input is [1, 61, 53, 59, 50] the target: 42\nwhen input is [1, 61, 53, 59, 50, 42] the target: 1\nwhen input is [1, 61, 53, 59, 50, 42, 1] the target: 21\nwhen input is [1, 61, 53, 59, 50, 42, 1, 21] the target: 1\nwhen input is [59] the target: 57\nwhen input is [59, 57] the target: 40\nwhen input is [59, 57, 40] the target: 39\nwhen input is [59, 57, 40, 39] the target: 52\nwhen input is [59, 57, 40, 39, 52] the target: 42\nwhen input is [59, 57, 40, 39, 52, 42] the target: 1\nwhen input is [59, 57, 40, 39, 52, 42, 1] the target: 40\nwhen input is [59, 57, 40, 39, 52, 42, 1, 40] the target: 47\nwhen input is [52] the target: 42\nwhen input is [52, 42] the target: 8\nwhen input is [52, 42, 8] the target: 0\nwhen input is [52, 42, 8, 0] the target: 0\nwhen input is [52, 42, 8, 0, 0] the target: 23\nwhen input is [52, 42, 8, 0, 0, 23] the target: 21\nwhen input is [52, 42, 8, 0, 0, 23, 21] the target: 26\nwhen input is [52, 42, 8, 0, 0, 23, 21, 26] the target: 19\nwhen input is [45] the target: 53\nwhen input is [45, 53] the target: 42\nwhen input is [45, 53, 42] the target: 57\nwhen input is [45, 53, 42, 57] the target: 0\nwhen input is [45, 53, 42, 57, 0] the target: 23\nwhen input is [45, 53, 42, 57, 0, 23] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43, 43] the target: 54\nwhen input is [52] the target: 1\nwhen input is [52, 1] the target: 61\nwhen input is [52, 1, 61] the target: 39\nwhen input is [52, 1, 61, 39] the target: 57\nwhen input is [52, 1, 61, 39, 57] the target: 1\nwhen input is [52, 1, 61, 39, 57, 1] the target: 51\nwhen input is [52, 1, 61, 39, 57, 1, 51] the target: 53\nwhen input is [52, 1, 61, 39, 57, 1, 51, 53] the target: 56\nwhen input is [39] the target: 49\nwhen input is [39, 49] the target: 12\nwhen input is [39, 49, 12] the target: 1\nwhen input is [39, 49, 12, 1] the target: 27\nwhen input is [39, 49, 12, 1, 27] the target: 1\nwhen input is [39, 49, 12, 1, 27, 1] the target: 58\nwhen input is [39, 49, 12, 1, 27, 1, 58] the target: 56\nwhen input is [39, 49, 12, 1, 27, 1, 58, 56] the target: 39\nwhen input is [53] the target: 44\nwhen input is [53, 44] the target: 1\nwhen input is [53, 44, 1] the target: 57\nwhen input is [53, 44, 1, 57] the target: 54\nwhen input is [53, 44, 1, 57, 54] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43, 43] the target: 41\nwhen input is [53, 44, 1, 57, 54, 43, 43, 41] the target: 46\nwhen input is [57] the target: 53\nwhen input is [57, 53] the target: 52\nwhen input is [57, 53, 52] the target: 57\nwhen input is [57, 53, 52, 57] the target: 8\nwhen input is [57, 53, 52, 57, 8] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0, 0] the target: 25\nwhen input is [57, 53, 52, 57, 8, 0, 0, 25] the target: 17\nwhen input is [1] the target: 42\nwhen input is [1, 42] the target: 43\nwhen input is [1, 42, 43] the target: 44\nwhen input is [1, 42, 43, 44] the target: 43\nwhen input is [1, 42, 43, 44, 43] the target: 41\nwhen input is [1, 42, 43, 44, 43, 41] the target: 58\nwhen input is [1, 42, 43, 44, 43, 41, 58] the target: 1\nwhen input is [1, 42, 43, 44, 43, 41, 58, 1] the target: 53\nwhen input is [21] the target: 1\nwhen input is [21, 1] the target: 61\nwhen input is [21, 1, 61] the target: 39\nwhen input is [21, 1, 61, 39] the target: 52\nwhen input is [21, 1, 61, 39, 52] the target: 42\nwhen input is [21, 1, 61, 39, 52, 42] the target: 43\nwhen input is [21, 1, 61, 39, 52, 42, 43] the target: 56\nwhen input is [21, 1, 61, 39, 52, 42, 43, 56] the target: 6\nwhen input is [43] the target: 43\nwhen input is [43, 43] the target: 51\nwhen input is [43, 43, 51] the target: 5\nwhen input is [43, 43, 51, 5] the target: 42\nwhen input is [43, 43, 51, 5, 42] the target: 1\nwhen input is [43, 43, 51, 5, 42, 1] the target: 40\nwhen input is [43, 43, 51, 5, 42, 1, 40] the target: 59\nwhen input is [43, 43, 51, 5, 42, 1, 40, 59] the target: 56\nwhen input is [45] the target: 50\nwhen input is [45, 50] the target: 63\nwhen input is [45, 50, 63] the target: 1\nwhen input is [45, 50, 63, 1] the target: 52\nwhen input is [45, 50, 63, 1, 52] the target: 53\nwhen input is [45, 50, 63, 1, 52, 53] the target: 61\nwhen input is [45, 50, 63, 1, 52, 53, 61] the target: 12\nwhen input is [45, 50, 63, 1, 52, 53, 61, 12] the target: 0\nwhen input is [52] the target: 53\nwhen input is [52, 53] the target: 58\nwhen input is [52, 53, 58] the target: 8\nwhen input is [52, 53, 58, 8] the target: 0\nwhen input is [52, 53, 58, 8, 0] the target: 25\nwhen input is [52, 53, 58, 8, 0, 25] the target: 63\nwhen input is [52, 53, 58, 8, 0, 25, 63] the target: 1\nwhen input is [52, 53, 58, 8, 0, 25, 63, 1] the target: 61\nwhen input is [53] the target: 58\nwhen input is [53, 58] the target: 6\nwhen input is [53, 58, 6] the target: 1\nwhen input is [53, 58, 6, 1] the target: 51\nwhen input is [53, 58, 6, 1, 51] the target: 63\nwhen input is [53, 58, 6, 1, 51, 63] the target: 1\nwhen input is [53, 58, 6, 1, 51, 63, 1] the target: 50\nwhen input is [53, 58, 6, 1, 51, 63, 1, 50] the target: 53\n\n\n요약하면 위의 16x8 입력 배열에는 총 128개의 예제가 포함되어 있으며 이들은 완전히 독립적입니다.\n\n\n\n\n이제 데이터가 준비되었으므로 신경망에 입력해 봅시다!\n이 모델은 트랜스포머 아키텍처를 사용하여 구현된 바이그램(Bigram) 언어 모델입니다. 주어진 입력 시퀀스를 기반으로 문자별로 텍스트를 생성합니다. 모델은 4개의 트랜스포머 블록으로 구성되며, 각 블록에는 멀티 헤드 자기 어텐션(multi-head self-attention) 메커니즘과 피드포워드 신경망이 포함되어 있고 그 뒤에 레이어 정규화가 이어집니다. 모델의 출력은 시퀀스의 다음 문자를 예측하기 위해 투영됩니다. 학습 중에 예측된 문자와 실제 문자 간의 교차 엔트로피 손실을 최소화합니다. 마지막으로 학습된 모델은 예측된 확률 분포에 기반하여 문자를 샘플링하여 새 텍스트를 생성하는 데 사용됩니다.\n\n\n\nimage.png\n\n\n아래 모델과 GPT는 모두 트랜스포머 아키텍처를 기반으로 합니다. 몇 가지 유사점과 차이점은 다음과 같습니다: * 레이어 수: GPT 모델은 일반적으로 많은 수의 트랜스포머 블록으로 구성됩니다(GPT1에는 12개, GPT2-L에는 48개, GPT3-L에는 384개의 블록이 있음). 우리 모델에는 4개의 블록이 있습니다. * GPT는 토큰별로 텍스트를 생성합니다(여기서 토큰은 일반적으로 단어 조각임). 우리 모델은 문자별로 텍스트를 생성합니다. * GPT는 가변 길이 문맥 시퀀스를 처리합니다. 우리 모델은 고정 크기 문맥 창(블록 크기)을 취하고 이 문맥을 기반으로 다음 토큰을 생성합니다.\n\n\nCode\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\n\nestimate_loss() 함수를 정의하여 학습 및 검증 데이터 모두에서 NN의 손실을 평가합니다. 모델은 일시적으로 평가 모드로 전환되어 그래디언트 계산을 비활성화하여 계산 효율성을 높입니다. 학습 및 검증 분할을 반복하여 데이터 배치를 검색하고 손실을 계산합니다. 각 분할에 대한 평균 손실을 계산한 후 모델은 학습 모드로 되돌아갑니다. 마지막으로 함수는 두 분할에 대한 평균 손실이 포함된 사전을 반환합니다. 이는 기계 학습 작업의 평가에 중요한 매개변수를 업데이트하지 않고 모델의 성능을 평가하는 데 도움이 됩니다.\n\n\nCode\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\n트랜스포머의 주요 성공 요인은 어텐션(attention)이며, 이는 Head와 MultiHeadAttention에서 정의합니다. Head 클래스는 트랜스포머 아키텍처의 기본 구성 요소인 자기 어텐션(self-attention)의 단일 헤드를 나타냅니다. 키(key), 쿼리(query), 값(value) 표현을 계산하기 위한 선형 레이어(self.key, self.query, self.value)와 마스킹을 위한 삼각형 버퍼를 초기화합니다. 순전파(forward) 메서드에서는 어텐션 점수를 계산하고, 마스킹을 적용하고, 어텐션 가중치를 계산하고, 이러한 가중치를 기반으로 값을 집계합니다.\nMultiHeadAttention 클래스는 병렬로 작동하는 여러 자기 어텐션 헤드를 나타냅니다. 멀티 헤드 어텐션을 수행하기 위해 Head 인스턴스 목록(self.heads)을 생성합니다. 순전파 메서드에서는 각 헤드를 입력 텐서 x에 개별적으로 적용하고 결과를 연결하고 연결된 출력을 투영하고 정규화를 위해 드롭아웃을 적용합니다. 이 클래스를 사용하면 모델이 여러 어텐션 메커니즘을 통해 입력 데이터의 다양한 측면을 동시에 포착하여 표현 능력과 성능을 향상시킬 수 있습니다.\n\n\nCode\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\n\nFeedForward 클래스는 트랜스포머 아키텍처에서 흔히 볼 수 있는 간단한 피드포워드 신경망 레이어를 나타냅니다. 두 개의 선형 레이어와 그 뒤를 잇는 정규화를 위한 ReLU 활성화 및 드롭아웃이 포함되어 있습니다. 순전파 메서드에서는 이러한 연산을 입력 텐서 x에 순차적으로 적용하여 고차원 공간으로 변환하고 비선형성을 도입합니다.\nBlock 클래스는 단일 트랜스포머 블록을 나타내며, 멀티 헤드 자기 어텐션 레이어(self.sa)와 피드포워드 레이어(self.ffwd)로 구성됩니다. 또한 각 하위 레이어 뒤에 레이어 정규화(self.ln1 및 self.ln2)를 통합합니다. 순전파 메서드에서는 입력 텐서 x를 자기 어텐션 레이어와 피드포워드 레이어에 순차적으로 통과시키고, 원래 입력을 각 레이어의 출력에 더하고(잔차 연결), 결과를 레이어 정규화를 사용하여 정규화합니다. 이 블록은 트랜스포머 모델의 핵심 구성 요소를 요약하여 어텐션을 통한 토큰 간 정보 교환과 피드포워드 레이어를 통한 비선형 변환을 모두 촉진합니다.\n\n\nCode\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\nBigramLanguageModel 클래스는 토큰 표현을 위한 임베딩 레이어, 시퀀스 내 토큰 위치를 포착하기 위한 위치 임베딩 레이어, 입력을 처리하기 위한 트랜스포머 블록 스택, 학습 안정을 위한 레이어 정규화, 다음 토큰을 예측하기 위한 선형 레이어로 구성됩니다. 순전파 동안 모델은 토큰을 나타내는 입력 인덱스를 취하고, 임베딩 및 위치 인코딩을 적용하고, 트랜스포머 블록을 통해 처리하고, 다음 토큰의 로짓을 예측합니다. 그런 다음 손실을 계산합니다. 생성을 위해 모델은 예측된 분포에서 반복적으로 샘플링하여 이전 문맥 및 모델에서 생성된 확률을 기반으로 다음 토큰을 선택합니다. 이 접근 방식을 통해 모델은 토큰 시퀀스를 자율적으로 생성할 수 있습니다.\n\n\nCode\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\n모델 인스턴스화하기\n\n\nCode\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n\n0.209729 M parameters\n\n\n\n\n\n코드는 max_iters 반복 횟수만큼 루프를 실행합니다. 매 eval_interval 반복마다(또는 마지막 반복에서) estimate_loss() 함수를 사용하여 학습 및 검증 세트 모두에서 손실을 평가하고 결과를 출력합니다. 그런 다음 get_batch() 함수를 사용하여 학습 세트에서 데이터 배치(xb, yb)를 샘플링합니다. 모델은 입력(xb) 및 목표(yb) 텐서로 호출되어 로짓과 손실을 계산합니다. 옵티마이저의 그래디언트는 set_to_none=True로 영(0)이 됩니다(optimizer.zero_grad()). 이는 메모리 사용을 더 효율적으로 만듭니다. 그패디언트를 계산하기 위해 역전파가 수행됩니다(loss.backward()). 마지막으로 옵티마이저는 계산된 그래디언트를 기반으로 모델 파라미터를 업데이트하기 위해 단계(optimizer.step())를 밟습니다.\n\n\nCode\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n\nstep 0: train loss 4.4166, val loss 4.4138\nstep 100: train loss 2.6565, val loss 2.6660\nstep 200: train loss 2.5054, val loss 2.4991\nstep 300: train loss 2.4156, val loss 2.4250\nstep 400: train loss 2.3504, val loss 2.3638\nstep 500: train loss 2.3134, val loss 2.3332\nstep 600: train loss 2.2550, val loss 2.2668\nstep 700: train loss 2.2216, val loss 2.2324\nstep 800: train loss 2.1821, val loss 2.2038\nstep 900: train loss 2.1458, val loss 2.1676\nstep 1000: train loss 2.1172, val loss 2.1455\nstep 1100: train loss 2.0883, val loss 2.1382\nstep 1200: train loss 2.0587, val loss 2.1036\nstep 1300: train loss 2.0410, val loss 2.0769\nstep 1400: train loss 2.0095, val loss 2.0547\nstep 1500: train loss 1.9951, val loss 2.0612\nstep 1600: train loss 1.9671, val loss 2.0487\nstep 1700: train loss 1.9665, val loss 2.0399\nstep 1800: train loss 1.9223, val loss 2.0177\nstep 1900: train loss 1.9239, val loss 2.0033\nstep 2000: train loss 1.8967, val loss 2.0079\nstep 2100: train loss 1.8890, val loss 1.9935\nstep 2200: train loss 1.8648, val loss 1.9667\nstep 2300: train loss 1.8607, val loss 1.9619\nstep 2400: train loss 1.8465, val loss 1.9414\nstep 2500: train loss 1.8273, val loss 1.9546\nstep 2600: train loss 1.8295, val loss 1.9418\nstep 2700: train loss 1.8314, val loss 1.9478\nstep 2800: train loss 1.8072, val loss 1.9341\nstep 2900: train loss 1.8032, val loss 1.9350\nstep 3000: train loss 1.7968, val loss 1.9243\nstep 3100: train loss 1.7785, val loss 1.9223\nstep 3200: train loss 1.7684, val loss 1.9112\nstep 3300: train loss 1.7700, val loss 1.9143\nstep 3400: train loss 1.7564, val loss 1.8982\nstep 3500: train loss 1.7397, val loss 1.8972\nstep 3600: train loss 1.7348, val loss 1.8899\nstep 3700: train loss 1.7338, val loss 1.8860\nstep 3800: train loss 1.7326, val loss 1.9028\nstep 3900: train loss 1.7289, val loss 1.8715\nstep 4000: train loss 1.7228, val loss 1.8684\nstep 4100: train loss 1.7142, val loss 1.8763\nstep 4200: train loss 1.7030, val loss 1.8693\nstep 4300: train loss 1.7099, val loss 1.8551\nstep 4400: train loss 1.7138, val loss 1.8726\nstep 4500: train loss 1.6938, val loss 1.8540\nstep 4600: train loss 1.6934, val loss 1.8410\nstep 4700: train loss 1.6836, val loss 1.8389\nstep 4800: train loss 1.6751, val loss 1.8453\nstep 4900: train loss 1.6787, val loss 1.8463\nstep 4999: train loss 1.6747, val loss 1.8296\n\n\n학습 후, 0으로 구성된 텐서인 초기 문맥(context)을 제공하여 텍스트 생성을 시연합니다. 문맥 텐서가 0으로 초기화되므로 본질적으로 텍스트 생성의 빈 시작점을 나타냅니다. 언어 모델은 이 빈 문맥을 생성된 시퀀스의 시작으로 사용하고 학습된 패턴과 제공된 문맥을 기반으로 점진적으로 토큰을 생성합니다. 토큰이 생성됨에 따라 문맥 텐서에 추가되어 모델이 진화하는 시퀀스를 기반으로 텍스트를 계속 생성할 수 있습니다.\n모델의 generate 메서드는 주어진 문맥을 기반으로 토큰 시퀀스(max_new_tokens=2000)를 생성하기 위해 호출됩니다. 생성된 시퀀스는 텍스트 형식으로 디코딩되어 출력됩니다. 보시다시피 생성된 텍스트가 완전히 읽을 수 있는 것은 아니지만 모델은 학습 데이터의 일반적인 구조를 포착하는 데 꽤 훌륭한 작업을 수행했습니다.\n\n\nCode\n# generate from the model\n# if generating proteins... if you want to see how 'protein-like' the sequences are, feed them into AlphaFold2 and see if the sequence folds!!\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))\n\n\n\nWhy not call, Clifffay, all there me fear the labluntch but your Cappence agmand to\nToo.\n\nMONSARY:\nGood\nWhere these bappedied Droum, to\nI will depi'gr not love fellow the my breath,\nThan furthon that My love tambled thy followns and the like daming this di;\nAnd and he but thever is very may moung twict the Levings. Consticly\nMe brothers gate?\nNot, now you some, what that my frece fullw'd thou brother,\nAs my live, ashow thyself den; and is thurncts'd wher here's up?\nI would loves minow. Pomfort life, shom Conty-driack: sol, and,\nAnd,--like as may that Kingly testens can xpurthymen, shall where like\nThel, I haffit, thy not loves!\nAnd consent the brody with in in but the carrition.\nCamb'd, take His this you\nruther thine why, I'll mansce's have with a what fell wailt afwifed\nGod stild thou lost; as now, I have this rypengt.\n\nLANTA:\nHe is not Plow; deyelf, good whither\nForth: you, would it: and thus for not\nAnd sabetruess this let old this\nAs kneep ausis, trulow.--greaden of them heavy:\nWhe\n\n\n또한 더 구체적인 문맥에서 텍스트를 생성해 봅시다.\n\n\nCode\n# input context. Change this to whatever you want!\n# if you are generating protein sequences... try inserting the first feww residues of a protein chain!\ninput_context = \"In the house of\"\n\n# encode input\nencoded_context = encode(input_context)\n\n# input into tensor\ncontext = torch.tensor([encoded_context], dtype=torch.long, device=device)\n\n# generate text using context\ngenerated_text = m.generate(context, max_new_tokens=100)[0].tolist()\n\n# decode and print text\nprint(decode(generated_text))\n\n\nIn the house of the brow you not.\n\nDUCHSHOMAND:\nGo denemings given they Comentence,\nSincoud ir, I am your her\nWater\n\n\n\n\n모델의 하이퍼파라미터를 수정하고 각각이 모델의 성능에 어떤 영향을 미치는지 확인해 봅시다!\n질문: n_embd(임베딩 크기)를 64에서 더 높은 값(예: 128 또는 256)으로 늘리세요. 이것이 원래 모델과 비교하여 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Increasing the embedding size allows the model to capture more nuanced patterns in the data. \\n A larger embedding dimension generally improves the model's capacity to understand and generate more complex language features. \\n Final loss should decrease.\")\n\n\n질문: n_layer(트랜스포머 레이어 수)를 4에서 더 높은 값(예: 6 또는 8)으로 늘리세요. 이것이 모델의 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More layers allow the model to learn deeper hierarchical patterns, which is important for capturing long-range dependencies in text. \\n The final loss should decrease due to better capacity to model long-range dependencies.\")\n\n\n질문: n_head(어텐션 헤드 수)를 4에서 더 높은 값(예: 8 또는 16)으로 늘리세요. 이것이 모델 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More attention heads allow the model to capture multiple aspects of relationships between tokens in parallel. \\n Increasing the number of attention heads can improve the model’s ability to focus on different parts of the input simultaneously. \\n The final loss should decrease.\")\n\n\n일반적으로 임베딩 크기, 레이어 수 또는 어텐션 헤드 수를 늘려 트랜스포머 모델을 확장하면 많은 자연어 처리 작업에서 성능이 향상되는 경향이 있습니다. 더 큰 모델은 데이터의 더 복잡한 패턴, 관계 및 의존성을 포착할 수 있는 용량이 더 크기 때문에 이러한 추세가 나타납니다. 파라미터가 많을수록 모델은 학습 데이터에서 더 잘 일반화하고 더 정확하고 일관되며 맥락적으로 풍부한 출력을 생성할 수 있습니다. 그러나 확장에 따른 이득은 특정 지점에서 감소하며, 특히 제한된 데이터나 계산 리소스 내에서는 과적합이나 비효율성을 피하기 위해 하이퍼파라미터를 신중하게 조정해야 합니다.\n관련 논문 * 언어 모델로 5억 년의 진화 시뮬레이션, Hayes et al, 2024 * ESM Cambrian: 비지도 학습으로 단백질의 신비 밝히기, 2024 * 개선된 항체 설계를 위한 항체 생식세열 편향 및 언어 모델에 미치는 영향 해결, 2024 * 단백질 언어 시각화 도구: 언어 모델 시대의 서열 유사성 네트워크, 2024 * 단백질 언어 모델 미세 조정으로 다양한 작업의 예측 향상, 2024 * 긴 문맥 단백질 언어 모델, 2024 * 학습 계산 최적 단백질 언어 모델, 2024 * 프롬프트된 언어 모델을 통한 다목적 제어 가능한 단백질 생성, 2024 * 언어 모델로 단백질 설계, 2024 * 단백질 언어 모델은 상호 작용하는 서열 모티프의 진화 통계 학습, 2024 * p-IgGen: 쌍체 항체 생성 언어 모델, 2024 * 단백질 언어 모델은 계산 최적인가?, 2024 * 특징 재사용 및 확장: 단백질 언어 모델을 통한 전이 학습 이해, 2024 * 구조 조건부 설계를 위한 단백질 언어 모델 적응, 2024 * 단백질 언어 모델에서 고차 상호 작용 복구에 관하여, 2024 * 단백질 언어 모델은 생명의 나무 전반에 걸친 불평등한 서열 샘플링으로 편향됨, 2024 * ESM All-Atom: 통합 분자 모델링을 위한 다중 스케일 단백질 언어 모델, 2024 * 단백질 유도 진화를 위한 지식 인식 강화 언어 모델, 2024 * 멀티 모달 대규모 언어 모델로 단백질 기능 예측 가능, 2024 * 다양한 다운스트림 작업에서 텍스트 통합 단백질 언어 모델 임베딩 및 임베딩 융합 벤치마킹, 2024 * 대규모 쌍체 항체 언어 모델, 2024 * 구조 정보 언어 모델을 통한 단백질 및 항체 복합체의 비지도 진화, 2024 * 기본 페어링으로 항체 언어 모델 개선, 2023 * 대규모 언어 모델은 다양한 계열에 걸쳐 기능적 단백질 서열 생성, 2023 * ProGen2: 단백질 언어 모델의 경계 탐구, 2022 * 항체 설계를 위한 생성적 언어 모델링, 2023 * 진화 규모의 단백질 서열 언어 모델로 정확한 구조 예측 가능, 2023"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#데이터셋-토큰화하기",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#데이터셋-토큰화하기",
    "title": "  섹션 1: 소개 – 약간의 역사",
    "section": "",
    "text": "이제 셰익스피어의 전체 학습 세트를 토큰화합니다. 토크나이저는 원시 텍스트 데이터를 토큰이라는 더 작은 단위로 변환합니다. 텍스트를 개별 토큰으로 분해하여 처리하는데, 토크나이저의 설정에 따라 단어, 하위 단어 또는 문자를 나타낼 수 있습니다. 토크나이저는 토큰을 숫자 인덱스에 매핑하는 어휘(vocabulary)를 구축하여 모델의 수치적 표현으로 텍스트 데이터를 변환하는 것을 용이하게 합니다.\n\n\n\n토크나이저는 어휘 외 단어 처리, 시퀀스 경계를 위한 특수 토큰 포함과 같은 작업을 처리하며, 형태학적으로 풍부한 언어나 희귀 단어에 대해 하위 단어 토큰화 기술을 활용할 수 있습니다. 궁극적으로 텍스트 데이터를 모델이 학습하고 처리하기에 적합한 형식으로 변환하여 언어 모델 학습을 위해 준비합니다. 여기서는 모든 고유/개별 문자를 하나의 정수에 매핑하는 매우 기본적인 토크나이저를 사용할 것입니다.\n\n\nCode\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# sanity check that our decoder can properly decode our encoder\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n전체 텍스트 데이터셋을 인코딩하여 torch 텐서에 저장해 봅시다. 이제 전체 텍스트 데이터셋은 단일 정수 시퀀스로 늘어납니다.\n\n\nCode\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will be provided to GPT in this format\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#학습-준비",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#학습-준비",
    "title": "  섹션 1: 소개 – 약간의 역사",
    "section": "",
    "text": "학습할 준비를 합시다! 전체 텍스트를 한꺼번에 입력하지 않을 것입니다. 계산 비용이 너무 많이 들고 상대적으로 더 나쁜 일반화로 이어질 수 있기 때문입니다. 대신 데이터셋에서 무작위 청크를 샘플링하여 한 번에 데이터 청크로만 학습합니다. 이 청크의 최대 길이를 block size라고 하며, 이것이 우리가 정의할 첫 번째 하이퍼파라미터입니다.\n\n\nCode\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\nCode\nblock_size = 8\ntrain_data[:block_size+1] # first 9 characters from training set\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n언어 모델은 이전 토큰 시퀀스가 주어졌을 때 다음 토큰을 예측하는 임무를 맡습니다. 학습 데이터를 입력-출력 쌍으로 분할합니다. 여기서 입력 x는 토큰 시퀀스로 구성되고 출력 y는 텍스트 바로 뒤에 오는 토큰을 나타냅니다.\n따라서 위의 예에서 모델은 이전 토큰 순서 18, 47, 56, 57, 58, 1, 15, 47,가 주어졌을 때 토큰 58을 예측하는 임무를 맡습니다.\n하지만 1개의 예측 작업 대신 이를 8개의 예측 작업으로 늘릴 수 있습니다! 이 청크만으로 모델은 다음을 예측하도록 학습될 수 있습니다: * 입력 18 및 출력 47 * 입력 18, 47 및 출력 56 * 입력 18, 47, 56 및 출력 57 * 등…\n각 위치를 반복하고 문맥-목표(context-target) 쌍을 형성함으로써 모델은 텍스트 데이터의 순차적 특성을 이해하고 그에 따라 예측을 수행하도록 학습되어 언어 패턴 학습 및 일관된 텍스트 시퀀스 생성을 용이하게 합니다. 이 접근 방식은 학습 데이터의 양을 늘려 학습 효율성을 높일 뿐만 아니라 모델을 다양한 문맥 길이에 노출시켜 짧은 시퀀스부터 긴 시퀀스까지 학습할 수 있게 합니다. 추론 또는 생성 중에 모델은 최소 1글자의 문맥을 제공받아 블록 크기까지 예측할 수 있습니다. 그러나 문맥이 모델이 학습된 최대 길이인 블록 크기를 초과하면 잘라내기(truncation)가 필요합니다. 잘라내기는 입력 시퀀스 길이가 모델의 용량을 초과하지 않도록 보장하여 학습 프로세스와의 일관성을 유지하고 모델의 기능 내에서 효과적인 예측 생성을 가능하게 합니다.\n\n\nCode\n# training on each segment is actually training on multiple sets of characters\nx = train_data[:block_size] # inputs to transformer\ny = train_data[1:block_size+1] # next block size characters (offset by 1). The targets for each input\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n입력 텍스트 데이터의 이러한 순차적 특성을 종종 토큰화된 데이터셋 텐서의 시간 차원(time dimension)이라고 합니다. 시간 차원은 입력 텍스트가 언어 모델에 의해 처리되는 방식에 영향을 미칩니다. 학습 중에 모델은 “시간” 차원을 따라 토큰을 반복하며 시퀀스 내의 각 토큰의 문맥을 고려합니다. 마찬가지로 추론 또는 예측 중에 모델은 시퀀스의 선행 토큰을 기반으로 한 번에 하나의 토큰씩 순차적으로 출력 토큰을 생성합니다.\n텐서의 시간 차원을 살펴보았고 이제 배치 차원(batch dimension)을 살펴보겠습니다. 실제로는 실제로 트랜스포머에 텍스트 배치를 한 번에 공급할 것입니다. 이를 통해 GPU를 최대한 활용하여 여러 데이터 청크를 병렬로 처리할 수 있습니다. 참고로 병렬로 처리되는 데이터 청크 간에는 “통신”이 없습니다. 각각은 독립적으로 수행됩니다.\n\n\nCode\nbatch_size = 16\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\nxb, yb = get_batch('train')\nprint('input intro transformer:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninput intro transformer:\ntorch.Size([16, 8])\ntensor([[ 1, 39, 52, 42,  1, 45, 43, 50],\n        [ 1, 58, 46, 39, 58,  1, 42, 53],\n        [ 1, 61, 53, 59, 50, 42,  1, 21],\n        [59, 57, 40, 39, 52, 42,  1, 40],\n        [52, 42,  8,  0,  0, 23, 21, 26],\n        [45, 53, 42, 57,  0, 23, 43, 43],\n        [52,  1, 61, 39, 57,  1, 51, 53],\n        [39, 49, 12,  1, 27,  1, 58, 56],\n        [53, 44,  1, 57, 54, 43, 43, 41],\n        [57, 53, 52, 57,  8,  0,  0, 25],\n        [ 1, 42, 43, 44, 43, 41, 58,  1],\n        [21,  1, 61, 39, 52, 42, 43, 56],\n        [43, 43, 51,  5, 42,  1, 40, 59],\n        [45, 50, 63,  1, 52, 53, 61, 12],\n        [52, 53, 58,  8,  0, 25, 63,  1],\n        [53, 58,  6,  1, 51, 63,  1, 50]])\ntargets:\ntorch.Size([16, 8])\ntensor([[39, 52, 42,  1, 45, 43, 50, 42],\n        [58, 46, 39, 58,  1, 42, 53,  1],\n        [61, 53, 59, 50, 42,  1, 21,  1],\n        [57, 40, 39, 52, 42,  1, 40, 47],\n        [42,  8,  0,  0, 23, 21, 26, 19],\n        [53, 42, 57,  0, 23, 43, 43, 54],\n        [ 1, 61, 39, 57,  1, 51, 53, 56],\n        [49, 12,  1, 27,  1, 58, 56, 39],\n        [44,  1, 57, 54, 43, 43, 41, 46],\n        [53, 52, 57,  8,  0,  0, 25, 17],\n        [42, 43, 44, 43, 41, 58,  1, 53],\n        [ 1, 61, 39, 52, 42, 43, 56,  6],\n        [43, 51,  5, 42,  1, 40, 59, 56],\n        [50, 63,  1, 52, 53, 61, 12,  0],\n        [53, 58,  8,  0, 25, 63,  1, 61],\n        [58,  6,  1, 51, 63,  1, 50, 53]])\n----\nwhen input is [1] the target: 39\nwhen input is [1, 39] the target: 52\nwhen input is [1, 39, 52] the target: 42\nwhen input is [1, 39, 52, 42] the target: 1\nwhen input is [1, 39, 52, 42, 1] the target: 45\nwhen input is [1, 39, 52, 42, 1, 45] the target: 43\nwhen input is [1, 39, 52, 42, 1, 45, 43] the target: 50\nwhen input is [1, 39, 52, 42, 1, 45, 43, 50] the target: 42\nwhen input is [1] the target: 58\nwhen input is [1, 58] the target: 46\nwhen input is [1, 58, 46] the target: 39\nwhen input is [1, 58, 46, 39] the target: 58\nwhen input is [1, 58, 46, 39, 58] the target: 1\nwhen input is [1, 58, 46, 39, 58, 1] the target: 42\nwhen input is [1, 58, 46, 39, 58, 1, 42] the target: 53\nwhen input is [1, 58, 46, 39, 58, 1, 42, 53] the target: 1\nwhen input is [1] the target: 61\nwhen input is [1, 61] the target: 53\nwhen input is [1, 61, 53] the target: 59\nwhen input is [1, 61, 53, 59] the target: 50\nwhen input is [1, 61, 53, 59, 50] the target: 42\nwhen input is [1, 61, 53, 59, 50, 42] the target: 1\nwhen input is [1, 61, 53, 59, 50, 42, 1] the target: 21\nwhen input is [1, 61, 53, 59, 50, 42, 1, 21] the target: 1\nwhen input is [59] the target: 57\nwhen input is [59, 57] the target: 40\nwhen input is [59, 57, 40] the target: 39\nwhen input is [59, 57, 40, 39] the target: 52\nwhen input is [59, 57, 40, 39, 52] the target: 42\nwhen input is [59, 57, 40, 39, 52, 42] the target: 1\nwhen input is [59, 57, 40, 39, 52, 42, 1] the target: 40\nwhen input is [59, 57, 40, 39, 52, 42, 1, 40] the target: 47\nwhen input is [52] the target: 42\nwhen input is [52, 42] the target: 8\nwhen input is [52, 42, 8] the target: 0\nwhen input is [52, 42, 8, 0] the target: 0\nwhen input is [52, 42, 8, 0, 0] the target: 23\nwhen input is [52, 42, 8, 0, 0, 23] the target: 21\nwhen input is [52, 42, 8, 0, 0, 23, 21] the target: 26\nwhen input is [52, 42, 8, 0, 0, 23, 21, 26] the target: 19\nwhen input is [45] the target: 53\nwhen input is [45, 53] the target: 42\nwhen input is [45, 53, 42] the target: 57\nwhen input is [45, 53, 42, 57] the target: 0\nwhen input is [45, 53, 42, 57, 0] the target: 23\nwhen input is [45, 53, 42, 57, 0, 23] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43] the target: 43\nwhen input is [45, 53, 42, 57, 0, 23, 43, 43] the target: 54\nwhen input is [52] the target: 1\nwhen input is [52, 1] the target: 61\nwhen input is [52, 1, 61] the target: 39\nwhen input is [52, 1, 61, 39] the target: 57\nwhen input is [52, 1, 61, 39, 57] the target: 1\nwhen input is [52, 1, 61, 39, 57, 1] the target: 51\nwhen input is [52, 1, 61, 39, 57, 1, 51] the target: 53\nwhen input is [52, 1, 61, 39, 57, 1, 51, 53] the target: 56\nwhen input is [39] the target: 49\nwhen input is [39, 49] the target: 12\nwhen input is [39, 49, 12] the target: 1\nwhen input is [39, 49, 12, 1] the target: 27\nwhen input is [39, 49, 12, 1, 27] the target: 1\nwhen input is [39, 49, 12, 1, 27, 1] the target: 58\nwhen input is [39, 49, 12, 1, 27, 1, 58] the target: 56\nwhen input is [39, 49, 12, 1, 27, 1, 58, 56] the target: 39\nwhen input is [53] the target: 44\nwhen input is [53, 44] the target: 1\nwhen input is [53, 44, 1] the target: 57\nwhen input is [53, 44, 1, 57] the target: 54\nwhen input is [53, 44, 1, 57, 54] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43] the target: 43\nwhen input is [53, 44, 1, 57, 54, 43, 43] the target: 41\nwhen input is [53, 44, 1, 57, 54, 43, 43, 41] the target: 46\nwhen input is [57] the target: 53\nwhen input is [57, 53] the target: 52\nwhen input is [57, 53, 52] the target: 57\nwhen input is [57, 53, 52, 57] the target: 8\nwhen input is [57, 53, 52, 57, 8] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0] the target: 0\nwhen input is [57, 53, 52, 57, 8, 0, 0] the target: 25\nwhen input is [57, 53, 52, 57, 8, 0, 0, 25] the target: 17\nwhen input is [1] the target: 42\nwhen input is [1, 42] the target: 43\nwhen input is [1, 42, 43] the target: 44\nwhen input is [1, 42, 43, 44] the target: 43\nwhen input is [1, 42, 43, 44, 43] the target: 41\nwhen input is [1, 42, 43, 44, 43, 41] the target: 58\nwhen input is [1, 42, 43, 44, 43, 41, 58] the target: 1\nwhen input is [1, 42, 43, 44, 43, 41, 58, 1] the target: 53\nwhen input is [21] the target: 1\nwhen input is [21, 1] the target: 61\nwhen input is [21, 1, 61] the target: 39\nwhen input is [21, 1, 61, 39] the target: 52\nwhen input is [21, 1, 61, 39, 52] the target: 42\nwhen input is [21, 1, 61, 39, 52, 42] the target: 43\nwhen input is [21, 1, 61, 39, 52, 42, 43] the target: 56\nwhen input is [21, 1, 61, 39, 52, 42, 43, 56] the target: 6\nwhen input is [43] the target: 43\nwhen input is [43, 43] the target: 51\nwhen input is [43, 43, 51] the target: 5\nwhen input is [43, 43, 51, 5] the target: 42\nwhen input is [43, 43, 51, 5, 42] the target: 1\nwhen input is [43, 43, 51, 5, 42, 1] the target: 40\nwhen input is [43, 43, 51, 5, 42, 1, 40] the target: 59\nwhen input is [43, 43, 51, 5, 42, 1, 40, 59] the target: 56\nwhen input is [45] the target: 50\nwhen input is [45, 50] the target: 63\nwhen input is [45, 50, 63] the target: 1\nwhen input is [45, 50, 63, 1] the target: 52\nwhen input is [45, 50, 63, 1, 52] the target: 53\nwhen input is [45, 50, 63, 1, 52, 53] the target: 61\nwhen input is [45, 50, 63, 1, 52, 53, 61] the target: 12\nwhen input is [45, 50, 63, 1, 52, 53, 61, 12] the target: 0\nwhen input is [52] the target: 53\nwhen input is [52, 53] the target: 58\nwhen input is [52, 53, 58] the target: 8\nwhen input is [52, 53, 58, 8] the target: 0\nwhen input is [52, 53, 58, 8, 0] the target: 25\nwhen input is [52, 53, 58, 8, 0, 25] the target: 63\nwhen input is [52, 53, 58, 8, 0, 25, 63] the target: 1\nwhen input is [52, 53, 58, 8, 0, 25, 63, 1] the target: 61\nwhen input is [53] the target: 58\nwhen input is [53, 58] the target: 6\nwhen input is [53, 58, 6] the target: 1\nwhen input is [53, 58, 6, 1] the target: 51\nwhen input is [53, 58, 6, 1, 51] the target: 63\nwhen input is [53, 58, 6, 1, 51, 63] the target: 1\nwhen input is [53, 58, 6, 1, 51, 63, 1] the target: 50\nwhen input is [53, 58, 6, 1, 51, 63, 1, 50] the target: 53\n\n\n요약하면 위의 16x8 입력 배열에는 총 128개의 예제가 포함되어 있으며 이들은 완전히 독립적입니다."
  },
  {
    "objectID": "notebooks/WS04_LMsForShakespeareAndProteins.html#질문",
    "href": "notebooks/WS04_LMsForShakespeareAndProteins.html#질문",
    "title": "  섹션 1: 소개 – 약간의 역사",
    "section": "",
    "text": "모델의 하이퍼파라미터를 수정하고 각각이 모델의 성능에 어떤 영향을 미치는지 확인해 봅시다!\n질문: n_embd(임베딩 크기)를 64에서 더 높은 값(예: 128 또는 256)으로 늘리세요. 이것이 원래 모델과 비교하여 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n Increasing the embedding size allows the model to capture more nuanced patterns in the data. \\n A larger embedding dimension generally improves the model's capacity to understand and generate more complex language features. \\n Final loss should decrease.\")\n\n\n질문: n_layer(트랜스포머 레이어 수)를 4에서 더 높은 값(예: 6 또는 8)으로 늘리세요. 이것이 모델의 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More layers allow the model to learn deeper hierarchical patterns, which is important for capturing long-range dependencies in text. \\n The final loss should decrease due to better capacity to model long-range dependencies.\")\n\n\n질문: n_head(어텐션 헤드 수)를 4에서 더 높은 값(예: 8 또는 16)으로 늘리세요. 이것이 모델 성능에 어떤 영향을 미치나요?\n\n\nCode\n#@markdown &lt;font color='#e59454'&gt;Run to See Answer!&lt;/font&gt;\n\nprint(\"\\n More attention heads allow the model to capture multiple aspects of relationships between tokens in parallel. \\n Increasing the number of attention heads can improve the model’s ability to focus on different parts of the input simultaneously. \\n The final loss should decrease.\")\n\n\n일반적으로 임베딩 크기, 레이어 수 또는 어텐션 헤드 수를 늘려 트랜스포머 모델을 확장하면 많은 자연어 처리 작업에서 성능이 향상되는 경향이 있습니다. 더 큰 모델은 데이터의 더 복잡한 패턴, 관계 및 의존성을 포착할 수 있는 용량이 더 크기 때문에 이러한 추세가 나타납니다. 파라미터가 많을수록 모델은 학습 데이터에서 더 잘 일반화하고 더 정확하고 일관되며 맥락적으로 풍부한 출력을 생성할 수 있습니다. 그러나 확장에 따른 이득은 특정 지점에서 감소하며, 특히 제한된 데이터나 계산 리소스 내에서는 과적합이나 비효율성을 피하기 위해 하이퍼파라미터를 신중하게 조정해야 합니다.\n관련 논문 * 언어 모델로 5억 년의 진화 시뮬레이션, Hayes et al, 2024 * ESM Cambrian: 비지도 학습으로 단백질의 신비 밝히기, 2024 * 개선된 항체 설계를 위한 항체 생식세열 편향 및 언어 모델에 미치는 영향 해결, 2024 * 단백질 언어 시각화 도구: 언어 모델 시대의 서열 유사성 네트워크, 2024 * 단백질 언어 모델 미세 조정으로 다양한 작업의 예측 향상, 2024 * 긴 문맥 단백질 언어 모델, 2024 * 학습 계산 최적 단백질 언어 모델, 2024 * 프롬프트된 언어 모델을 통한 다목적 제어 가능한 단백질 생성, 2024 * 언어 모델로 단백질 설계, 2024 * 단백질 언어 모델은 상호 작용하는 서열 모티프의 진화 통계 학습, 2024 * p-IgGen: 쌍체 항체 생성 언어 모델, 2024 * 단백질 언어 모델은 계산 최적인가?, 2024 * 특징 재사용 및 확장: 단백질 언어 모델을 통한 전이 학습 이해, 2024 * 구조 조건부 설계를 위한 단백질 언어 모델 적응, 2024 * 단백질 언어 모델에서 고차 상호 작용 복구에 관하여, 2024 * 단백질 언어 모델은 생명의 나무 전반에 걸친 불평등한 서열 샘플링으로 편향됨, 2024 * ESM All-Atom: 통합 분자 모델링을 위한 다중 스케일 단백질 언어 모델, 2024 * 단백질 유도 진화를 위한 지식 인식 강화 언어 모델, 2024 * 멀티 모달 대규모 언어 모델로 단백질 기능 예측 가능, 2024 * 다양한 다운스트림 작업에서 텍스트 통합 단백질 언어 모델 임베딩 및 임베딩 융합 벤치마킹, 2024 * 대규모 쌍체 항체 언어 모델, 2024 * 구조 정보 언어 모델을 통한 단백질 및 항체 복합체의 비지도 진화, 2024 * 기본 페어링으로 항체 언어 모델 개선, 2023 * 대규모 언어 모델은 다양한 계열에 걸쳐 기능적 단백질 서열 생성, 2023 * ProGen2: 단백질 언어 모델의 경계 탐구, 2022 * 항체 설계를 위한 생성적 언어 모델링, 2023 * 진화 규모의 단백질 서열 언어 모델로 정확한 구조 예측 가능, 2023"
  },
  {
    "objectID": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#결론",
    "href": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#결론",
    "title": "모두 합치기: 단백질 설계",
    "section": "결론",
    "text": "결론\n이 노트북을 마치면서 무조건 단백질을 만들고, 스캐폴드로 단백질을 생성하고, 관심 영역에 대한 드노보 바인더를 만드는 방법을 배웠습니다. 부분 확산과 같은 고급 또는 추가 주제의 경우 전체 RFDiffusion 노트북을 방문하거나 리간드 및 DNA로 확산하기 위한 RFDiffusion-AA 노트북을 살펴보세요.\n이 노트북과 이전의 모든 튜토리얼을 마친 후 딥러닝 단백질 설계 도구를 사용하는 방법뿐만 아니라 작동 원리 뒤에 숨겨진 이론도 배웠습니다. 이는 도구 개발이든 다양한 생물학적 작업에 단백질 설계를 적용하는 것이든 미래의 연구를 조사하기 위한 견고한 기반입니다. 수고하셨습니다!\n이것으로 여러분의 연구와 그 너머에 행운이 있기를 빕니다!\n– DL4Proteins 노트북 팀"
  },
  {
    "objectID": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#파이프라인에-대한-예시-및-지침",
    "href": "notebooks/WS09_PuttingItAllTogether_DesigningProteins.html#파이프라인에-대한-예시-및-지침",
    "title": "모두 합치기: 단백질 설계",
    "section": "파이프라인에 대한 예시 및 지침",
    "text": "파이프라인에 대한 예시 및 지침\n\n연속 체인을 정의하려면 contigs를 사용하세요. 여러 contig를 정의하려면 :를 사용하고 contig 내의 여러 세그먼트를 정의하려면 /를 사용하세요. 예를 들어:\n무조건 (unconditional) - contigs='100' - 길이 100의 단량체(monomer) 확산 - contigs='50:100' - 길이 50 및 100의 이종 올리고머(hetero-oligomer) 확산\n모티프 스캐폴딩 (motif scaffolding) - contigs='40/A163-181/40' pdb='5TPN' - contigs='A3-30/36/A33-68' pdb='6MRR' - 정의된 PDB 범위의 두 세그먼트 사이에 길이 36의 루프 확산.\n바인더 설계 (binder design) - contigs='A:50' pdb='4N5T' - 정의된 PDB의 체인 A에 대해 길이 50의 바인더 확산. - contigs='E6-155:70-100' pdb='5KQV' hotspot='E64,E88,E96' - 체인 E와 정의된 핫스팟에 대해 길이 70에서 100(무작위 샘플링)의 바인더 확산.\n팁과 요령 - pdb='' 업로드 프롬프트를 얻으려면 비워 두세요. - contigs='50-100' 대시를 사용하여 샘플링할 길이 범위를 지정하세요.\n예시 솔루션\n무조건 확산: contigs='100',iterations=any\n모티프 스캐폴딩: contigs='A1-29/5-5/A98-126 : A30-97', pdb=6w70, iterations=any\n바인더: contigs='A1-63:B1-68:100-100', hotspots=B58, A41, B56, A44"
  }
]